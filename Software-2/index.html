<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  
  <meta name="referrer" content="no-referrer"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Software 2.0 2.0 | Curiosity</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Software 2.0 2.0" />
<meta name="author" content="Zeeshan Khan Suri" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Realizing Performance in Era of Deep Learning by Scaling with Data" />
<meta property="og:description" content="Realizing Performance in Era of Deep Learning by Scaling with Data" />
<link rel="canonical" href="https://zshn25.github.io/Software-2/" />
<meta property="og:url" content="https://zshn25.github.io/Software-2/" />
<meta property="og:site_name" content="Curiosity" />
<meta property="og:image" content="https://zshn25.github.io/images/loop.svg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-04T21:21:23-06:00" />
<script type="application/ld+json">
{"url":"https://zshn25.github.io/Software-2/","@type":"BlogPosting","headline":"Software 2.0 2.0","dateModified":"2022-12-04T21:21:23-06:00","datePublished":"2022-12-04T21:21:23-06:00","image":"https://zshn25.github.io/images/loop.svg","mainEntityOfPage":{"@type":"WebPage","@id":"https://zshn25.github.io/Software-2/"},"author":{"@type":"Person","name":"Zeeshan Khan Suri"},"description":"Realizing Performance in Era of Deep Learning by Scaling with Data","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://zshn25.github.io/feed.xml" title="Curiosity" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>


<script src="/assets/js/applytheme.js"></script>
<script src="/assets/js/main.js"></script>

<!-- <link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"> -->
<link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"></head>
<body><header class="site-header fixed-top">

  <div class="wrapper"><a class="site-title no-underline hover-grow" rel="author" href="/"> <img src="/images/logo.png" style="max-width:40px;" alt="Curiosity logo"> Curiosity</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about/">About</a>
            <a class="page-link" href="/categories/">Tags</a>
          <span id="nav-switch-theme" class="nav-anchor">
            <span class="nav-theme-icon fas fa-fw" aria-hidden="true" title="Theme"></span>
            <span class="sr-only">Toggle Theme</span>
          </span>
        </div>
      </nav></div>
</header>
    
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">
        
        
          
      
        Software 2.0 2.0
      
        
        
      </h1>
    <p class="page-description">Realizing Performance in Era of Deep Learning by Scaling with Data</p><p class="post-meta post-meta-title">
      <i class="far fa-calendar-alt"></i><time class="dt-published" datetime="2022-12-04T21:21:23-06:00" itemprop="datePublished">
        Dec 4, 2022
      </time>• <i class="far fa-user"></i> 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Zeeshan Khan Suri</span></span>
       • <i class="far fa-clock"></i> <span class="read-time" title="Estimated read time">
    
    
      3 mins read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#mlops">#mlops</a>
        
      
      </p>
    

    </header>

  
  

  <div class="post-content e-content" itemprop="articleBody">
    
    <p>This post is a rebuttal on Andrej Karpathy’s <a href="https://karpathy.medium.com/software-2-0-a64152b37c35">Software 2.0</a>. It is brilliant and you should definitely read it but I thought the critical takeaway was lost in the many great points and I wanted to make that more explicit.</p><hr />

<p>Traditional methods for solving a software problem involve people coming up with the required algorithmic steps to tackle it.</p>

<p>For example, think about a problem of estimating the depth of an object from an image. A traditional algorithm would contain the following steps</p>

<ul>
  <li>
    <p>Gather 2 frames with enough baseline such that there is change within the scene but not too much</p>
  </li>
  <li>
    <p>Find features such as SIFT, etc. on one frame, assuming the same features also exist on the other frame.</p>
  </li>
  <li>
    <p>Find correspondence between these features, filter the correspondences and estimate a 2D flow</p>
  </li>
  <li>
    <p>Derive relative depth from the flow</p>
  </li>
</ul>
      <h2 id="scaling-with-data">
        
        
          <a href="#scaling-with-data" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Scaling with data
        
        
      </h2>
    

<p>Such an approach would work good enough in general and can be deployed for all kinds of scenarios which satisfy the approach’s approximations/assumptions but its performance would not depend on the amount of available data.</p>

<p style="text-align: center;"><img src="/images/performance_classical.svg" alt="s" /></p>
<p style="text-align: center;"><sub><sup><em>Performance of a traditional algorithm does not depend on the amount of avaiable data</em>
</sup></sub></p>
      <h3 id="how-can-we-do-better">
        
        
          <a href="#how-can-we-do-better" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> How can we do better?
        
        
      </h3>
    

<p>We can try to improve each of the approach’s steps and gain overall performance, and decades of research in Computer Vision does exactly that. But, by changing our fundemental thinking towards learning based approaches, unprecidented improvements were realized.</p>
      <h2 id="era-of-learning">
        
        
          <a href="#era-of-learning" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Era of learning
        
        
      </h2>
    

<p>Learning based approaches replaced the handcrafted features (SURF in the above example) to derive these features from the data. Neural networks replaced the rest</p>

<blockquote class="no_toc">
  <p>Checkout my blog post on <a href="/self-supervision-for-foundation-models/">self-supervised learning for foundation models</a></p>
</blockquote>
      <h3 id="neural-networks-are-function-approximators">
        
        
          <a href="#neural-networks-are-function-approximators" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Neural networks are function approximators
        
        
      </h3>
    

<p>Traditional approach is to come up with a model/algorithm that converts an input to an output.</p>
      <h3 id="-textinput-rightarrow--model--rightarrow-textoutput-">
        
        
          <a href="#-textinput-rightarrow--model--rightarrow-textoutput-" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> $ \text{input} \rightarrow \ model \ \rightarrow \text{output} $
        
        
      </h3>
    

<p>Deep learning replaces this with 2 steps:</p>

<ol>
  <li>
    <p>Training: Given many examples of input and output data, the model is learnt.</p>
  </li>
  <li>
    <p>Deployment: The learnt model is used on new inputs to infer their outputs.</p>
  </li>
</ol>

<p>This allows the performance of the model to be dependent on the amount of training data available.</p>

<p style="text-align: center;"><img src="/images/performance_data.svg" alt="s" /></p>
<p style="text-align: center;"><sub><sup><em>As the training data increases, the performance of a DL model increases but traditional methods do not depend on the training data</em>
</sup></sub></p>
      <h3 id="production-time">
        
        
          <a href="#production-time" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Production time
        
        
      </h3>
    

<p>Traditional algorithms have to be almost always completely re-written from their prototyping phase to production while keeping the computational power and memory in check. At test time, Deep learning replaces all of this with just one model inference call. So, we go from re-implementing production level code to passing the input through a bunch of linear algebra. Neural networks cost same amount of memory and even better comoutational effeciency during inference time.</p>
      <h2 id="algorithm-20ic-thinking">
        
        
          <a href="#algorithm-20ic-thinking" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Algorithm 2.0ic thinking
        
        
      </h2>
    

<ol>
  <li>
    <p>First, to gain performance, it is not eough to copy paste one training example into multiple copies. The model needs to be intellignetly scaled by feeding in huge amounts of new data with much variation, representing the true population as truely as possible.</p>
  </li>
  <li>
    <p>Secondly, new data doesn’t come for free. It is labor-intensive and expensive to label data. Smart gathering is required. One cannot just copy paste existing data and assume the data has increased. It has to increase meaningfully.</p>
  </li>
  <li>
    <p>Thirdly, in order to be able to feed in that much amount of data, there needs to be infrastructure in place.</p>
  </li>
  <li>
    <p>As in the case of traditional algorithms, one cannot just hand-engineer the algorithm and use it in deployment forever. One needs to continuously improve/train the learnt model and continuoiusly deploy it. This is also where the data can be selected smartly for the next iteration of improvement.</p>
  </li>
</ol>
      <h2 id="the-holy-grail-or-just-a-fad">
        
        
          <a href="#the-holy-grail-or-just-a-fad" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> The holy grail? or just a fad?
        
        
      </h2>
    

<p>For many applications, the performance gain is really not that critical. Other factors such as explainability, fail-safe, etc are equally important, which the current deeo learning approaches lack. The availability of data is another critical factor in deciding to choose learning based approaches.</p>

<p style="text-align: center;"><img src="/images/performance_lowdata.svg" alt="s" /></p>
<p style="text-align: center;"><sub><sup><em>If data is limited (green part), traditional algorithms provide better performance</em>
</sup></sub></p>
      <h2 id="continuous-improvement">
        
        
          <a href="#continuous-improvement" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Continuous improvement
        
        
      </h2>
    

<p>But, if the performance gains are to be realized, scaling with data is the way to go.</p>

<iframe src="https://ourworldindata.org/grapher/artificial-intelligence-number-training-datapoints" loading="lazy" style="width: 100%; height: 600px; border: 0px none;"></iframe>
<p style="text-align: center;"><sub><sup><em>Amount of data used to train notable AI systems. Source: <a href="https://ourworldindata.org/grapher/artificial-intelligence-number-training-datapoints">Our World in Data</a></em>
</sup></sub></p>

<p>And the way to execute this smartly is by continuous improvement</p>

<p style="text-align: center;"><img src="/images/loop.svg" alt="s" /></p>
<p style="text-align: center;"><sub><sup><em>Continuous Improvement loop</em>
</sup></sub></p>

  </div><hr>
  <!-- This code is taken from Jekyll-Codex (jekyllcodex.org). MIT License
Copyright (c) 2020 Usecue BV -->




<div style="text-align: center;">
    <span style="color: rgb(134, 134, 134);">Share on : </span>
    <div id="share-buttons">
        <div class="facebook" title="Share this on Facebook" onclick="window.open('http://www.facebook.com/share.php?u=https://zshn25.github.io/Software-2/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759h-306v-759h-255v-296h255v-218q0-186 104-288.5t277-102.5q147 0 228 12z"/></svg></div>
        <div class="twitter" title="Share this on Twitter" onclick="window.open('https://twitter.com/intent/tweet?text=Software 2.0 2.0&url=https://zshn25.github.io/Software-2/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5t-115.5 248.5-184.5 210.5-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5t-114-159.5q33 5 61 5 43 0 85-11-112-23-185.5-111.5t-73.5-205.5v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5t371.5 99.5q-8-38-8-74 0-134 94.5-228.5t228.5-94.5q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div>
        <div class="linkedin" title="Share this on Linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=https://zshn25.github.io/Software-2/&title=&summary=&source=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></svg></div>
        <!-- <div class="pinterest" title="Share this on Pinterest" onclick="window.open('https://pinterest.com/pin/create/button/?url=&media=https://zshn25.github.ioimages/loop.svg&description=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M256 597q0-108 37.5-203.5t103.5-166.5 152-123 185-78 202-26q158 0 294 66.5t221 193.5 85 287q0 96-19 188t-60 177-100 149.5-145 103-189 38.5q-68 0-135-32t-96-88q-10 39-28 112.5t-23.5 95-20.5 71-26 71-32 62.5-46 77.5-62 86.5l-14 5-9-10q-15-157-15-188 0-92 21.5-206.5t66.5-287.5 52-203q-32-65-32-169 0-83 52-156t132-73q61 0 95 40.5t34 102.5q0 66-44 191t-44 187q0 63 45 104.5t109 41.5q55 0 102-25t78.5-68 56-95 38-110.5 20-111 6.5-99.5q0-173-109.5-269.5t-285.5-96.5q-200 0-334 129.5t-134 328.5q0 44 12.5 85t27 65 27 45.5 12.5 30.5q0 28-15 73t-37 45q-2 0-17-3-51-15-90.5-56t-61-94.5-32.5-108-11-106.5z"/></svg></div> -->
        <div class="mail" title="Share this through Email" onclick="window.open('mailto:?&body=https://zshn25.github.io/Software-2/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47h-1472q-66 0-113-47t-47-113v-794q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 100-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38-42.5-30.5q-91-64-262-182.5t-205-142.5q-62-42-117-115.5t-55-136.5q0-78 41.5-130t118.5-52h1472q65 0 112.5 47t47.5 113z"/></svg></div>
    </div>
</div>
  
  <!-- Source: https://stackoverflow.com/questions/25348389/jekyll-and-liquid-show-related-posts-by-amount-of-equal-tags-2 
License: https://creativecommons.org/licenses/by-sa/3.0/-->
<div class="relatedPosts">
      <h3>
        
        
          You might also like
        
        
      </h3>
    

    
    

    
    

    
        

            
            

            

            

        
    
        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/pc4consistentdepth/">Pose Constraints for Self-supervised Monocular Depth and Ego-Motion</a>  •  
                        <i class="far fa-calendar-alt"></i> Apr 17, 2023  •  
                        <i class="far fa-clock"></i> 
       1 min read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#self-supervision">#self-supervision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#depth-estimation">#depth-estimation</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/CNNs-vs-Transformers/">Convolution vs. Attention</a>  •  
                        <i class="far fa-calendar-alt"></i> Mar 9, 2023  •  
                        <i class="far fa-clock"></i> 
       6 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#convolution">#convolution</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#attention">#attention</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/How-Monocular-Depth-Estimation-works/">Self-supervised monocular depth estimation</a>  •  
                        <i class="far fa-calendar-alt"></i> Oct 30, 2022  •  
                        <i class="far fa-clock"></i> 
       8 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/Layers-fusion-for-faster-inference/">Layers fusion for faster neural network inference</a>  •  
                        <i class="far fa-calendar-alt"></i> Apr 27, 2021  •  
                        <i class="far fa-clock"></i> 
       3 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#pytorch">#pytorch</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#fast-inference">#fast-inference</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/ResNet-feature-pyramid-in-Pytorch/">ResNet feature pyramid in Pytorch</a>  •  
                        <i class="far fa-calendar-alt"></i> Feb 9, 2021  •  
                        <i class="far fa-clock"></i> 
       10 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#pytorch">#pytorch</a>
                                
                            
                    </p>
                </article>
                
                
                    
    
    
</div>
  

<nav class="paginate-container"  aria-label="Pagination"></nav>
  <div class="pagination" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
      
        
          <a class="previous_page" style="flex: 1 1 0; width: 45%; padding-right: 0.5em; text-align: right; background-color: transparent;" rel="previous" aria-label="Previous Page" href="/How-Monocular-Depth-Estimation-works/"><b>Previous:</b> Self-supervised monocular depth estimation
</a>
        
      
      
        
          <a class="next_page" style="flex: 1 1 0; width: 45%; padding-left: 0.5em; text-align: left; background-color: transparent;" rel="next" aria-label="Next Page" href="/CNNs-vs-Transformers/"><b>Next:</b> Convolution vs. Attention
</a>
        
      
  </div>
</nav><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="zshn25/zshn25.github.io"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Software-2/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://feedrabbit.com/?url=https://zshn25.github.io/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">© Zeeshan Khan Suri </li>
          <div xmlns:cc="http://creativecommons.org/ns#" > Text under  <a href="http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"></a> unless stated otherwise. Images and other media have their own copyright. </div>
          <li><a class="u-email" href="mailto:zshn25[at]gmail[dot]com">zshn25[at]gmail[dot]com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>I am curious. Therefore I am.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul></div>

  </div>

</footer>
</body>

</html>