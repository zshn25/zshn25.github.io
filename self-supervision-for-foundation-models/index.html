<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  
  <meta name="referrer" content="no-referrer"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Self-supervised learning for vision foundation models | Curiosity</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Self-supervised learning for vision foundation models" />
<meta name="author" content="Zeeshan Khan Suri" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Performance of deep neural networks scales with data and model size" />
<meta property="og:description" content="Performance of deep neural networks scales with data and model size" />
<link rel="canonical" href="https://zshn25.github.io/self-supervision-for-foundation-models/" />
<meta property="og:url" content="https://zshn25.github.io/self-supervision-for-foundation-models/" />
<meta property="og:site_name" content="Curiosity" />
<meta property="og:image" content="https://zshn25.github.io/images/foundation.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-05-11T22:21:23-05:00" />
<script type="application/ld+json">
{"url":"https://zshn25.github.io/self-supervision-for-foundation-models/","@type":"BlogPosting","headline":"Self-supervised learning for vision foundation models","dateModified":"2023-05-11T22:21:23-05:00","datePublished":"2023-05-11T22:21:23-05:00","image":"https://zshn25.github.io/images/foundation.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://zshn25.github.io/self-supervision-for-foundation-models/"},"author":{"@type":"Person","name":"Zeeshan Khan Suri"},"description":"Performance of deep neural networks scales with data and model size","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://zshn25.github.io/feed.xml" title="Curiosity" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>


<script src="/assets/js/applytheme.js"></script>
<script src="/assets/js/main.js"></script>

<!-- <link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"> -->
<link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"></head>
<body><header class="site-header fixed-top">

  <div class="wrapper"><a class="site-title no-underline hover-grow" rel="author" href="/"> <img src="/images/logo.png" style="max-width:40px;" alt="Curiosity logo"> Curiosity</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about/">About</a>
            <a class="page-link" href="/categories/">Tags</a>
          <span id="nav-switch-theme" class="nav-anchor">
            <span class="nav-theme-icon fas fa-fw" aria-hidden="true" title="Theme"></span>
            <span class="sr-only">Toggle Theme</span>
          </span>
        </div>
      </nav></div>
</header>
    
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">
        
        
          
      
        Self-supervised learning for vision foundation models
      
        
        
      </h1>
    <p class="post-meta post-meta-title">
      <i class="far fa-calendar-alt"></i><time class="dt-published" datetime="2023-05-11T22:21:23-05:00" itemprop="datePublished">
        May 11, 2023
      </time>• <i class="far fa-user"></i> 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Zeeshan Khan Suri</span></span>
       • <i class="far fa-clock"></i> <span class="read-time" title="Estimated read time">
    
    
      15 mins read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#large-neural-networks">#large-neural-networks</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#foundation-models">#foundation-models</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#self-supervision">#self-supervision</a>
        
      
      </p>
    

    </header>

  
  

  <div class="post-content e-content" itemprop="articleBody">
    
    <blockquote title="The blockquote title" class="no_toc">
      <h2 id="performance-of-deep-neural-networks-scales-with-data-and-model-size">
        
        
          <a href="#performance-of-deep-neural-networks-scales-with-data-and-model-size" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> <a href="/Software-2/">Performance of deep neural networks scales with data and model size</a>
        
        
      </h2>
    
</blockquote>

<iframe src="https://ourworldindata.org/grapher/artificial-intelligence-training-computation" loading="lazy" style="width: 100%; height: 600px; border: 0px none;"></iframe>
<p style="text-align: center;"><sub><sup><em>Model size of notable AI systems over time. Source: <a href="https://ourworldindata.org/grapher/artificial-intelligence-training-computation">Our World in Data</a></em>
</sup></sub></p>

<p>Recent developments in deep learning can be attributed to the growing amount of data and model size. This realization has lead to the rapidly developing field of <strong>foundational models</strong>, i.e. large scale, complex neural networks, for e.g. <a href="https://openai.com/research/gpt-4">GPT 4</a>, <a href="https://dinov2.metademolab.com/">DINO v2</a>, <a href="https://facebookresearch.github.io/ImageBind/paper">ImageBind</a>, etc., trained on very large, diverse datasets. By feeding-in huge amounts of high quality labeled data through large artificial neural network models, machines are shown to be taught to outperform humans at multiple tasks.</p>

<p style="text-align: center;"><a href="/Software-2/"><img src="/images/performance_data.svg" alt="s" /></a></p>
<p style="text-align: center;"><sub><sup><em>As the training data increases, the performance of a DL model increases but traditional methods do not depend on the training data</em>
</sup></sub></p>

<p>But, collecting such massive amounts of carefully labeled data costs enormous time, effort and money; and are error prone. For e.g. Northcutt et al.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>, and https://labelerrors.com/ outline errors in the labels of widely used datasets. What if there was a way to use massively available but <strong>unlabled</strong> data?</p>

<p>In this post, I motivate the necessity for large, general-purpose data representation models, known as foundation models, and in what ways self-supervised learning enables this achievement. Stay tuned!</p>
      <h2 id="what-are-foundation-models">
        
        
          <a href="#what-are-foundation-models" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> What are foundation models?
        
        
      </h2>
    

<p>Behind the fancy wording<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote">2</a></sup>, the idea is to learn a generic model that produces good input representations, is robust to variety of inputs, and can be directly used for multiple other tasks without the need for re-training. Any model that is capable of doing so can be named a foundation model.</p>

<blockquote>
  <p>“A foundation model is any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks”</p>

  <p>Bommasani, Rishi, et al.<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote">3</a></sup></p>
</blockquote>

<p>The way to realize such a model is to use vast quantities of data and the way to do so is by self-supervised learning. <mark>The scale and diversity of training data is what makes a model foundational.</mark> Also, training on more data leads to generalization and robustness to distribution shifts and adversarial samples<sup id="fnref:12:1" role="doc-noteref"><a href="#fn:12" class="footnote">2</a></sup></p>
      <h2 id="history-feature-representations-of-images">
        
        
          <a href="#history-feature-representations-of-images" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> History: Feature representations of images
        
        
      </h2>
    

<p>Tradition computer vision methods extract distinctive pixel-level features from input images. High-dimensional representations of these features, known as <strong>feature descriptors</strong> are computed from the detected local features which are then used for various applications, such as tracking, retrieval, structure from motion, localization, etc. The goal of transforming the detected features into high-dimensional descriptors is to embed some notion of semantic and neighboring information to the pixel-level features. The goal is to make them distinctive (good for matching across different viewpoints) and invariant to vairations in 3D viewpoint, illumination, etc.</p>
      <h3 id="features-of-good-features">
        
        
          <a href="#features-of-good-features" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Features of good features
        
        
      </h3>
    

<p style="text-align: center;"><img src="https://github.com/cvg/Hierarchical-Localization/raw/master/doc/loc_aachen.svg" alt="sift" /></p>
<p style="text-align: center;"><sub><sup>Good features must be robust against 3D viewpoint, illumination, scaling, etc. Image from <a href="https://www.visuallocalization.net/datasets/">Aachen Day-Night Dataset</a> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>, <a href="https://github.com/cvg/Hierarchical-Localization">Hierarchical-Localization</a>
</sup></sub></p>

<p>To work robustly in real world scenarios, these feature descriptors must be invariant against view-point changes (rotation and translation of camera in 3D), scaling, illumination changes, and other adverse conditions. As seen in the above example image, such features of good features make matching under varying conditions possible, making them highly useful.</p>
      <h2 id="learning-features">
        
        
          <a href="#learning-features" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Learning features
        
        
      </h2>
    

<p>With the rise of deep learning, features and their descriptors are no longer hand-engineered, but learnt from the underlying data. Vision models such as CNNs and transformers process input images to extract high-dimensional latent representations, which could be thought as features. Unlike pixel-level features such as <abbr title="Scale-invariant feature transform">SIFT</abbr>, <abbr title="Oriented FAST and rotated BRIEF">ORB</abbr>, latents are <strong>not local</strong> (do not correspond to pixel locations) but <strong>based on the semantics</strong> of the global image contents.</p>

<blockquote class="no_toc">
  <p>Checkout my blog post on <a href="/ResNet-feature-pyramid-in-Pytorch/">how to extract feature pyramids from torchvison models</a></p>
</blockquote>

<p style="text-align: center;"><a href="https://commons.wikimedia.org/wiki/File:Denoising-autoencoder.png"><img src="https://upload.wikimedia.org/wikipedia/commons/1/18/Denoising-autoencoder.png" alt="feature-learning" /></a></p>
<p style="text-align: center;"><sub><sup>. Image from <a href="https://commons.wikimedia.org/wiki/File:Denoising-autoencoder.png">neerajkrbansal1996</a>, CC0, via Wikimedia Commons*
</sup></sub></p>

<p>As early as in 2009, the idea to use self-supervised learning for</p>
      <h2 id="the-need-for-foundation-models">
        
        
          <a href="#the-need-for-foundation-models" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> The need for foundation models
        
        
      </h2>
    

<p>If you’re thinking, if existing models already extract features, why is there a need for foundation models?</p>

<p>You are absolutely right. Existing models such as YOLO<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">4</a></sup> object detection, extract features, which are further passed to the object-detection head. But, there are two reasons where foundation models come into play.</p>

<ol>
  <li>The features extracted from Yolo are <strong>task-specific</strong>. Features that are useful for object-detection, might not be useful or optimal for other tasks, for example, for depth estimation. We want our ideal features to be <strong>task-agnostic</strong>. The reasoning behind this is the underlying assumption that there exist features that are useful for all kinds of tasks, just as we saw in the traditional feature detectors and descriptors like <abbr title="Scale-invariant feature transform">SIFT</abbr>.</li>
  <li>We want to train on lots of data. But, supervised tasks such as object detection require expensive labeled data, and thus are infeasible to scale.</li>
</ol>

<p style="text-align: center;"><img src="/images/multi-task.svg" alt="multi-task" /></p>
<p style="text-align: center;"><sub><sup>The features of a network trained on a pretext task, for e.g., depth estimation, might not necessarily be useful or optimal for other tasks, for e.g., classification, as depicted in green. Cat image by <a href="https://www.pexels.com/photo/selective-focus-photography-of-orange-tabby-cat-1170986/">EVG Kowalievska</a>.
</sup></sub></p>
      <h1 id="self-supervised-learning">
        
        
          <a href="#self-supervised-learning" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Self-supervised learning
        
        
      </h1>
    

<p>Animals, including humans, learn a many concepts such as object permanance, etc., without explicit supervision.</p>

<!-- 
<center>
<div class="container">
    <iframe src="https://www.youtube-nocookie.com/embed/OLrYzY3jVPY" frameborder="0" allowfullscreen class="video"></iframe>
</div>
</center>
{: width="100%" .shadow}
{:refdef: style="text-align: center;"}
<sub><sup>
</sup></sub>
{: refdef} -->

<blockquote>
  <p>Self-supervised is where the supervision signal comes from data itself, unlike supervised learning, where supervision comes from explicit human labels and unsupervised learning, where there is no supervision.</p>
</blockquote>

<p>Is it possible to make machines learn general concepts without explicit supervision? If so, how?</p>

<p>If we are able formulate <strong>pretext tasks</strong>, we can use <mark>self-supervised learning</mark> to <mark>learn good data representations and general concepts from massively-available, unlabelled data</mark>, we can use such models directly (without fine-tuning, Zero-Shot) for multiple <strong>downstream tasks</strong>. (atleast, that’s the hope).</p>

<p style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Feature_learning#/media/File:Feature_Learning_Diagram.png"><img src="https://upload.wikimedia.org/wikipedia/commons/0/0b/Feature_Learning_Diagram.png" alt="feature-learning" /></a></p>
<p style="text-align: center;"><sub><sup>Implicit feature representations are learned via pretext tasks and can be used as input for specific downstream tasks. Image from <a href="https://commons.wikimedia.org/wiki/File:Feature_Learning_Diagram.png">Fgpacini</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons*
</sup></sub></p>
      <h2 id="pretext-tasks-for-images">
        
        
          <a href="#pretext-tasks-for-images" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Pretext tasks for images
        
        
      </h2>
    

<p>Any task that doesn’t require explitcit labels can be used as a pretext task. The goal is to be able to make use of as much data as possible. This task is not actually the task what the model would finally be required to do, but this task would allow to use much more data via self-supervised learning.</p>

<blockquote>
      <h2 id="for-a-more-comprehensive-list-of-self-supervised-pretext-tasks-checkout-this-page-from-papers-with-code-and-this-curated-list-of-research-in-self-supervised-learning">
        
        
          <a href="#for-a-more-comprehensive-list-of-self-supervised-pretext-tasks-checkout-this-page-from-papers-with-code-and-this-curated-list-of-research-in-self-supervised-learning" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> For a more comprehensive list of self-supervised pretext tasks, checkout <a href="https://paperswithcode.com/methods/category/self-supervised-learning">this page from Papers with Code</a>, and <a href="https://github.com/jason718/awesome-self-supervised-learning">this curated list</a> of research in self-supervised learning
        
        
      </h2>
    
</blockquote>

<p>Prediction/Generation and Similarity are the two main concepts that can be exploited to design self-supervised pretext tasks.</p>
      <h3 id="generative-pretext-tasks-based-on-restoration">
        
        
          <a href="#generative-pretext-tasks-based-on-restoration" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Generative pretext tasks based on restoration
        
        
      </h3>
    

<p>These methods involve artificially manipulating the input and teaching the network to undo the manipulation. The hope is that in order to restore large a variety of training data, the network learns robust image representations. Examples of such input manipulations include masking out image regions<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote">5</a></sup>, adding Gaussian noise<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote">6</a></sup>, reordering/permuting patches as a Jigsaw puzzle<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">7</a></sup>, transformations of the image, such as rotation, and viewpoint change<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote">8</a></sup>, removing color<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote">9</a></sup>, etc. There is no explicit need to manipulate the original image, for e.g. a <abbr title="Generative adversarial network">GAN</abbr> was used to regenerate the input image from compressed latent features<sup id="fnref:21" role="doc-noteref"><a href="#fn:21" class="footnote">10</a></sup>.</p>

<p style="text-align: center;"><img src="/images/generative_cat.png" alt="multi-task" /></p>
<p style="text-align: center;"><sub><sup>Examples of image degradations: masking, noising, permuting, rotating, grey-scaling. The pretext task is to recover the original image.
</sup></sub></p>

<blockquote>
  <p><strong>Note:</strong> Tasks such as Jigsaw puzzle and transformations of the image are not exactly generative. But, I still like to generalize them as generative. Only that the generation is parameterized to a low dimension, for e.g. classification on hash table for Jigsaw puzzle and regression of angles for rotation.</p>
</blockquote>

<p>The difficulty in these methods is the parameterization of degradation is dependent on the exact degradation method, making it hard to combine with other manipulations. For example, the parameters of adding gaussian noise are the mean and standard deviation of the gaussian noise function, which can be estimated by the network to subtract and recover the original image. For rotation, this would be the angle, which means there should now be another head to predict the angle if these both are to be combined.</p>

<!-- A large variety of pretext tasks have been proposed to learn neural representations of the data's underlying structure. For example, Noroozi et al.propose to find a reordering of tiles from a 3x3 grid of a square region cropped from an image as the pretext task; \cite{9879206} propose masked autoencoders, which masks random patches of the input image and reconstructs the missing patches as the pretext task; \cite{pmlr-v139-radford21a}'s CLIP: given an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in their dataset as the pretext task. 

Since the main objective in this tasks is to rely on existing unlabeled data for pre-training, one of the main similarities in these tasks is to degrade the input and define a task for the network to reconstruct the original input. For example, \cite{8579073} shuffle and \cite{9879206} remove patches from the image, \cite{rombach2021highresolution} successively apply Gaussian noise to the image -->
      <h3 id="feature-similarity-as-a-pretext-task">
        
        
          <a href="#feature-similarity-as-a-pretext-task" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Feature similarity as a pretext task
        
        
      </h3>
    

<p>Rather than having an explicit task, the idea is to maximize similarity of features of similar images. This simply means, collecting data samples which represent similar information and having a loss that minimizes distance between their feature representations.</p>

<p>Just doing this causes the feature representations to collapse, i.e., all data just points to the same representation. To prevent this, negative samples (having distict representation) are introduced, which act as a relative contrast to the positive samples (having similar representation).</p>
      <h4 id="contrastive-learning">
        
        
          <a href="#contrastive-learning" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Contrastive learning
        
        
      </h4>
    

<p style="text-align: center;"><img src="/images/contrastive.svg" alt="multi-task" /></p>
<p style="text-align: center;"><sub><sup>Contrastive learning example: Cat and it’s augmentations (positive samples), shown by green arrows, must have similar feature representations than the Colosseum image chosen at random (negative sample).
</sup></sub></p>

<p>One of the most influential self-supervised techniques is contrastive learning which, as the name implies, uses not only positive samples but also contrasts them with negative samples. The figure above outlines an example of contrastive learning. The positive examples are shown by green arrows are images sampled from an image with different augmentations, such as rotation, cropping, color, etc. Another image (Colosseum) is chosen at random which acts as the negative sample. The fundamental assumption of contrastive learning is that features of the positive samples lie closer to each other (<strong>positive samples attract</strong>) than to those of a negative sample (<strong>positive-negative samples repel</strong>). This is realized by minimizing the distance between positive samples while maximizing the distance between negative samples in feature space.</p>

<p>Data sampling of the positives and the negatives becomes the key here and different ways to sample them could be incorporated. Multi-modal information or pseudo-labels, such as associated text, audio, other sensor information, could be used to sample negatives and positives. SimCLR<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote">11</a></sup>, for example uses various augmentations of the same image and CMC uses various sensor information of the underlying scene for sampling positives.</p>

<p>One could also sample other images as positives based on methods such as clustering in the feature space or if some kind of labels or GT is known.</p>
      <h3 id="feature-similarity-vs-generative-pretext-tasks">
        
        
          <a href="#feature-similarity-vs-generative-pretext-tasks" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Feature similarity vs. Generative pretext tasks
        
        
      </h3>
    

<p>The advantage of feature similarity based methods is that there is no need for exact pixel to pixel mapping of the input and output, as opposed to generative pretext tasks, where the restored output is a one-to-one mapping of the manipulated/distorted input. One could use non-exact data (for example neighboring frames of a video) for sampling feature learning. One could also combine various augmentation methods mentioned in generaive pretext tasks. Feature similarity would essentially be a way to automatically incorporate various handcrafted ad-hoc heuristics via augmentations and positive-negative sampling.</p>

<p>Recent work titled “What Do Self-Supervised Vision Transformers Learn?” compares contrastive learning with a prominant generative pretext task called Masked Image Modeling (<abbr title="Masked image modeling">MIM</abbr>)<sup id="fnref:6:1" role="doc-noteref"><a href="#fn:6" class="footnote">5</a></sup> and finds:</p>

<ol>
  <li>
    <p>CL plays a significant role in the later layers of the ViT model, capturing longer-range global patterns, such as the shape of an object. However, it also leads to reduced diversity of representations, thereby worsening scalability and dense prediction performance.</p>
  </li>
  <li>
    <p><abbr title="Masked image modeling">MIM</abbr> utilizes high-frequency signals of the representations and mainly focuses on the early layers of the ViTs and is more texture-oriented.</p>
  </li>
</ol>
      <h3 id="other-pretext-tasks">
        
        
          <a href="#other-pretext-tasks" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Other pretext tasks
        
        
      </h3>
    

<p>The generative pretext tasks can be combined with contrastive pretext tasks. For e.g., <a href="https://mbaradad.github.io/learning_with_noise/">Learning to See by Looking at Noise</a> combines the generative and feature similarity by generating synthetic images from random processes and having a contrastive loss between crops of synthetic images and real ones.</p>
      <h1 id="applications">
        
        
          <a href="#applications" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Applications
        
        
      </h1>
    

<p>The learnt features can be applied to a huge range of downstream tasks. Generic tasks include image classification, object detection, segmentation, depth estimation, etc. Here, I will mention some less-known but very interesting downstream applications.</p>

<!--
<div>
    <img src="https://contrastive-learning.github.io/intriguing/assets/img/multi/img_methods_bi.jpeg" alt="" style="object-fit: cover; object-position: 0 0; text-align: center;">
</div>
-->

<p style="text-align: center;"><img src="https://contrastive-learning.github.io/intriguing/assets/img/multi/img_methods_bi.jpeg" alt="multi-task" /></p>
<p style="text-align: center;"><sub><sup> Local regions grouped together after applying K-means on contrastively learnt features. Source: <a href="https://contrastive-learning.github.io/intriguing/">Intriguing Properties of Contrastive Losses</a><sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote">12</a></sup>
</sup></sub></p>

<p>Semantic Similarity <a href="The Unreasonable Effectiveness of Deep Features as a Perceptual Metric">Deep Features as a Perceptual Metric</a><sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote">13</a></sup></p>

<p>Socretic models https://socraticmodels.github.io/</p>

<p>{In-context learning} enables users to provide a query and a few examples from which a model derives an answer without being trained on such queries. https://www.semanticscholar.org/paper/Foundation-models-in-brief%3A-A-historical%2C-focus-Schneider/775b2dc88cf04993f8596332444a906bec2db807</p>
      <h1 id="risks-challanges">
        
        
          <a href="#risks-challanges" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Risks, Challanges
        
        
      </h1>
    

<ul>
  <li>
    <p>{homogenization} of models might replace a myriad of task-specific models with fewer very large models controlled by few corporations leading to a shift in power and control over AI</p>
  </li>
  <li>
    <p>Modelling pretext tasks involves manual selection of data augmentation techniques which is based on our expert heuristics and introduces human biases into the model. Best would be to design methods that just take a bunch of data and decide for themselves which ones are similar, which ones are not. Like real unsupervised learning. But, that is difficult to design.</p>
  </li>
  <li>
    <p>Most research is done on curated object-centric datasets, where the whole image consists of a single object instance. While there is some evidence that it also works for multiple objects<sup id="fnref:14:1" role="doc-noteref"><a href="#fn:14" class="footnote">12</a></sup>, further investigation is needed to check how good the techniques work for generic scenes. Works such as DetCon<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote">14</a></sup> and it’s follow-up: ODIN<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote">15</a></sup>, work towards this goal by relying on pre-processing the whole image into object centric ones.</p>
  </li>
  <li>
    <p>Mining positive and negative samples for contrastive learning is crucial for its performance.</p>
  </li>
  <li>
    <p>Large amounts of quality data will enable better performance and generalizability to downstream tasks. Different modalities can be used to mine such data, including synthetic data. For filtering high quality data, methods such as active learning need to be incorporated.</p>
  </li>
</ul>

<blockquote class="no_toc">
      <h2 id="continuous-improvement-active-learning-pipeline-for-scaling-with-data">
        
        
          <a href="#continuous-improvement-active-learning-pipeline-for-scaling-with-data" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> <a href="/Software-2/">Continuous improvement (Active learning) pipeline for scaling with data</a>
        
        
      </h2>
    
</blockquote>
      <h1 id="tldr-conclusion">
        
        
          <a href="#tldr-conclusion" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> tldr; Conclusion
        
        
      </h1>
    

<p>Performance of supervised models depends on the availability of high quality labeled data, which is tedious to aquire, and thereby limiting. Self-supervised learning not only makes it possible to realize gains via abundant unlabeled data, but also gives way for foundation models that learn a generic neural representation of inputs, whose knowledge can be transformed to application specific downstream tasks. The key to this is designing self-supervised pretext tasks to make use of unlabeled data. The underlying principle of generative pretext tasks is to corrupt the input and make the network recover the original input from the corrupt one. Feature similarity based pretext tasks learn data representations that are similar for similar inputs and dissimilar for dissimilar inputs.</p><hr />
      <h1 id="further-resources">
        
        
          <a href="#further-resources" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Further Resources
        
        
      </h1>
    
      <h2 id="language-foundation-models-llm-and-beyond">
        
        
          <a href="#language-foundation-models-llm-and-beyond" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Language Foundation models (<abbr title="Large language model">LLM</abbr>) and beyond
        
        
      </h2>
    

<p>In this post, I focus on vision foundation models. But, there are plenty of resources for launguage foundation models a.k.a. Large Language Models (LLMs).</p>

<ul>
  <li><a href="https://fullstackdeeplearning.com/llm-bootcamp/">Full Stack <abbr title="Large language model">LLM</abbr> Bootcamp, 2023</a></li>
</ul>
      <h2 id="lectures--videos">
        
        
          <a href="#lectures--videos" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Lectures / Videos
        
        
      </h2>
    

<ul>
  <li>Yann LeCunn’s <a href="https://www.facebook.com/epflcampus/videos/1960325127394608">lecture</a>, [<a href="https://drive.google.com/file/d/12pDCno02FJPDEBk4iGuuaj8b2rr48Hh0/view">Slides</a></li>
  <li><a href="https://fullstackdeeplearning.com/course/2022/">FSDL 2022 Course</a>’s Lecture on Language <a href="https://fullstackdeeplearning.com/course/2022/lecture-7-foundation-models/">Foundation Models</a></li>
  <li><a href="https://futureofai.mit.edu/">MIT FUTURE OF AI: Self-Supervised Learning and Foundation Models</a></li>
  <li>Self-Supervised Learning: Self-Prediction and Contrastive Learning, <a href="https://www.facebook.com/epflcampus/videos/1960325127394608">NIPS 21 Tutorial Video</a>, <a href="https://nips.cc/media/neurips-2021/Slides/21895.pdf">[Slides]</a></li>
</ul>
      <h2 id="libraries">
        
        
          <a href="#libraries" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Libraries
        
        
      </h2>
    

<ul>
  <li><a href="https://github.com/facebookresearch/vissl"><img src="https://github.com/facebookresearch/vissl/raw/main/.github/logo/Logo_Color_Light_BG.png" alt="" style="height: 2.5em; text-align: left;" /></a></li>
  <li><a href="https://docs.lightly.ai/self-supervised-learning/index.html"><img src="https://docs.lightly.ai/self-supervised-learning/_static/lightly_logo_crop_white_text.png" alt="" style="height: 2.5em; text-align: left;" /></a></li>
  <li><a href="https://github.com/vturrisi/solo-learn"><img src="https://github.com/vturrisi/solo-learn/raw/main/logo.png" alt="" style="height: 2.5em; text-align: left;" /></a></li>
</ul>
      <h2 id="read">
        
        
          <a href="#read" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Read
        
        
      </h2>
    

<ul>
  <li>Yann LeCunn’s blog post <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">Self-supervised learning: The dark matter of intelligence</a></li>
  <li>Balestriero, Randall et al. <a href="https://arxiv.org/abs/2304.12210">A Cookbook of Self-Supervised Learning</a> ArXiv abs/2304.12210 (2023): n. pag.</li>
  <li>Weng, Lilian (2019). <a href="https://lilianweng.github.io/posts/2019-11-10-self-supervised/">Self-Supervised Representation Learning</a> writes about pretext tasks in detail</li>
  <li>Silva, Thalles Santos (2020). <a href="https://sthalles.github.io/self-supervised-learning/">Self-Supervised Learning and the Quest for Reducing Labeled Data in Deep Learning</a> gives a great introduction on how self-supervised learning can enable foundation models, anagolous to Yann LeCunn’s <a href="https://www.facebook.com/epflcampus/videos/1960325127394608">lecture</a></li>
  <li>Ozbulak, Utku, et al. “<a href="https://arxiv.org/abs/2305.13689">Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training.</a>” arXiv preprint arXiv:2305.13689 (2023)</li>
</ul>
      <h2 id="data">
        
        
          <a href="#data" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Data
        
        
      </h2>
    

<ul>
  <li><a href="https://laion.ai/blog/laion-5b/">LAION-5B</a> Open Large-scale CLIP-filtered image-text paired dataset</li>
</ul><hr />

<p>© Zeeshan Khan Suri, <a href="http://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i></a></p>

<p>If this article was helpful to you, consider citing</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{suri_self-supervision-for-foundation-models_2023,
      title={Self-supervised learning for vision foundation models},
      url={https://zshn25.github.io/self-supervision-for-foundation-models/}, 
      journal={Curiosity}, 
      author={Suri, Zeeshan Khan}, 
      year={2023}, 
      month={May}}
</code></pre></div></div>
      <h1 id="references">
        
        
          <a href="#references" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> References
        
        
      </h1>
    

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Northcutt, C. G., Athalye, A., &amp; Mueller, J. (2021). <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/f2217062e9a397a1dca429e7d70bc6ca-Abstract-round1.html">Pervasive label errors in test sets destabilize machine learning  enchmarks</a>. Proceedings of the 35th Conference on Neural Information Processing Systems Track on Datasets and Benchmarks <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>Schmidt, Ludwig, et al. “<a href="https://proceedings.neurips.cc/paper/2018/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf">Adversarially robust generalization requires more data.</a>” Advances in neural information processing systems 31 (2018). <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:12:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:19" role="doc-endnote">
      <p>Bommasani, Rishi, et al. <a href="https://crfm.stanford.edu/report.html">“On the opportunities and risks of foundation models.”</a> arXiv preprint arXiv:2108.07258 (2021). <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Redmon, Joseph (2016). “You only look once: Unified, real-time object detection”. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. <a href="https://arxiv.org/abs/1506.02640">arXiv:1506.02640</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>K. He, X. Chen, S. Xie, Y. Li, P. Dollár and R. Girshick, “<a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html">Masked Autoencoders Are Scalable Vision Learners</a>,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 15979-15988, doi: 10.1109/CVPR52688.2022.01553. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Xiang, W., Yang, H., Huang, D., &amp; Wang, Y. (2023). <a href="https://arxiv.org/abs/2303.09769">Denoising Diffusion Autoencoders are Unified Self-supervised Learners</a>. ArXiv, abs/2303.09769. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>M. Noroozi and P. Favaro. <a href="https://arxiv.org/pdf/1603.09246.pdf">Unsupervised learning of visual representations by solving jigsaw puzzles</a>. In ECCV, 2016. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Pulkit Agrawal, Joao Carreira, and Jitendra Malik. 2015. <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Agrawal_Learning_to_See_ICCV_2015_paper.pdf">Learning to See by Moving</a>. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) (ICCV ‘15). IEEE Computer Society, USA, 37–45. https://doi.org/10.1109/ICCV.2015.13 <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Zhang, Isola, Efros. <a href="https://arxiv.org/abs/1603.08511">Colorful Image Colorization</a>. In ECCV, 2016 <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:21" role="doc-endnote">
      <p>Radford, Alec, Luke Metz, and Soumith Chintala. “<a href="https://arxiv.org/abs/1511.06434">Unsupervised representation learning with deep convolutional generative adversarial networks.</a>” arXiv preprint arXiv:1511.06434 (2015). <a href="#fnref:21" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17" role="doc-endnote">
      <p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. <a href="https://dl.acm.org/doi/pdf/10.5555/3524938.3525087">A simple framework for contrastive learning of visual representations.</a> In Proceedings of the 37th International Conference on Machine Learning (ICML’20), Vol. 119. JMLR.org, Article 149, 1597–1607. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p>Chen, Ting and Lala Li. “<a href="https://proceedings.neurips.cc/paper/2021/hash/628f16b29939d1b060af49f66ae0f7f8-Abstract.html">Intriguing Properties of Contrastive Losses.</a>” Neural Information Processing Systems (2020). <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:14:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1801.03924">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</a> Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang CVPR 2018. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:15" role="doc-endnote">
      <p>Hénaff, Olivier J., et al. “<a href="http://openaccess.thecvf.com/content/ICCV2021/html/Henaff_Efficient_Visual_Pretraining_With_Contrastive_Detection_ICCV_2021_paper.html">Efficient visual pretraining with contrastive detection.</a>” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:16" role="doc-endnote">
      <p>Hénaff, Olivier J., et al. “<a href="https://arxiv.org/abs/2203.08777">Object discovery and representation networks.</a>” Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVII. Cham: Springer Nature Switzerland, 2022. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><hr>
  <!-- This code is taken from Jekyll-Codex (jekyllcodex.org). MIT License
Copyright (c) 2020 Usecue BV -->




<div style="text-align: center;">
    <span style="color: rgb(134, 134, 134);">Share on : </span>
    <div id="share-buttons">
        <div class="facebook" title="Share this on Facebook" onclick="window.open('http://www.facebook.com/share.php?u=https://zshn25.github.io/self-supervision-for-foundation-models/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759h-306v-759h-255v-296h255v-218q0-186 104-288.5t277-102.5q147 0 228 12z"/></svg></div>
        <div class="twitter" title="Share this on Twitter" onclick="window.open('https://twitter.com/intent/tweet?text=Self-supervised learning for vision foundation models&url=https://zshn25.github.io/self-supervision-for-foundation-models/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5t-115.5 248.5-184.5 210.5-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5t-114-159.5q33 5 61 5 43 0 85-11-112-23-185.5-111.5t-73.5-205.5v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5t371.5 99.5q-8-38-8-74 0-134 94.5-228.5t228.5-94.5q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div>
        <div class="linkedin" title="Share this on Linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=https://zshn25.github.io/self-supervision-for-foundation-models/&title=&summary=&source=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></svg></div>
        <!-- <div class="pinterest" title="Share this on Pinterest" onclick="window.open('https://pinterest.com/pin/create/button/?url=&media=https://zshn25.github.ioimages/foundation.jpeg&description=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M256 597q0-108 37.5-203.5t103.5-166.5 152-123 185-78 202-26q158 0 294 66.5t221 193.5 85 287q0 96-19 188t-60 177-100 149.5-145 103-189 38.5q-68 0-135-32t-96-88q-10 39-28 112.5t-23.5 95-20.5 71-26 71-32 62.5-46 77.5-62 86.5l-14 5-9-10q-15-157-15-188 0-92 21.5-206.5t66.5-287.5 52-203q-32-65-32-169 0-83 52-156t132-73q61 0 95 40.5t34 102.5q0 66-44 191t-44 187q0 63 45 104.5t109 41.5q55 0 102-25t78.5-68 56-95 38-110.5 20-111 6.5-99.5q0-173-109.5-269.5t-285.5-96.5q-200 0-334 129.5t-134 328.5q0 44 12.5 85t27 65 27 45.5 12.5 30.5q0 28-15 73t-37 45q-2 0-17-3-51-15-90.5-56t-61-94.5-32.5-108-11-106.5z"/></svg></div> -->
        <div class="mail" title="Share this through Email" onclick="window.open('mailto:?&body=https://zshn25.github.io/self-supervision-for-foundation-models/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47h-1472q-66 0-113-47t-47-113v-794q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 100-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38-42.5-30.5q-91-64-262-182.5t-205-142.5q-62-42-117-115.5t-55-136.5q0-78 41.5-130t118.5-52h1472q65 0 112.5 47t47.5 113z"/></svg></div>
    </div>
</div>
  
  <!-- Source: https://stackoverflow.com/questions/25348389/jekyll-and-liquid-show-related-posts-by-amount-of-equal-tags-2 
License: https://creativecommons.org/licenses/by-sa/3.0/-->
<div class="relatedPosts">
      <h3>
        
        
          You might also like
        
        
      </h3>
    

    
    

    
    

    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/pc4consistentdepth/">Pose Constraints for Self-supervised Monocular Depth and Ego-Motion</a>  •  
                        <i class="far fa-calendar-alt"></i> Apr 17, 2023  •  
                        <i class="far fa-clock"></i> 
       1 min read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#self-supervision">#self-supervision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#depth-estimation">#depth-estimation</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/How-Monocular-Depth-Estimation-works/">Self-supervised monocular depth estimation</a>  •  
                        <i class="far fa-calendar-alt"></i> Oct 30, 2022  •  
                        <i class="far fa-clock"></i> 
       8 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/compilation-3d-mesh-resources/">Compilation of 3D mesh models</a>  •  
                        <i class="far fa-calendar-alt"></i> Apr 14, 2021  •  
                        <i class="far fa-clock"></i> 
       2 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-mesh">#3d-mesh</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#datasets">#datasets</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/is-convolution-linear/">Is convolution linear?</a>  •  
                        <i class="far fa-calendar-alt"></i> Mar 11, 2021  •  
                        <i class="far fa-clock"></i> 
       2 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#mathematics">#mathematics</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/ResNet-feature-pyramid-in-Pytorch/">ResNet feature pyramid in Pytorch</a>  •  
                        <i class="far fa-calendar-alt"></i> Feb 9, 2021  •  
                        <i class="far fa-clock"></i> 
       10 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#pytorch">#pytorch</a>
                                
                            
                    </p>
                </article>
                
                
                    
    
    
</div>
  

<nav class="paginate-container"  aria-label="Pagination"></nav>
  <div class="pagination" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
      
        
          <a class="previous_page" style="flex: 1 1 0; width: 45%; padding-right: 0.5em; text-align: right; background-color: transparent;" rel="previous" aria-label="Previous Page" href="/pc4consistentdepth/"><b>Previous:</b> Pose Constraints for Self-supervised Monocular Depth and Ego-Motion
</a>
        
      
      
        <span class="next_page" aria-disabled="true">Next</span>
      
  </div>
</nav><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="zshn25/zshn25.github.io"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/self-supervision-for-foundation-models/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://feedrabbit.com/?url=https://zshn25.github.io/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">© Zeeshan Khan Suri </li>
          <div xmlns:cc="http://creativecommons.org/ns#" > Text under  <a href="http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"></a> unless stated otherwise. Images and other media have their own copyright. </div>
          <li><a class="u-email" href="mailto:zshn25[at]gmail[dot]com">zshn25[at]gmail[dot]com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>I am curious. Therefore I am.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul></div>

  </div>

</footer>
</body>

</html>