<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  
  <meta name="referrer" content="no-referrer"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Pose Constraints for Self-supervised Monocular Depth and Ego-Motion | Curiosity</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Pose Constraints for Self-supervised Monocular Depth and Ego-Motion" />
<meta name="author" content="Zeeshan Khan Suri (DENSO ADAS Engineering Services GmbH)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Enforcing pose network to be consistent improves depth consistency" />
<meta property="og:description" content="Enforcing pose network to be consistent improves depth consistency" />
<link rel="canonical" href="https://zshn25.github.io/pc4consistentdepth/" />
<meta property="og:url" content="https://zshn25.github.io/pc4consistentdepth/" />
<meta property="og:site_name" content="Curiosity" />
<meta property="og:image" content="https://wsrv.nl/?url=https://zshn25.github.io/images/3dreco/out.gif&h=300" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-17T21:21:23-05:00" />
<script type="application/ld+json">
{"url":"https://zshn25.github.io/pc4consistentdepth/","@type":"BlogPosting","headline":"Pose Constraints for Self-supervised Monocular Depth and Ego-Motion","dateModified":"2023-04-17T21:21:23-05:00","datePublished":"2023-04-17T21:21:23-05:00","image":"https://wsrv.nl/?url=https://zshn25.github.io/images/3dreco/out.gif&h=300","mainEntityOfPage":{"@type":"WebPage","@id":"https://zshn25.github.io/pc4consistentdepth/"},"author":{"@type":"Person","name":"Zeeshan Khan Suri (DENSO ADAS Engineering Services GmbH)"},"description":"Enforcing pose network to be consistent improves depth consistency","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://zshn25.github.io/feed.xml" title="Curiosity" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>


<script src="/assets/js/applytheme.js"></script>
<script src="/assets/js/main.js"></script>

<!-- <link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"> -->
<link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"></head>
<body><header class="site-header fixed-top">

  <div class="wrapper"><a class="site-title no-underline hover-grow" rel="author" href="/"> <img src="/images/logo.png" style="max-width:40px;" alt="Curiosity logo"> Curiosity</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about/">About</a>
            <a class="page-link" href="/categories/">Tags</a>
          <span id="nav-switch-theme" class="nav-anchor">
            <span class="nav-theme-icon fas fa-fw" aria-hidden="true" title="Theme"></span>
            <span class="sr-only">Toggle Theme</span>
          </span>
        </div>
      </nav></div>
</header>
    
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">
        
        
          
      
        Pose Constraints for Self-supervised Monocular Depth and Ego-Motion
      
        
        
      </h1>
    <p class="page-description">Enforcing pose network to be consistent improves depth consistency</p><p class="post-meta post-meta-title">
      <i class="far fa-calendar-alt"></i><time class="dt-published" datetime="2023-04-17T21:21:23-05:00" itemprop="datePublished">
        Apr 17, 2023
      </time>• <i class="far fa-user"></i> 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Zeeshan Khan Suri (DENSO ADAS Engineering Services GmbH)</span></span>
       • <i class="far fa-clock"></i> <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#self-supervision">#self-supervision</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#depth-estimation">#depth-estimation</a>
        
      
      </p>
    

    </header>

  
  

  <div class="post-content e-content" itemprop="articleBody">
    
    <p> </p>

<!-- Checkout Bulma buttom elements for Nerfies in custom.styles.scss -->
<div style="text-align: center;">
    <!-- PDF Link. -->
    <span class="link-block">
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-31438-4_23" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="ai ai-springer"></i>
        </span>
        <span>Paper</span>
      </a>
    </span>
    <span class="link-block">
      <a href="https://arxiv.org/abs/2304.08916" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="ai ai-arxiv"></i>
        </span>
        <span>arXiv</span>
      </a>
    </span>
    <!-- Video Link. -->
    <span class="link-block">
      <a href="https://www.youtube.com/watch?v=AN1AGR85N2A"  target="_blank"
          class="external-link button is-normal is-rounded is-dark disabled">
        <span class="icon">
            <i class="fab fa-youtube"></i>
        </span>
        <span>Video</span>
      </a>
    </span>
    <!-- Code Link. -->
    <span class="link-block">
      <a href="https://github.com/zshn25/pc4consistentdepth" target="_blank"
          class="external-link button is-normal is-rounded is-dark"> 
        <span class="icon">
            <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
        </a>
    </span>
    <!-- Dataset Link. -->
    <span class="link-block">
      <a href="https://www.cvlibs.net/datasets/kitti/" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="far fa-images"></i>
        </span>
        <span>Data</span>
        </a>
  </div>

<p> </p>

<p style="text-align: center;"><img src="/images/3dreco/out.gif" alt="teaser" width="100%" class="shadow" /></p>
<p style="text-align: center;"><sub><sup><em>Consistent Depth without any post-processing. <a href="https://www.cvlibs.net/datasets/kitti/index.php">KITTI dataset</a><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">1</a></sup></em>
</sup></sub></p>
      <h2 id="abstract">
        
        
          <a href="#abstract" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Abstract
        
        
      </h2>
    

<p>Self-supervised monocular depth estimation approaches suffer not only from scale ambiguity but also infer temporally inconsistent depth maps w.r.t. scale.</p>

<p style="text-align: center;"><img src="/images/3dreco/Li_Enforcing_Temporal_Consistency_in_Video_Depth_Estimation_ICCVW_2021.png" alt="Enforcing_Temporal_Consistency_in_Video_Depth_Estimation" /></p>
<p style="text-align: center;"><sub><sup><em>Two depth estimation results on four frames showing the inconsistency problem in existing <abbr title="monocular depth estimation">MDE</abbr> methods. Image taken from Li et. al.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">2</a></sup></em>
</sup></sub></p>

<p>While disambiguating scale during training is not possible without some kind of ground truth supervision, having scale consistent depth predictions would make it possible to calculate scale once during inference as a post-processing step and use it over-time. With this as a goal, a set of temporal consistency losses that minimize pose inconsistencies over time are introduced. Evaluations show that introducing these constraints not only reduces depth inconsistencies but also improves the baseline performance of depth and ego-motion prediction.</p>

<p style="text-align: center;"><img src="/images/3dreco/md2seqscales.png" alt="teaser" /></p>
<p style="text-align: center;"><sub><sup><em>Scale factors (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mtext>GT</mtext><mtext>pred</mtext></mfrac></mrow><annotation encoding="application/x-tex">\frac{\text{GT}}{\text{pred}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3534389999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">pred</span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">GT</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>) within each KITTI sequences are highly varying</em>
</sup></sub></p>
      <h2 id="introduction">
        
        
          <a href="#introduction" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Introduction
        
        
      </h2>
    

<p>For an introduction to self-supervised monocular depth estimation, checkout my previous blog post on <a href="/How-Monocular-Depth-Estimation-works/">Self-supervised Monocular Depth Estimation</a></p>

<center width="100%" class="shadow">
<div class="container">
    <iframe src="https://www.youtube-nocookie.com/embed/AN1AGR85N2A" frameborder="0" allowfullscreen="" class="video"></iframe>
</div>
</center><hr />
      <h2 id="bibtex">
        
        
          <a href="#bibtex" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> BibTeX
        
        
      </h2>
    

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{10.1007/978-3-031-31438-4_23,
author="Suri, Zeeshan Khan",
editor="Gade, Rikke and Felsberg, Michael and K{\"a}m{\"a}r{\"a}inen, Joni-Kristian",
title="Pose Constraints for Consistent Self-supervised Monocular Depth and Ego-Motion",
booktitle="Image Analysis",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="340--353",
isbn="978-3-031-31438-4",
doi={10.1007/978-3-031-31438-4_23}
}
</code></pre></div></div>

<!-- Checkout Bulma buttom elements for Nerfies in custom.styles.scss -->
<div style="text-align: center;">
    <!-- PDF Link. -->
    <span class="link-block">
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-31438-4_23" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="ai ai-springer"></i>
        </span>
        <span>Paper</span>
      </a>
    </span>
    <span class="link-block">
      <a href="https://arxiv.org/abs/2304.08916" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="ai ai-arxiv"></i>
        </span>
        <span>arXiv</span>
      </a>
    </span>
    <!-- Video Link. -->
    <span class="link-block">
      <a href="https://www.youtube.com/watch?v=AN1AGR85N2A"  target="_blank"
          class="external-link button is-normal is-rounded is-dark disabled">
        <span class="icon">
            <i class="fab fa-youtube"></i>
        </span>
        <span>Video</span>
      </a>
    </span>
    <!-- Code Link. -->
    <span class="link-block">
      <a href="https://github.com/zshn25/pc4consistentdepth" target="_blank"
          class="external-link button is-normal is-rounded is-dark"> 
        <span class="icon">
            <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
        </a>
    </span>
    <!-- Dataset Link. -->
    <span class="link-block">
      <a href="https://www.cvlibs.net/datasets/kitti/" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="far fa-images"></i>
        </span>
        <span>Data</span>
        </a>
  </div>
      <h1 id="references">
        
        
          <a href="#references" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> References:
        
        
      </h1>
    

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">
      <p>A Geiger, P Lenz, C Stiller, and R Urtasun. 2013. Vision meets robotics: The KITTI dataset. Int. J. Rob. Res. 32, 11 (September 2013), 1231–1237. https://doi.org/10.1177/0278364913491297 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>Li, S., Luo, Y., Zhu, Y., Zhao, X., Li, Y., Shan, Y.: <a href="https://openaccess.thecvf.com/content/ICCV2021W/PBDL/papers/Li_Enforcing_Temporal_Consistency_in_Video_Depth_Estimation_ICCVW_2021_paper.pdf">Enforcing temporal consistency in video depth estimation</a>. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops. pp. 1145–1154 (October 2021) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><hr>
  <!-- This code is taken from Jekyll-Codex (jekyllcodex.org). MIT License
Copyright (c) 2020 Usecue BV -->




<div style="text-align: center;">
    <span style="color: rgb(134, 134, 134);">Share on : </span>
    <div id="share-buttons">
        <div class="facebook" title="Share this on Facebook" onclick="window.open('http://www.facebook.com/share.php?u=https://zshn25.github.io/pc4consistentdepth/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759h-306v-759h-255v-296h255v-218q0-186 104-288.5t277-102.5q147 0 228 12z"/></svg></div>
        <div class="twitter" title="Share this on Twitter" onclick="window.open('https://twitter.com/intent/tweet?text=Pose Constraints for Self-supervised Monocular Depth and Ego-Motion&url=https://zshn25.github.io/pc4consistentdepth/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5t-115.5 248.5-184.5 210.5-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5t-114-159.5q33 5 61 5 43 0 85-11-112-23-185.5-111.5t-73.5-205.5v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5t371.5 99.5q-8-38-8-74 0-134 94.5-228.5t228.5-94.5q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div>
        <div class="linkedin" title="Share this on Linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=https://zshn25.github.io/pc4consistentdepth/&title=&summary=&source=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></svg></div>
        <!-- <div class="pinterest" title="Share this on Pinterest" onclick="window.open('https://pinterest.com/pin/create/button/?url=&media=https://zshn25.github.iohttps://wsrv.nl/?url=https://zshn25.github.io/images/3dreco/out.gif&h=300&description=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M256 597q0-108 37.5-203.5t103.5-166.5 152-123 185-78 202-26q158 0 294 66.5t221 193.5 85 287q0 96-19 188t-60 177-100 149.5-145 103-189 38.5q-68 0-135-32t-96-88q-10 39-28 112.5t-23.5 95-20.5 71-26 71-32 62.5-46 77.5-62 86.5l-14 5-9-10q-15-157-15-188 0-92 21.5-206.5t66.5-287.5 52-203q-32-65-32-169 0-83 52-156t132-73q61 0 95 40.5t34 102.5q0 66-44 191t-44 187q0 63 45 104.5t109 41.5q55 0 102-25t78.5-68 56-95 38-110.5 20-111 6.5-99.5q0-173-109.5-269.5t-285.5-96.5q-200 0-334 129.5t-134 328.5q0 44 12.5 85t27 65 27 45.5 12.5 30.5q0 28-15 73t-37 45q-2 0-17-3-51-15-90.5-56t-61-94.5-32.5-108-11-106.5z"/></svg></div> -->
        <div class="mail" title="Share this through Email" onclick="window.open('mailto:?&body=https://zshn25.github.io/pc4consistentdepth/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47h-1472q-66 0-113-47t-47-113v-794q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 100-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38-42.5-30.5q-91-64-262-182.5t-205-142.5q-62-42-117-115.5t-55-136.5q0-78 41.5-130t118.5-52h1472q65 0 112.5 47t47.5 113z"/></svg></div>
    </div>
</div>
  
  <!-- Source: https://stackoverflow.com/questions/25348389/jekyll-and-liquid-show-related-posts-by-amount-of-equal-tags-2 
License: https://creativecommons.org/licenses/by-sa/3.0/-->
<div class="relatedPosts">
      <h3>
        
        
          You might also like
        
        
      </h3>
    

    
    

    
    

    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/self-supervision-for-foundation-models/">Self-supervised learning for vision foundation models</a>  •  
                        <i class="far fa-calendar-alt"></i> May 11, 2023  •  
                        <i class="far fa-clock"></i> 
       17 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#large-neural-networks">#large-neural-networks</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#foundation-models">#foundation-models</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#self-supervision">#self-supervision</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/CNNs-vs-Transformers/">Convolution vs. Attention</a>  •  
                        <i class="far fa-calendar-alt"></i> Mar 9, 2023  •  
                        <i class="far fa-clock"></i> 
       6 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#convolution">#convolution</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#attention">#attention</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/Software-2/">Software 2.0 2.0</a>  •  
                        <i class="far fa-calendar-alt"></i> Dec 4, 2022  •  
                        <i class="far fa-clock"></i> 
       3 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#mlops">#mlops</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/How-Monocular-Depth-Estimation-works/">Self-supervised monocular depth estimation</a>  •  
                        <i class="far fa-calendar-alt"></i> Oct 30, 2022  •  
                        <i class="far fa-clock"></i> 
       8 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/Layers-fusion-for-faster-inference/">Layers fusion for faster neural network inference</a>  •  
                        <i class="far fa-calendar-alt"></i> Apr 27, 2021  •  
                        <i class="far fa-clock"></i> 
       3 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#pytorch">#pytorch</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#fast-inference">#fast-inference</a>
                                
                            
                    </p>
                </article>
                
                
                    
    
    
</div>
  

<nav class="paginate-container"  aria-label="Pagination"></nav>
  <div class="pagination" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
      
        
          <a class="previous_page" style="flex: 1 1 0; width: 45%; padding-right: 0.5em; text-align: right; background-color: transparent;" rel="previous" aria-label="Previous Page" href="/CNNs-vs-Transformers/"><b>Previous:</b> Convolution vs. Attention
</a>
        
      
      
        
          <a class="next_page" style="flex: 1 1 0; width: 45%; padding-left: 0.5em; text-align: left; background-color: transparent;" rel="next" aria-label="Next Page" href="/self-supervision-for-foundation-models/"><b>Next:</b> Self-supervised learning for vision foundation models
</a>
        
      
  </div>
</nav><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="zshn25/zshn25.github.io"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/pc4consistentdepth/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://feedrabbit.com/?url=https://zshn25.github.io/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">© Zeeshan Khan Suri </li>
          <div xmlns:cc="http://creativecommons.org/ns#" > Text under  <a href="http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"></a> unless stated otherwise. Images and other media have their own copyright. </div>
          <li><a class="u-email" href="mailto:zshn25[at]gmail[dot]com">zshn25[at]gmail[dot]com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>I am curious. Therefore I am.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul></div>

  </div>

</footer>
</body>

</html>