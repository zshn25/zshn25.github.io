<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://zshn25.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://zshn25.github.io/" rel="alternate" type="text/html" /><updated>2023-09-22T09:58:34-05:00</updated><id>https://zshn25.github.io/feed.xml</id><title type="html">Curiosity</title><subtitle>I am curious. Therefore I am.</subtitle><author><name>Zeeshan Khan Suri</name><email>zshn25[at]gmail[dot]com</email></author><entry><title type="html">Self-supervised learning for vision foundation models</title><link href="https://zshn25.github.io/self-supervision-for-foundation-models/" rel="alternate" type="text/html" title="Self-supervised learning for vision foundation models" /><published>2023-05-11T22:21:23-05:00</published><updated>2023-05-11T22:21:23-05:00</updated><id>https://zshn25.github.io/self-supervision-for-foundation-models</id><author><name>Zeeshan Khan Suri</name></author><category term="computer-vision" /><category term="large-neural-networks" /><category term="foundation-models" /><category term="self-supervision" /><summary type="html">Performance of deep neural networks scales with data and model size</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://zshn25.github.io/images/foundation.jpeg" /><media:content medium="image" url="https://zshn25.github.io/images/foundation.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Pose Constraints for Self-supervised Monocular Depth and Ego-Motion</title><link href="https://zshn25.github.io/pc4consistentdepth/" rel="alternate" type="text/html" title="Pose Constraints for Self-supervised Monocular Depth and Ego-Motion" /><published>2023-04-17T21:21:23-05:00</published><updated>2023-04-17T21:21:23-05:00</updated><id>https://zshn25.github.io/pc4consistentdepth</id><author><name>Zeeshan Khan Suri (DENSO ADAS Engineering Services GmbH)</name></author><category term="deep-learning" /><category term="self-supervision" /><category term="3d-reconstruction" /><category term="structure-from-motion" /><category term="visual-odometry" /><category term="depth-estimation" /><summary type="html"> </summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://wsrv.nl/?url={{site.baseurl}}/images/3dreco/out.gif&amp;w=300&amp;h=300" /><media:content medium="image" url="https://wsrv.nl/?url={{site.baseurl}}/images/3dreco/out.gif&amp;w=300&amp;h=300" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Convolution vs. Attention</title><link href="https://zshn25.github.io/CNNs-vs-Transformers/" rel="alternate" type="text/html" title="Convolution vs. Attention" /><published>2023-03-09T20:21:23-06:00</published><updated>2023-03-09T20:21:23-06:00</updated><id>https://zshn25.github.io/CNNs-vs-Transformers</id><author><name>Zeeshan Khan Suri</name></author><category term="deep-learning" /><category term="convolution" /><category term="attention" /><summary type="html">Layers in neural netowrks can be seen as a function that takes in a multi-dimensional input and produces an output. For simplicity, let’s assume the input and output dimesnions to be the same.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/320px-Vision_Transformer.gif" /><media:content medium="image" url="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/320px-Vision_Transformer.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Software 2.0 2.0</title><link href="https://zshn25.github.io/Software-2/" rel="alternate" type="text/html" title="Software 2.0 2.0" /><published>2022-12-04T21:21:23-06:00</published><updated>2022-12-04T21:21:23-06:00</updated><id>https://zshn25.github.io/Software-2</id><author><name>Zeeshan Khan Suri</name></author><category term="deep-learning" /><category term="mlops" /><summary type="html">This post is a rebuttal on Andrej Karpathy’s Software 2.0. It is brilliant and you should definitely read it but I thought the critical takeaway was lost in the many great points and I wanted to make that more explicit.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://zshn25.github.io/images/loop.svg" /><media:content medium="image" url="https://zshn25.github.io/images/loop.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Self-supervised monocular depth estimation</title><link href="https://zshn25.github.io/How-Monocular-Depth-Estimation-works/" rel="alternate" type="text/html" title="Self-supervised monocular depth estimation" /><published>2022-10-30T21:21:23-05:00</published><updated>2022-10-30T21:21:23-05:00</updated><id>https://zshn25.github.io/How-Monocular-Depth-Estimation-works</id><author><name>Zeeshan Khan Suri</name></author><category term="deep-learning" /><category term="computer-vision" /><category term="3d-reconstruction" /><category term="structure-from-motion" /><category term="visual-odometry" /><summary type="html">Animals (and in extension, humans) are unable to directly perceive the 3D surroundings around us. Each of our eyes projects the 3D world onto 2D, losing the depth dimension. Instead, we rely on our brain to reconstruct these 2D projections to perceive depth. Having more than one eye allows us to geometrically reconstruct depth via triangulation1, but how are creatures with a single eye (for e.g., due to a defective eye, or due to a birth disorder2) still able to perceive it? Triangulation &amp;#8617; Cyclopia &amp;#8617;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/2551b04d-fd4e-4ec9-9869-3e8c9ac5e7bf/d93a0lc-e4ecbd7d-6120-4925-be1d-1e250d7e0830.png/v1/fill/w_1024,h_576,q_80,strp/cyclops_greek_mythology_by_nilesdino_d93a0lc-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NTc2IiwicGF0aCI6IlwvZlwvMjU1MWIwNGQtZmQ0ZS00ZWM5LTk4NjktM2U4YzlhYzVlN2JmXC9kOTNhMGxjLWU0ZWNiZDdkLTYxMjAtNDkyNS1iZTFkLTFlMjUwZDdlMDgzMC5wbmciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.kEfvRjpv5KXW_HtOAtsBltiTNTF7DswBz8TwLvRVwyo" /><media:content medium="image" url="https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/2551b04d-fd4e-4ec9-9869-3e8c9ac5e7bf/d93a0lc-e4ecbd7d-6120-4925-be1d-1e250d7e0830.png/v1/fill/w_1024,h_576,q_80,strp/cyclops_greek_mythology_by_nilesdino_d93a0lc-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NTc2IiwicGF0aCI6IlwvZlwvMjU1MWIwNGQtZmQ0ZS00ZWM5LTk4NjktM2U4YzlhYzVlN2JmXC9kOTNhMGxjLWU0ZWNiZDdkLTYxMjAtNDkyNS1iZTFkLTFlMjUwZDdlMDgzMC5wbmciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.kEfvRjpv5KXW_HtOAtsBltiTNTF7DswBz8TwLvRVwyo" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>