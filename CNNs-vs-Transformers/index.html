<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  
  <meta name="referrer" content="no-referrer"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Convolution vs. Attention | Curiosity</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Convolution vs. Attention" />
<meta name="author" content="Zeeshan Khan Suri" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Similarities and differences between convolutions and attention module of Transformers" />
<meta property="og:description" content="Similarities and differences between convolutions and attention module of Transformers" />
<link rel="canonical" href="https://zshn25.github.io/CNNs-vs-Transformers/" />
<meta property="og:url" content="https://zshn25.github.io/CNNs-vs-Transformers/" />
<meta property="og:site_name" content="Curiosity" />
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/320px-Vision_Transformer.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-03-09T20:21:23-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/320px-Vision_Transformer.gif" />
<meta property="twitter:title" content="Convolution vs. Attention" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Zeeshan Khan Suri"},"dateModified":"2023-03-09T20:21:23-06:00","datePublished":"2023-03-09T20:21:23-06:00","description":"Similarities and differences between convolutions and attention module of Transformers","headline":"Convolution vs. Attention","image":"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/320px-Vision_Transformer.gif","mainEntityOfPage":{"@type":"WebPage","@id":"https://zshn25.github.io/CNNs-vs-Transformers/"},"url":"https://zshn25.github.io/CNNs-vs-Transformers/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://zshn25.github.io/feed.xml" title="Curiosity" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>


<script src="/assets/js/applytheme.js"></script>
<script src="/assets/js/main.js"></script>

<!-- <link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"> -->
<link id="dark-css" href="/assets/css/dark.scss" rel="stylesheet" media="(prefers-color-scheme: dark)"></head>
<body><header class="site-header fixed-top">

  <div class="wrapper"><a class="site-title no-underline hover-grow" rel="author" href="/"> <img src="/images/logo.png" style="max-width:40px;" alt="Curiosity logo"> Curiosity</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about/">About</a>
            <a class="page-link" href="/categories/">Tags</a>
          <span id="nav-switch-theme" class="nav-anchor">
            <span class="nav-theme-icon fas fa-fw" aria-hidden="true" title="Theme"></span>
            <span class="sr-only">Toggle Theme</span>
          </span>
        </div>
      </nav></div>
</header>
    
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">
        
        
          
      
        Convolution vs. Attention
      
        
        
      </h1>
    <p class="page-description">Similarities and differences between convolutions and attention module of Transformers</p><p class="post-meta post-meta-title">
      <i class="far fa-calendar-alt"></i><time class="dt-published" datetime="2023-03-09T20:21:23-06:00" itemprop="datePublished">
        Mar 9, 2023
      </time>• <i class="far fa-user"></i> 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Zeeshan Khan Suri</span></span>
       • <i class="far fa-clock"></i> <span class="read-time" title="Estimated read time">
    
    
      6 mins read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#convolution">#convolution</a>
        &nbsp;
      
        <a class="category-tags-link label label-default" href="/categories/#attention">#attention</a>
        
      
      </p>
    

    </header>

  
  

  <div class="post-content e-content" itemprop="articleBody">
    
    <p>Layers in neural netowrks can be seen as a function that takes in a multi-dimensional input and produces an output. For simplicity, let’s assume the input and output dimesnions to be the same.</p>
      <h2 id="fully-connected-layer">
        
        
          <a href="#fully-connected-layer" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Fully-connected Layer
        
        
      </h2>
    

<p>In a fully connected layer each output is just a linear combination of all the inputs.</p>

<p style="text-align: center;">$\textcolor{FF7800}{\textbf{y}} =  \textcolor{9966FF}{W} \textcolor{2EC27E}{\textbf{x}} $</p>

<p style="text-align: center;"><img src="/images/fullyconnected.svg" alt="fullyconnected" /></p>
<p style="text-align: center;"><sub><sup><em>In a fully connected layer, each output depends on (and is connected to) all inputs</em>
</sup></sub></p>

<p>In fact, in Pytorch, a fully-connected layer is just represented by the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear">linear layer</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>The weights matrix $\textcolor{9966FF}{W}$ is the learnable parameter which learns to assign weights of each output to all of the inputs.</p>

<p>The problem with the fully-connected layer is that it consumes a lot of learnable parameters. Since each output is connected to all inputs, a lot of the weights have to be learnt. But, for most input types it is wasteful to connect to all inputs since dependencies are often local, for e.g. spatio-temporal neighbors, than global.</p>

<p>In principle, connecting to all inputs shouldn’t be a problem since given enough training samples and enough time to train, the network should be able to learn to assign meaningful (and sparse) weights to some locations (like the spatio-temporal neighbors) than to the rest, which could be exploited by post-processing steps such as Pruning to fasten inference.</p>

<p>But, especially for data such as images where we know the inherent local dependencies of pixels to their spatial neighbors, it is a good idea to limit the layer to such neighborhood.</p>
      <h2 id="convolution">
        
        
          <a href="#convolution" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Convolution
        
        
      </h2>
    

<p>Convolutional neural networks (CNNs) are typically used for <strong>spatial</strong> data processing, such as images, where there is a spatial relationship between the data or temporal data such as audio. For example, neighboring pixels (in X or Y direction) are related to each other. A convolutional filter is applied to such data to extract features, such as edges, textures, etc., in images.</p>

<p style="text-align: center;"><img src="/images/convolution.svg" alt="convolution" /></p>
<p style="text-align: center;"><sub><sup><em>A convolutional layer only attends at it’s neighbors</em>
</sup></sub></p>
      <h2 id="attention">
        
        
          <a href="#attention" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Attention
        
        
      </h2>
    

<p>Transformers on the other hand are typically used for <strong>sequential</strong> data processing, such as text, natural language, where short-term and long-term dependencies are present. The actual dependencies are not explicit in this case. For example, in the sentence “Alice had gone to the supermarket to meet Bob”, one of the verb “meet”, is located far-away from the subject “Alice” and this dependency is not spatial but differs a lot. This is even more for longer inputs with multiple paragraphs where the final sentence could have had a dependecy to a sentence somewhere in the beginning. Transformers are based on the so called attention mechanisms which learns these relationships between the elements in the sequence.</p>

<p style="text-align: center;"><a href="https://commons.wikimedia.org/wiki/File:Vision_Transformer.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Vision_Transformer.gif" alt="Odontodactylus scyllarus eyes" width="100%" class="shadow" /></a></p>
<p style="text-align: center;"><sub><sup><em>Attentions is the key component of Transformers and have been successfully applied to image data. <a href="https://commons.wikimedia.org/wiki/File:Vision_Transformer.gif">Davide Coccomini</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons</em>
</sup></sub></p>
      <h3 id="basic-self-attention">
        
        
          <a href="#basic-self-attention" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Basic Self-attention
        
        
      </h3>
    

<p>The basic idea of self-attention is to assign different importance to the inputs based on the inputs themselves. In comparison to convolution, self-attention allows the receptive field to be the entire spatial locations.</p>

<p style="text-align: center;"><img src="/images/selfattention.svg" alt="attention" /></p>
<p style="text-align: center;"><sub><sup><em>An attention layer assigns importance to the inputs based on the inputs themselves</em>
</sup></sub></p>

<p>The weights are computed based on cosine similarity of the inputs $\textcolor{2EC27E}{\textbf{x}}$ to themselves</p>

<p style="text-align: center;"><img src="/images/xxt.svg" alt="attention" /></p>
<p style="text-align: center;"><sub><sup><em>$\textcolor{9966FF}{W} =  \textcolor{2EC27E}{\textbf{x}}  \textcolor{2EC27E}{\textbf{x}^T}$</em>
</sup></sub></p>

<div class="Toast Toast--warning googoo" role="alert">
   <span aria-hidden="true" class="Toast-icon octicon octicon-alert"><svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg></span>
   <span class="Toast-content">Input $\textcolor{2EC27E}{\textbf{x}^T}$ and its transforms are usually multi-dimensional. Vector notation is used for illustration</span>
</div>
      <h3 id="self-attention">
        
        
          <a href="#self-attention" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Self-attention
        
        
      </h3>
    

<p>Note that the basic version of self-attention does not include any learnable parameters. For this reason, the “Attention is all you need”<sup>[</sup><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup>]</sup> variation of the self-attention includes 3 learnable weight matrices (key $\textcolor{9966FF}{W_K}$, query $\textcolor{9966FF}{W_Q}$ and value $\textcolor{9966FF}{W_V}$). But, the basic principle remains the same. The key $\textcolor{9966FF}{W_K}$ and query $\textcolor{9966FF}{W_Q}$ matrices are used to transform the input into key $\textcolor{2EC27E}{\textbf{k}} = \textcolor{9966FF}{W_K} \textcolor{2EC27E}{\textbf{x}}$, and query $\textcolor{2EC27E}{\textbf{q}} = \textcolor{9966FF}{W_Q} \textcolor{2EC27E}{\textbf{x}}$, whose similarity $\textcolor{2EC27E}{\textbf{q}} \textcolor{2EC27E}{\textbf{k}^\mathrm{T}}$,  weighs the output (value), $\textcolor{FF7800}{\textbf{v}} =  \textcolor{9966FF}{W_V} \textcolor{2EC27E}{\textbf{x}} $.<sup>[</sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup><sup>]</sup></p>

<p style="text-align: center;">$\textcolor{FF7800}{\textbf{y}} = \text{softmax}\left(\frac{\textcolor{2EC27E}{\textbf{q}} \textcolor{2EC27E}{\textbf{k}^\mathrm{T}}}{\sqrt{d_k}}\right)\textcolor{FF7800}{\textbf{v}}$</p>

<p>Also note that in the basic version, the self-similarity of the inputs always causes the diagonal to be of the highest similarity and makes the weight matrix symmetric. This problem is also elevated by transforming the same input using two seperate learnable weight matrices, $\textcolor{9966FF}{W_K}$ and $\textcolor{9966FF}{W_Q}$</p>
      <h2 id="convolution-vs-attention-which-is-better">
        
        
          <a href="#convolution-vs-attention-which-is-better" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Convolution vs. Attention: Which is better?
        
        
      </h2>
    

<p>Although attention based models such as vision transformers have shown to outperform CNN based methods, a careful analysis of the two shows comparible performance.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup></p>

<p>In early layers of a neural network for images, spatial relations can be captured by convolutions and the later layers could benifit from long-range receptive fields offered by attention. Hence, both can be combined.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup></p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Am I going to argue that &quot;Conv is all you need&quot;?<br />No!<br />My favorite architecture is DETR-like: ConvNet (or ConvNeXt) for the first layers, then something more memory-based and permutation invariant like transformer blocks for object-based reasoning on top.<a href="https://t.co/LA2J72N93A">https://t.co/LA2J72N93A</a></p>&mdash; Yann LeCun (@ylecun) <a href="https://twitter.com/ylecun/status/1481198016266739715?ref_src=twsrc%5Etfw">January 12, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      <h2 id="summary">
        
        
          <a href="#summary" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Summary
        
        
      </h2>
    

<ul>
  <li>
    <p>Different ways of connecting inputs to each other were discussed. A fully-connected layer connects all inputs to each other. This leads to exponential increase in network parameters and computational complexity. While the network can learn to assign different weights, this can take a lot of data and prolonged training.</p>
  </li>
  <li>
    <p>Convolutional layer incorporates desirable inductive biases about the data to reduce computation and connects only to the neighbors. Spatial and temporal data benifits from doing so. Convolution is translation invariant. However, the dimensions of outputs of a convolution depend on the input dimensions.</p>
  </li>
  <li>
    <p>A self-attention layer assigns importance to inputs based on their similarity. For e.g., in the sentence “Alice is adventurous and she is in wonderland.” the word “she” refers to “Alice” and ideally, their embeddings should be similar, which can be used by the self-attention layer to determine contexts. Similar to fully-connected, far away connections can be established if the input features or embeddings are similar. However, not having enough data may lead to overfitting the inputs.</p>
  </li>
  <li>
    <p>In early layers of a neural network for images, spatial relations can be captured by convolutions and the later layers could benifit from long-range receptive fields offered by attention. Hence, both can be combined. Works such as CoAtNet<sup>[</sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">5</a></sup><sup>]</sup> offer layers combining the two.</p>
  </li>
</ul>
      <h2 id="references">
        
        
          <a href="#references" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> References
        
        
      </h2>
    
      <h2 id="further-readings-and-videos">
        
        
          <a href="#further-readings-and-videos" class="heading-anchor" aria-hidden="true" tabindex="-1"><svg class='octicon octicon-heading-anchor' viewBox='0 0 16 16' version='1.1' width='16' height='32' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg></a> Further Readings and Videos
        
        
      </h2>
    

<ul>
  <li><a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0</a></li>
  <li><a href="https://aman.ai/primers/ai/transformers/">https://aman.ai/primers/ai/transformers/</a></li>
  <li>https://www.youtube.com/watch?v=KmAISyVvE1Y&amp;list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV&amp;index=1</li>
</ul>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). <a href="https://dl.acm.org/doi/10.5555/3295222.3295349">Attention is all you need</a>. Advances in neural information processing systems, 30. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#Scaled_dot-product_attention">https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#Scaled_dot-product_attention</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Z. Liu, H. Mao, C. -Y. Wu, C. Feichtenhofer, T. Darrell and S. Xie, “<a href="https://arxiv.org/abs/2201.03545">A ConvNet for the 2020s</a>,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 11966-11976, doi: 10.1109/CVPR52688.2022.01167. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., &amp; Girshick, R.B. (2021). <a href="https://proceedings.neurips.cc/paper/2021/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf">Early Convolutions Help Transformers See Better</a>. Neural Information Processing Systems. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Dai, Z., Liu, H., Le, Q. V., &amp; Tan, M. (2021). <a href="https://papers.nips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html">Coatnet: Marrying convolution and attention for all data sizes</a>. Advances in Neural Information Processing Systems, 34, 3965-3977. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><hr>
  <!-- This code is taken from Jekyll-Codex (jekyllcodex.org). MIT License
Copyright (c) 2020 Usecue BV -->




<div style="text-align: center;">
    <span style="color: rgb(134, 134, 134);">Share on : </span>
    <div id="share-buttons">
        <div class="facebook" title="Share this on Facebook" onclick="window.open('http://www.facebook.com/share.php?u=https://zshn25.github.io/CNNs-vs-Transformers/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759h-306v-759h-255v-296h255v-218q0-186 104-288.5t277-102.5q147 0 228 12z"/></svg></div>
        <div class="twitter" title="Share this on Twitter" onclick="window.open('https://twitter.com/intent/tweet?text=Convolution vs. Attention&url=https://zshn25.github.io/CNNs-vs-Transformers/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5t-115.5 248.5-184.5 210.5-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5t-114-159.5q33 5 61 5 43 0 85-11-112-23-185.5-111.5t-73.5-205.5v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5t371.5 99.5q-8-38-8-74 0-134 94.5-228.5t228.5-94.5q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div>
        <div class="linkedin" title="Share this on Linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=https://zshn25.github.io/CNNs-vs-Transformers/&title=&summary=&source=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></svg></div>
        <!-- <div class="pinterest" title="Share this on Pinterest" onclick="window.open('https://pinterest.com/pin/create/button/?url=&media=https://zshn25.github.iohttps://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/320px-Vision_Transformer.gif&description=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M256 597q0-108 37.5-203.5t103.5-166.5 152-123 185-78 202-26q158 0 294 66.5t221 193.5 85 287q0 96-19 188t-60 177-100 149.5-145 103-189 38.5q-68 0-135-32t-96-88q-10 39-28 112.5t-23.5 95-20.5 71-26 71-32 62.5-46 77.5-62 86.5l-14 5-9-10q-15-157-15-188 0-92 21.5-206.5t66.5-287.5 52-203q-32-65-32-169 0-83 52-156t132-73q61 0 95 40.5t34 102.5q0 66-44 191t-44 187q0 63 45 104.5t109 41.5q55 0 102-25t78.5-68 56-95 38-110.5 20-111 6.5-99.5q0-173-109.5-269.5t-285.5-96.5q-200 0-334 129.5t-134 328.5q0 44 12.5 85t27 65 27 45.5 12.5 30.5q0 28-15 73t-37 45q-2 0-17-3-51-15-90.5-56t-61-94.5-32.5-108-11-106.5z"/></svg></div> -->
        <div class="mail" title="Share this through Email" onclick="window.open('mailto:?&body=https://zshn25.github.io/CNNs-vs-Transformers/');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47h-1472q-66 0-113-47t-47-113v-794q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 100-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38-42.5-30.5q-91-64-262-182.5t-205-142.5q-62-42-117-115.5t-55-136.5q0-78 41.5-130t118.5-52h1472q65 0 112.5 47t47.5 113z"/></svg></div>
    </div>
</div>
  
  <!-- Source: https://stackoverflow.com/questions/25348389/jekyll-and-liquid-show-related-posts-by-amount-of-equal-tags-2 
License: https://creativecommons.org/licenses/by-sa/3.0/-->
<div class="relatedPosts">
      <h3>
        
        
          You might also like
        
        
      </h3>
    

    
    

    
    

    
        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/pc4consistentdepth/">Pose Constraints for Self-supervised Monocular Depth and Ego-Motion</a>  •  
                        <i class="far fa-calendar-alt"></i> Apr 17, 2023  •  
                        <i class="far fa-clock"></i> 
       1 min read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#self-supervision">#self-supervision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#depth-estimation">#depth-estimation</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/Software-2/">Software 2.0 2.0</a>  •  
                        <i class="far fa-calendar-alt"></i> Dec 4, 2022  •  
                        <i class="far fa-clock"></i> 
       3 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#mlops">#mlops</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/How-Monocular-Depth-Estimation-works/">Self-supervised monocular depth estimation</a>  •  
                        <i class="far fa-calendar-alt"></i> Oct 30, 2022  •  
                        <i class="far fa-clock"></i> 
       8 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#3d-reconstruction">#3d-reconstruction</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#structure-from-motion">#structure-from-motion</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#visual-odometry">#visual-odometry</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/Layers-fusion-for-faster-inference/">Layers fusion for faster neural network inference</a>  •  
                        <i class="far fa-calendar-alt"></i> Apr 27, 2021  •  
                        <i class="far fa-clock"></i> 
       3 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#pytorch">#pytorch</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#fast-inference">#fast-inference</a>
                                
                            
                    </p>
                </article>
                
                
            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            

        
    
        

            
            

            

            
                <article class="archive-item">
                    <p class="post-meta"><a class="post-meta-title" href="/ResNet-feature-pyramid-in-Pytorch/">ResNet feature pyramid in Pytorch</a>  •  
                        <i class="far fa-calendar-alt"></i> Feb 9, 2021  •  
                        <i class="far fa-clock"></i> 
       10 mins read   •  
                        <i class="fas fa-tags category-tags-icon"></i>
                            
                                <a class="category-tags-link label label-default" href="/categories/#deep-learning">#deep-learning</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#computer-vision">#computer-vision</a>
                                &nbsp;
                            
                                <a class="category-tags-link label label-default" href="/categories/#pytorch">#pytorch</a>
                                
                            
                    </p>
                </article>
                
                
                    
    
    
</div>
  

<nav class="paginate-container"  aria-label="Pagination"></nav>
  <div class="pagination" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
      
        
          <a class="previous_page" style="flex: 1 1 0; width: 45%; padding-right: 0.5em; text-align: right; background-color: transparent;" rel="previous" aria-label="Previous Page" href="/Software-2/"><b>Previous:</b> Software 2.0 2.0
</a>
        
      
      
        
          <a class="next_page" style="flex: 1 1 0; width: 45%; padding-left: 0.5em; text-align: left; background-color: transparent;" rel="next" aria-label="Next Page" href="/pc4consistentdepth/"><b>Next:</b> Pose Constraints for Self-supervised Monocular Depth and Ego-Motion
</a>
        
      
  </div>
</nav><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="zshn25/zshn25.github.io"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/CNNs-vs-Transformers/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://feedrabbit.com/?url=https://zshn25.github.io/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">© Zeeshan Khan Suri </li>
          <div xmlns:cc="http://creativecommons.org/ns#" > Text under  <a href="http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"></a> unless stated otherwise. Images and other media have their own copyright. </div>
          <li><a class="u-email" href="mailto:zshn25[at]gmail[dot]com">zshn25[at]gmail[dot]com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>I am curious. Therefore I am.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul></div>

  </div>

</footer>
</body>

</html>