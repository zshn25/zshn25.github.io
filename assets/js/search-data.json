{
  
    
        "post0": {
            "title": "Self-supervised learning for vision foundation models",
            "content": "Performance of deep neural networks scales with data and model size . Model size of notable AI systems over time. Source: Our World in Data . Recent developments in deep learning can be attributed to the growing amount of data and model size. This realization has lead to the rapidly developing field of foundational models, i.e. large scale, complex neural networks, for e.g. GPT 4, DINO v2, ImageBind, etc., trained on very large, diverse datasets. By feeding-in huge amounts of high quality labeled data through large artificial neural network models, machines are shown to be taught to outperform humans at multiple tasks. . . As the training data increases, the performance of a DL model increases but traditional methods do not depend on the training data . But, collecting such massive amounts of carefully labeled data costs enormous time, effort and money; and are error prone. For e.g. Northcutt et al.1, and https://labelerrors.com/ outline errors in the labels of widely used datasets. What if there was a way to use massively available but unlabled data? . In this post, I motivate the necessity for large, general-purpose data representation models, known as foundation models, and in what ways self-supervised learning enables this achievement. Stay tuned! . What are foundation models? . Behind the fancy wording2, the idea is to learn a generic model that produces good input representations, is robust to variety of inputs, and can be directly used for multiple other tasks without the need for re-training. Any model that is capable of doing so can be named a foundation model. . “A foundation model is any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks” . Bommasani, Rishi, et al.3 . The way to realize such a model is to use vast quantities of data and the way to do so is by self-supervised learning. The scale and diversity of training data is what makes a model foundational. Also, training on more data leads to generalization and robustness to distribution shifts and adversarial samples2 . History: Feature representations of images . Tradition computer vision methods extract distinctive pixel-level features from input images. High-dimensional representations of these features, known as feature descriptors are computed from the detected local features which are then used for various applications, such as tracking, retrieval, structure from motion, localization, etc. The goal of transforming the detected features into high-dimensional descriptors is to embed some notion of semantic and neighboring information to the pixel-level features. The goal is to make them distinctive (good for matching across different viewpoints) and invariant to vairations in 3D viewpoint, illumination, etc. . Features of good features . . Good features must be robust against 3D viewpoint, illumination, scaling, etc. Image from Aachen Day-Night Dataset CC BY-NC-SA 4.0, Hierarchical-Localization . To work robustly in real world scenarios, these feature descriptors must be invariant against view-point changes (rotation and translation of camera in 3D), scaling, illumination changes, and other adverse conditions. As seen in the above example image, such features of good features make matching under varying conditions possible, making them highly useful. . Learning features . With the rise of deep learning, features and their descriptors are no longer hand-engineered, but learnt from the underlying data. Vision models such as CNNs and transformers process input images to extract high-dimensional latent representations, which could be thought as features. Unlike pixel-level features such as SIFT, ORB, latents are not local (do not correspond to pixel locations) but based on the semantics of the global image contents. . Checkout my blog post on how to extract feature pyramids from torchvison models . . . Image from neerajkrbansal1996, CC0, via Wikimedia Commons* . As early as in 2009, the idea to use self-supervised learning for . The need for foundation models . If you’re thinking, if existing models already extract features, why is there a need for foundation models? . You are absolutely right. Existing models such as YOLO4 object detection, extract features, which are further passed to the object-detection head. But, there are two reasons where foundation models come into play. . The features extracted from Yolo are task-specific. Features that are useful for object-detection, might not be useful or optimal for other tasks, for example, for depth estimation. We want our ideal features to be task-agnostic. The reasoning behind this is the underlying assumption that there exist features that are useful for all kinds of tasks, just as we saw in the traditional feature detectors and descriptors like SIFT. | We want to train on lots of data. But, supervised tasks such as object detection require expensive labeled data, and thus are infeasible to scale. | . The features of a network trained on a pretext task, for e.g., depth estimation, might not necessarily be useful or optimal for other tasks, for e.g., classification, as depicted in green. Cat image by EVG Kowalievska. . Self-supervised learning . Animals, including humans, learn a many concepts such as object permanance, etc., without explicit supervision. . Self-supervised is where the supervision signal comes from data itself, unlike supervised learning, where supervision comes from explicit human labels and unsupervised learning, where there is no supervision. . Is it possible to make machines learn general concepts without explicit supervision? If so, how? . If we are able formulate pretext tasks, we can use self-supervised learning to learn good data representations and general concepts from massively-available, unlabelled data, we can use such models directly (without fine-tuning, Zero-Shot) for multiple downstream tasks. (atleast, that’s the hope). . . Implicit feature representations are learned via pretext tasks and can be used as input for specific downstream tasks. Image from Fgpacini, CC BY-SA 4.0, via Wikimedia Commons* . Pretext tasks for images . Any task that doesn’t require explitcit labels can be used as a pretext task. The goal is to be able to make use of as much data as possible. This task is not actually the task what the model would finally be required to do, but this task would allow to use much more data via self-supervised learning. . For a more comprehensive list of self-supervised pretext tasks, checkout this page from Papers with Code, and this curated list of research in self-supervised learning . Prediction/Generation and Similarity are the two main concepts that can be exploited to design self-supervised pretext tasks. . Generative pretext tasks based on restoration . These methods involve artificially manipulating the input and teaching the network to undo the manipulation. The hope is that in order to restore large a variety of training data, the network learns robust image representations. Examples of such input manipulations include masking out image regions5, adding Gaussian noise6, reordering/permuting patches as a Jigsaw puzzle7, transformations of the image, such as rotation, and viewpoint change8, removing color9, etc. There is no explicit need to manipulate the original image, for e.g. a GAN was used to regenerate the input image from compressed latent features10. . . Examples of image degradations: masking, noising, permuting, rotating, grey-scaling. The pretext task is to recover the original image. . Note: Tasks such as Jigsaw puzzle and transformations of the image are not exactly generative. But, I still like to generalize them as generative. Only that the generation is parameterized to a low dimension, for e.g. classification on hash table for Jigsaw puzzle and regression of angles for rotation. . The difficulty in these methods is the parameterization of degradation is dependent on the exact degradation method, making it hard to combine with other manipulations. For example, the parameters of adding gaussian noise are the mean and standard deviation of the gaussian noise function, which can be estimated by the network to subtract and recover the original image. For rotation, this would be the angle, which means there should now be another head to predict the angle if these both are to be combined. . Feature similarity as a pretext task . Rather than having an explicit task, the idea is to maximize similarity of features of similar images. This simply means, collecting data samples which represent similar information and having a loss that minimizes distance between their feature representations. . Just doing this causes the feature representations to collapse, i.e., all data just points to the same representation. To prevent this, negative samples (having distict representation) are introduced, which act as a relative contrast to the positive samples (having similar representation). . Contrastive learning . . Contrastive learning example: Cat and it’s augmentations (positive samples), shown by green arrows, must have similar feature representations than the Colosseum image chosen at random (negative sample). . One of the most influential self-supervised techniques is contrastive learning which, as the name implies, uses not only positive samples but also contrasts them with negative samples. The figure above outlines an example of contrastive learning. The positive examples are shown by green arrows are images sampled from an image with different augmentations, such as rotation, cropping, color, etc. Another image (Colosseum) is chosen at random which acts as the negative sample. The fundamental assumption of contrastive learning is that features of the positive samples lie closer to each other (positive samples attract) than to those of a negative sample (positive-negative samples repel). This is realized by minimizing the distance between positive samples while maximizing the distance between negative samples in feature space. . Data sampling of the positives and the negatives becomes the key here and different ways to sample them could be incorporated. Multi-modal information or pseudo-labels, such as associated text, audio, other sensor information, could be used to sample negatives and positives. SimCLR11, for example uses various augmentations of the same image and CMC uses various sensor information of the underlying scene for sampling positives. . One could also sample other images as positives based on methods such as clustering in the feature space or if some kind of labels or GT is known. . Feature similarity vs. Generative pretext tasks . The advantage of feature similarity based methods is that there is no need for exact pixel to pixel mapping of the input and output, as opposed to generative pretext tasks, where the restored output is a one-to-one mapping of the manipulated/distorted input. One could use non-exact data (for example neighboring frames of a video) for sampling feature learning. One could also combine various augmentation methods mentioned in generaive pretext tasks. Feature similarity would essentially be a way to automatically incorporate various handcrafted ad-hoc heuristics via augmentations and positive-negative sampling. . Recent work titled “What Do Self-Supervised Vision Transformers Learn?” compares contrastive learning with a prominant generative pretext task called Masked Image Modeling (MIM)5 and finds: . CL plays a significant role in the later layers of the ViT model, capturing longer-range global patterns, such as the shape of an object. However, it also leads to reduced diversity of representations, thereby worsening scalability and dense prediction performance. . | MIM utilizes high-frequency signals of the representations and mainly focuses on the early layers of the ViTs and is more texture-oriented. . | Other pretext tasks . The generative pretext tasks can be combined with contrastive pretext tasks. For e.g., Learning to See by Looking at Noise combines the generative and feature similarity by generating synthetic images from random processes and having a contrastive loss between crops of synthetic images and real ones. . Applications . The learnt features can be applied to a huge range of downstream tasks. Generic tasks include image classification, object detection, segmentation, depth estimation, etc. Here, I will mention some less-known but very interesting downstream applications. . . Local regions grouped together after applying K-means on contrastively learnt features. Source: Intriguing Properties of Contrastive Losses12 . Semantic Similarity Deep Features as a Perceptual Metric13 . Socretic models https://socraticmodels.github.io/ . {In-context learning} enables users to provide a query and a few examples from which a model derives an answer without being trained on such queries. https://www.semanticscholar.org/paper/Foundation-models-in-brief%3A-A-historical%2C-focus-Schneider/775b2dc88cf04993f8596332444a906bec2db807 . Risks, Challanges . {homogenization} of models might replace a myriad of task-specific models with fewer very large models controlled by few corporations leading to a shift in power and control over AI . | Modelling pretext tasks involves manual selection of data augmentation techniques which is based on our expert heuristics and introduces human biases into the model. Best would be to design methods that just take a bunch of data and decide for themselves which ones are similar, which ones are not. Like real unsupervised learning. But, that is difficult to design. . | Most research is done on curated object-centric datasets, where the whole image consists of a single object instance. While there is some evidence that it also works for multiple objects12, further investigation is needed to check how good the techniques work for generic scenes. Works such as DetCon14 and it’s follow-up: ODIN15, work towards this goal by relying on pre-processing the whole image into object centric ones. . | Mining positive and negative samples for contrastive learning is crucial for its performance. . | Large amounts of quality data will enable better performance and generalizability to downstream tasks. Different modalities can be used to mine such data, including synthetic data. For filtering high quality data, methods such as active learning need to be incorporated. . | . Continuous improvement (Active learning) pipeline for scaling with data . tldr; Conclusion . Performance of supervised models depends on the availability of high quality labeled data, which is tedious to aquire, and thereby limiting. Self-supervised learning not only makes it possible to realize gains via abundant unlabeled data, but also gives way for foundation models that learn a generic neural representation of inputs, whose knowledge can be transformed to application specific downstream tasks. The key to this is designing self-supervised pretext tasks to make use of unlabeled data. The underlying principle of generative pretext tasks is to corrupt the input and make the network recover the original input from the corrupt one. Feature similarity based pretext tasks learn data representations that are similar for similar inputs and dissimilar for dissimilar inputs. . . Further Resources . Language Foundation models (LLM) and beyond . In this post, I focus on vision foundation models. But, there are plenty of resources for launguage foundation models a.k.a. Large Language Models (LLMs). . Full Stack LLM Bootcamp, 2023 | . Lectures / Videos . Yann LeCunn’s lecture, [Slides | FSDL 2022 Course’s Lecture on Language Foundation Models | MIT FUTURE OF AI: Self-Supervised Learning and Foundation Models | Self-Supervised Learning: Self-Prediction and Contrastive Learning, NIPS 21 Tutorial Video, [Slides] | . Libraries . | | | . Read . Yann LeCunn’s blog post Self-supervised learning: The dark matter of intelligence | Balestriero, Randall et al. A Cookbook of Self-Supervised Learning ArXiv abs/2304.12210 (2023): n. pag. | Weng, Lilian (2019). Self-Supervised Representation Learning writes about pretext tasks in detail | Silva, Thalles Santos (2020). Self-Supervised Learning and the Quest for Reducing Labeled Data in Deep Learning gives a great introduction on how self-supervised learning can enable foundation models, anagolous to Yann LeCunn’s lecture | Ozbulak, Utku, et al. “Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training.” arXiv preprint arXiv:2305.13689 (2023) | . Data . LAION-5B Open Large-scale CLIP-filtered image-text paired dataset | . . © Zeeshan Khan Suri, . If this article was helpful to you, consider citing . @misc{suri_self-supervision-for-foundation-models_2023, title={Self-supervised learning for vision foundation models}, url={https://zshn25.github.io/self-supervision-for-foundation-models/}, journal={Curiosity}, author={Suri, Zeeshan Khan}, year={2023}, month={May}} . References . Northcutt, C. G., Athalye, A., &amp; Mueller, J. (2021). Pervasive label errors in test sets destabilize machine learning enchmarks. Proceedings of the 35th Conference on Neural Information Processing Systems Track on Datasets and Benchmarks &#8617; . | Schmidt, Ludwig, et al. “Adversarially robust generalization requires more data.” Advances in neural information processing systems 31 (2018). &#8617; &#8617;2 . | Bommasani, Rishi, et al. “On the opportunities and risks of foundation models.” arXiv preprint arXiv:2108.07258 (2021). &#8617; . | Redmon, Joseph (2016). “You only look once: Unified, real-time object detection”. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. arXiv:1506.02640 &#8617; . | K. He, X. Chen, S. Xie, Y. Li, P. Dollár and R. Girshick, “Masked Autoencoders Are Scalable Vision Learners,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 15979-15988, doi: 10.1109/CVPR52688.2022.01553. &#8617; &#8617;2 . | Xiang, W., Yang, H., Huang, D., &amp; Wang, Y. (2023). Denoising Diffusion Autoencoders are Unified Self-supervised Learners. ArXiv, abs/2303.09769. &#8617; . | M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. &#8617; . | Pulkit Agrawal, Joao Carreira, and Jitendra Malik. 2015. Learning to See by Moving. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) (ICCV ‘15). IEEE Computer Society, USA, 37–45. https://doi.org/10.1109/ICCV.2015.13 &#8617; . | Zhang, Isola, Efros. Colorful Image Colorization. In ECCV, 2016 &#8617; . | Radford, Alec, Luke Metz, and Soumith Chintala. “Unsupervised representation learning with deep convolutional generative adversarial networks.” arXiv preprint arXiv:1511.06434 (2015). &#8617; . | Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning (ICML’20), Vol. 119. JMLR.org, Article 149, 1597–1607. &#8617; . | Chen, Ting and Lala Li. “Intriguing Properties of Contrastive Losses.” Neural Information Processing Systems (2020). &#8617; &#8617;2 . | The Unreasonable Effectiveness of Deep Features as a Perceptual Metric Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang CVPR 2018. &#8617; . | Hénaff, Olivier J., et al. “Efficient visual pretraining with contrastive detection.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. &#8617; . | Hénaff, Olivier J., et al. “Object discovery and representation networks.” Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVII. Cham: Springer Nature Switzerland, 2022. &#8617; . |",
            "url": "https://zshn25.github.io/self-supervision-for-foundation-models/",
            "relUrl": "/self-supervision-for-foundation-models/",
            "date": " • May 11, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Pose Constraints for Self-supervised Monocular Depth and Ego-Motion",
            "content": "  . Paper arXiv Video Code Data &lt;/div&gt;   . . Consistent Depth without any post-processing. KITTI dataset1 . Abstract . Self-supervised monocular depth estimation approaches suffer not only from scale ambiguity but also infer temporally inconsistent depth maps w.r.t. scale. . . Two depth estimation results on four frames showing the inconsistency problem in existing MDE methods. Image taken from Li et. al.2 . While disambiguating scale during training is not possible without some kind of ground truth supervision, having scale consistent depth predictions would make it possible to calculate scale once during inference as a post-processing step and use it over-time. With this as a goal, a set of temporal consistency losses that minimize pose inconsistencies over time are introduced. Evaluations show that introducing these constraints not only reduces depth inconsistencies but also improves the baseline performance of depth and ego-motion prediction. . . Scale factors (GTpred frac{ text{GT}}{ text{pred}}predGT​) within each KITTI sequences are highly varying . Introduction . For an introduction to self-supervised monocular depth estimation, checkout my previous blog post on Self-supervised Monocular Depth Estimation . . BibTeX . @InProceedings{10.1007/978-3-031-31438-4_23, author=&quot;Suri, Zeeshan Khan&quot;, editor=&quot;Gade, Rikke and Felsberg, Michael and K{ &quot;a}m{ &quot;a}r{ &quot;a}inen, Joni-Kristian&quot;, title=&quot;Pose Constraints for Consistent Self-supervised Monocular Depth and Ego-Motion&quot;, booktitle=&quot;Image Analysis&quot;, year=&quot;2023&quot;, publisher=&quot;Springer Nature Switzerland&quot;, address=&quot;Cham&quot;, pages=&quot;340--353&quot;, isbn=&quot;978-3-031-31438-4&quot;, doi={10.1007/978-3-031-31438-4_23} } . Paper arXiv Video Code Data &lt;/div&gt; References: . A Geiger, P Lenz, C Stiller, and R Urtasun. 2013. Vision meets robotics: The KITTI dataset. Int. J. Rob. Res. 32, 11 (September 2013), 1231–1237. https://doi.org/10.1177/0278364913491297 &#8617; . | Li, S., Luo, Y., Zhu, Y., Zhao, X., Li, Y., Shan, Y.: Enforcing temporal consistency in video depth estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops. pp. 1145–1154 (October 2021) &#8617; . |",
            "url": "https://zshn25.github.io/pc4consistentdepth/",
            "relUrl": "/pc4consistentdepth/",
            "date": " • Apr 17, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Convolution vs. Attention",
            "content": "Layers in neural netowrks can be seen as a function that takes in a multi-dimensional input and produces an output. For simplicity, let’s assume the input and output dimesnions to be the same. . Fully-connected Layer . In a fully connected layer each output is just a linear combination of all the inputs. . $ textcolor{FF7800}{ textbf{y}} = textcolor{9966FF}{W} textcolor{2EC27E}{ textbf{x}} $ . . In a fully connected layer, each output depends on (and is connected to) all inputs . In fact, in Pytorch, a fully-connected layer is just represented by the linear layer. . import torch torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None) . The weights matrix $ textcolor{9966FF}{W}$ is the learnable parameter which learns to assign weights of each output to all of the inputs. . The problem with the fully-connected layer is that it consumes a lot of learnable parameters. Since each output is connected to all inputs, a lot of the weights have to be learnt. But, for most input types it is wasteful to connect to all inputs since dependencies are often local, for e.g. spatio-temporal neighbors, than global. . In principle, connecting to all inputs shouldn’t be a problem since given enough training samples and enough time to train, the network should be able to learn to assign meaningful (and sparse) weights to some locations (like the spatio-temporal neighbors) than to the rest, which could be exploited by post-processing steps such as Pruning to fasten inference. . But, especially for data such as images where we know the inherent local dependencies of pixels to their spatial neighbors, it is a good idea to limit the layer to such neighborhood. . Convolution . Convolutional neural networks (CNNs) are typically used for spatial data processing, such as images, where there is a spatial relationship between the data or temporal data such as audio. For example, neighboring pixels (in X or Y direction) are related to each other. A convolutional filter is applied to such data to extract features, such as edges, textures, etc., in images. . . A convolutional layer only attends at it’s neighbors . Attention . Transformers on the other hand are typically used for sequential data processing, such as text, natural language, where short-term and long-term dependencies are present. The actual dependencies are not explicit in this case. For example, in the sentence “Alice had gone to the supermarket to meet Bob”, one of the verb “meet”, is located far-away from the subject “Alice” and this dependency is not spatial but differs a lot. This is even more for longer inputs with multiple paragraphs where the final sentence could have had a dependecy to a sentence somewhere in the beginning. Transformers are based on the so called attention mechanisms which learns these relationships between the elements in the sequence. . . Attentions is the key component of Transformers and have been successfully applied to image data. Davide Coccomini, CC BY-SA 4.0, via Wikimedia Commons . Basic Self-attention . The basic idea of self-attention is to assign different importance to the inputs based on the inputs themselves. In comparison to convolution, self-attention allows the receptive field to be the entire spatial locations. . . An attention layer assigns importance to the inputs based on the inputs themselves . The weights are computed based on cosine similarity of the inputs $ textcolor{2EC27E}{ textbf{x}}$ to themselves . . $ textcolor{9966FF}{W} = textcolor{2EC27E}{ textbf{x}} textcolor{2EC27E}{ textbf{x}^T}$ . . Input $ textcolor{2EC27E}{ textbf{x}^T}$ and its transforms are usually multi-dimensional. Vector notation is used for illustration Self-attention . Note that the basic version of self-attention does not include any learnable parameters. For this reason, the “Attention is all you need”[1] variation of the self-attention includes 3 learnable weight matrices (key $ textcolor{9966FF}{W_K}$, query $ textcolor{9966FF}{W_Q}$ and value $ textcolor{9966FF}{W_V}$). But, the basic principle remains the same. The key $ textcolor{9966FF}{W_K}$ and query $ textcolor{9966FF}{W_Q}$ matrices are used to transform the input into key $ textcolor{2EC27E}{ textbf{k}} = textcolor{9966FF}{W_K} textcolor{2EC27E}{ textbf{x}}$, and query $ textcolor{2EC27E}{ textbf{q}} = textcolor{9966FF}{W_Q} textcolor{2EC27E}{ textbf{x}}$, whose similarity $ textcolor{2EC27E}{ textbf{q}} textcolor{2EC27E}{ textbf{k}^ mathrm{T}}$, weighs the output (value), $ textcolor{FF7800}{ textbf{v}} = textcolor{9966FF}{W_V} textcolor{2EC27E}{ textbf{x}} $.[2] . $ textcolor{FF7800}{ textbf{y}} = text{softmax} left( frac{ textcolor{2EC27E}{ textbf{q}} textcolor{2EC27E}{ textbf{k}^ mathrm{T}}}{ sqrt{d_k}} right) textcolor{FF7800}{ textbf{v}}$ . Also note that in the basic version, the self-similarity of the inputs always causes the diagonal to be of the highest similarity and makes the weight matrix symmetric. This problem is also elevated by transforming the same input using two seperate learnable weight matrices, $ textcolor{9966FF}{W_K}$ and $ textcolor{9966FF}{W_Q}$ . Convolution vs. Attention: Which is better? . Although attention based models such as vision transformers have shown to outperform CNN based methods, a careful analysis of the two shows comparible performance.3 . In early layers of a neural network for images, spatial relations can be captured by convolutions and the later layers could benifit from long-range receptive fields offered by attention. Hence, both can be combined.4 . Am I going to argue that &quot;Conv is all you need&quot;?No!My favorite architecture is DETR-like: ConvNet (or ConvNeXt) for the first layers, then something more memory-based and permutation invariant like transformer blocks for object-based reasoning on top.https://t.co/LA2J72N93A . &mdash; Yann LeCun (@ylecun) January 12, 2022 Summary . Different ways of connecting inputs to each other were discussed. A fully-connected layer connects all inputs to each other. This leads to exponential increase in network parameters and computational complexity. While the network can learn to assign different weights, this can take a lot of data and prolonged training. . | Convolutional layer incorporates desirable inductive biases about the data to reduce computation and connects only to the neighbors. Spatial and temporal data benifits from doing so. Convolution is translation invariant. However, the dimensions of outputs of a convolution depend on the input dimensions. . | A self-attention layer assigns importance to inputs based on their similarity. For e.g., in the sentence “Alice is adventurous and she is in wonderland.” the word “she” refers to “Alice” and ideally, their embeddings should be similar, which can be used by the self-attention layer to determine contexts. Similar to fully-connected, far away connections can be established if the input features or embeddings are similar. However, not having enough data may lead to overfitting the inputs. . | In early layers of a neural network for images, spatial relations can be captured by convolutions and the later layers could benifit from long-range receptive fields offered by attention. Hence, both can be combined. Works such as CoAtNet[5] offer layers combining the two. . | . References . Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. &#8617; . | https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#Scaled_dot-product_attention &#8617; . | Z. Liu, H. Mao, C. -Y. Wu, C. Feichtenhofer, T. Darrell and S. Xie, “A ConvNet for the 2020s,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 11966-11976, doi: 10.1109/CVPR52688.2022.01167. &#8617; . | Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., &amp; Girshick, R.B. (2021). Early Convolutions Help Transformers See Better. Neural Information Processing Systems. &#8617; . | Dai, Z., Liu, H., Le, Q. V., &amp; Tan, M. (2021). Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34, 3965-3977. &#8617; . |",
            "url": "https://zshn25.github.io/CNNs-vs-Transformers/",
            "relUrl": "/CNNs-vs-Transformers/",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Software 2.0 2.0",
            "content": "This post is a rebuttal on Andrej Karpathy’s Software 2.0. It is brilliant and you should definitely read it but I thought the critical takeaway was lost in the many great points and I wanted to make that more explicit. . . Traditional methods for solving a software problem involve people coming up with the required algorithmic steps to tackle it. . For example, think about a problem of estimating the depth of an object from an image. A traditional algorithm would contain the following steps . Gather 2 frames with enough baseline such that there is change within the scene but not too much . | Find features such as SIFT, etc. on one frame, assuming the same features also exist on the other frame. . | Find correspondence between these features, filter the correspondences and estimate a 2D flow . | Derive relative depth from the flow . | . Scaling with data . Such an approach would work good enough in general and can be deployed for all kinds of scenarios which satisfy the approach’s approximations/assumptions but its performance would not depend on the amount of available data. . . Performance of a traditional algorithm does not depend on the amount of avaiable data . How can we do better? . We can try to improve each of the approach’s steps and gain overall performance, and decades of research in Computer Vision does exactly that. But, by changing our fundemental thinking towards learning based approaches, unprecidented improvements were realized. . Era of learning . Learning based approaches replaced the handcrafted features (SURF in the above example) to derive these features from the data. Neural networks replaced the rest . Checkout my blog post on self-supervised learning for foundation models . Neural networks are function approximators . Traditional approach is to come up with a model/algorithm that converts an input to an output. . $ text{input} rightarrow model rightarrow text{output} $ . Deep learning replaces this with 2 steps: . Training: Given many examples of input and output data, the model is learnt. . | Deployment: The learnt model is used on new inputs to infer their outputs. . | This allows the performance of the model to be dependent on the amount of training data available. . . As the training data increases, the performance of a DL model increases but traditional methods do not depend on the training data . Production time . Traditional algorithms have to be almost always completely re-written from their prototyping phase to production while keeping the computational power and memory in check. At test time, Deep learning replaces all of this with just one model inference call. So, we go from re-implementing production level code to passing the input through a bunch of linear algebra. Neural networks cost same amount of memory and even better comoutational effeciency during inference time. . Algorithm 2.0ic thinking . First, to gain performance, it is not eough to copy paste one training example into multiple copies. The model needs to be intellignetly scaled by feeding in huge amounts of new data with much variation, representing the true population as truely as possible. . | Secondly, new data doesn’t come for free. It is labor-intensive and expensive to label data. Smart gathering is required. One cannot just copy paste existing data and assume the data has increased. It has to increase meaningfully. . | Thirdly, in order to be able to feed in that much amount of data, there needs to be infrastructure in place. . | As in the case of traditional algorithms, one cannot just hand-engineer the algorithm and use it in deployment forever. One needs to continuously improve/train the learnt model and continuoiusly deploy it. This is also where the data can be selected smartly for the next iteration of improvement. . | The holy grail? or just a fad? . For many applications, the performance gain is really not that critical. Other factors such as explainability, fail-safe, etc are equally important, which the current deeo learning approaches lack. The availability of data is another critical factor in deciding to choose learning based approaches. . . If data is limited (green part), traditional algorithms provide better performance . Continuous improvement . But, if the performance gains are to be realized, scaling with data is the way to go. . Amount of data used to train notable AI systems. Source: Our World in Data . And the way to execute this smartly is by continuous improvement . . Continuous Improvement loop .",
            "url": "https://zshn25.github.io/Software-2/",
            "relUrl": "/Software-2/",
            "date": " • Dec 4, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Self-supervised monocular depth estimation",
            "content": "Animals (and in extension, humans) are unable to directly perceive the 3D surroundings around us. Each of our eyes projects the 3D world onto 2D, losing the depth dimension. Instead, we rely on our brain to reconstruct these 2D projections to perceive depth. Having more than one eye allows us to geometrically reconstruct depth via triangulation1, but how are creatures with a single eye (for e.g., due to a defective eye, or due to a birth disorder2) still able to perceive it? . . Each eye of Mantis Shrimp possesses trinocular vision. Cédric Peneau, CC BY-SA 4.0, via Wikimedia Commons . We rely on monocular cues for perceiving 3D even with a single eye. These monocular cues include those from the static image such as perspective, relative sizes of familiar objects, shading, occlusion, etc.; and those from motion, such as parallax, depth from motion, etc.3 . Can we teach machines to estimate depth from a single image? . If animals are able to use these cues to reason about the relative 3D structure, the question arises, is there a way to make machines do the same? A famous paper from 2009 called Make3D4 says “yes!”. . But this post is not about that. We will be looking at more recent neural network based approaches. After all, neural networks can be thought as function approximators and with enough data, should be able to approximate the function $ mathcal{f}$ that maps an RGB pixel $i in mathbb{R}^3$ to its depth $d$ . $d = mathcal{f}(i)$ . In-fact we are looking at methods that do not rely on the availability ground-truth (GT) depths. Why? Because it is expensive and tedious to gather such ground truth and difficult to calibrate and align different sensor ouputs, making it difficult to scale. But, how can we teach a neural network to estimate the underlying depth without having ground truth? Thanks for asking that! Geometry comes to the rescue. The idea is to synthesize different views of the same scene and compare these synthesized views with the real ones for supervision. It underlies on the brightness constantcy assumption5, where parts of the same scene are assumed to be observed in multiple views. A similar assumption is used in binocular vision, where the content of what a left eye/camera sees is very similar to that of what the right one sees; and in motion/optical-flow estimation, where the motion is the vector $u_{xy}, v_{xy}$ defined by the change in pixel locations as . $I(x, y, t) = I(x + u_{xy}, y + v_{xy}, t + 1)$ . Stereo supervision . Garg et.al., propose a method called monodepth6, to estimate depth from a single image, trained on stereo image pairs (without the need for depth GT). The idea is similar: synthesize the right view from the left one and compare the synthesized and the real right views as supervision. . . . Left and right images of the same scene from the KITTI dataset7 . But, how will this help learn depth? This is because view synthesis is a function of depth. In a rectified stereo setting, the optical flow is unidirectional (horizontal) and so only its magnitude a.k.a. disparity is to be found. The problem then boils down to finding a per-pixel disparity that when applied to the left image, gives the right image. . $I_{l}(x, y) stackrel{!}{=} I_r(x + d_{xy}, y)$ . The depth from disparity can be calculated by $ text{depth} = frac{ text{focal length} times text{baseline}}{ text{disparity}}$. . . Depth as a function of disparity via triangulation1. By fr:Utilisateur:COLETTE, CC BY-SA 3.0, via Wikimedia Commons . The pipeline goes as follows: . A network predicts the dense disparity map of the left image. | Relative camera pose (Ego-motion) is known, which in this case of recitfied stereo is just a scalar, representing the horizontal shift of 5.4 cm in X-direction. | Using the estimated disparity, a per-pixel flow in the left image’s coordinates is calculated based on the known relative rigid camera pose. | The right input image is warped using the flow, into the view of the left image. | The warped right image onto the left’s view needs to be consistent with the original left image if the estimated depth from the network is correct. A loss is thus calculated between the two and is propogated through the depth network, thereby, making it learn to predict monocular depth. | The Monocular case . While the stereo case is analogous to animals using binocular vision to perceive 3D, what about the monocular case, where creatures are still able to reconstruct the underlying 3D structure of the scene using a single eye? Can the above method be extended for monocular case? . . Cyclops by Nilesdino, CC BY-NC-ND 3.0 . The answer is not straightforward. Even if we think about exploiting motion, the relative ego-motion of the camera from one frame at time $t$ to the next $t+1$ is not known, unlike in stereo case where the pose between the left and right cameras is known (and uni-directional for calibrated cases). Not only is the camera pose not known between two timesteps, it is also changing and not constant as in stereo case. So, in order to extend the stereo approach to monocular case, relative camera pose or camera ego-motion has to also be predicted between any 2 consecutive frames. . . SfMLearner: Unsupervised Learning of Depth and Ego-Motion from Video8 . Zhou et.al’s SfMLearner: Unsupervised Learning of Depth and Ego-Motion from Video8 does exactly that. They introduce an additional network which estimates the 6 DOF rigid ego-motion/pose $T in text{SE}(3)$, between the cameras of the two consecutive frames with pixel locations $[x_t, y_t, 1]^T$ and $[x_{t+1}, y_{t+1}, 1]^T$ resp.. The working principle is similar to before but the warping of one frame onto the other is done by . $ left( begin{array}{c}x_{t+1} y_{t+1} 1 end{array} right) stackrel{!}{=} KTK^{-1}d_{xy} left( begin{array}{c}x_t y_t 1 end{array} right)$, . where the camera intrinsics matrix $K = Bigl[ begin{smallmatrix}k_x&amp;0&amp;p_x 0&amp;k_y&amp;p_y 0&amp;0&amp;1 end{smallmatrix} Bigr]$, with focal lengths $f_x, f_y$ and principal point $p_x, p_y$ is assumed to be known. Next, we will see how this makes sense . . $K^{-1}d_{xy} left( begin{array}{c}x_t y_t 1 end{array} right)$ . Intiutively, given the depth $D$, one could unproject the image coordinates using the depth and the inverse camera intrinsics onto 3D. . . $TK^{-1}d_{xy} left( begin{array}{c}x_t y_t 1 end{array} right)$ . Then, the camera is transformed to that at time $t+1$. . . $TK^{-1}d_{xy} left( begin{array}{c}x_t y_t 1 end{array} right)$ . And, the points are projected back onto this transformed camera using the same camera intrinsics, to synthesize the image at time $t+1$ . . $I_{t} , Big langle KTK^{-1}d_{xy} left( begin{array}{c}x_t y_t 1 end{array} right) Big rangle $ . , where $ Big langle Big rangle $ is the sampling operator. This projection of $I_t$ onto the coordinate at time $t+1$ geometrically synthesizes the image $ hat{I}_{t+1}$ and can now be compared to the original frame from time $t+1$ and the loss is backpropogated to both the depth and the pose networks. . Note . This is not exactly correct. The pose $T$ is used, not to transform the camera, but to transform the point cloud (in the reverse direction to that of the camera pose), and the image $I_{t+1}$ is warped onto $t$, but the underlying concept remains the same. I will now explain how this idea is executed. . Flow field due to rigid camera motion . The resulting 2D rigid flow (flow due to rigid camera motion) that transforms each pixel at time $t+1$ to that at time $t$ can be visualized as follows. . . $KTK^{-1}d_{xy} left( begin{array}{c}x_t y_t 1 end{array} right)$ . Note that this flow is a function of depth and 6 DOF transformation, i.e. the rigif flow field depends not only on the rigid 6 DOF transformation of the camera, but also on the distance of each point to the camera. Intuitively, this makes sense since objects fat away seem to move less in the image plane than those far away. This is known as motion parallax . . Motion Parallax: Objects farther away appear to move lesser than those close-by. Nathaniel Domek, CC BY 3.0, via Wikimedia Commons . . © Zeeshan Khan Suri, . If this article was helpful to you, consider citing . @misc{suri_how_monocular_depth_estimation_2022, title={Self-supervised monocular depth estimation}, url={https://zshn25.github.io/How-Monocular-Depth-Estimation-works/}, journal={Curiosity}, author={Suri, Zeeshan Khan}, year={2022}, month={Oct}} . References: . Triangulation &#8617; &#8617;2 . | Cyclopia &#8617; . | Monocular cues for depth perception &#8617; . | A. Saxena, M. Sun and A. Y. Ng, “Make3D: Learning 3D Scene Structure from a Single Still Image,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, pp. 824-840, May 2009, doi: 10.1109/TPAMI.2008.132. &#8617; . | Brightness Constancy &#8617; . | Garg, R., B.G., V.K., Carneiro, G., Reid, I. (2016). Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds) Computer Vision – ECCV 2016. ECCV 2016. Lecture Notes in Computer Science(), vol 9912. Springer, Cham. &#8617; . | A Geiger, P Lenz, C Stiller, and R Urtasun. 2013. Vision meets robotics: The KITTI dataset. Int. J. Rob. Res. 32, 11 (September 2013), 1231–1237. https://doi.org/10.1177/0278364913491297 &#8617; . | T. Zhou, M. Brown, N. Snavely and D. G. Lowe, “Unsupervised Learning of Depth and Ego-Motion from Video,” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6612-6619, doi: 10.1109/CVPR.2017.700. &#8617; &#8617;2 . |",
            "url": "https://zshn25.github.io/How-Monocular-Depth-Estimation-works/",
            "relUrl": "/How-Monocular-Depth-Estimation-works/",
            "date": " • Oct 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Aristotle’s regime, the good citizen and justice",
            "content": ". The whole post is a collection of excerpts from Aristotle’s Politics Book 3 Definitions of state/regime, it’s citizens, a good citizen and a good regime . A state is composite, like any other whole made up of many parts; these are the citizens, who compose it. It is evident, therefore, that we must begin by asking, Who is the citizen, and what is the meaning of the term? . We may say, first, that a citizen is not a citizen because he lives in a certain place, for resident aliens and slaves share in the place; But the citizen is he who shares in the administration of justice, and in offices. He who has the power to take part in the deliberative or judicial administration of any state is said by us to be a citizens of that state; and, speaking generally, a state is a body of citizens sufficing for the purposes of life. &gt;Whoever is entitled to participate in an office involving deliberation and decision making is a citizen of the city. . The regime is an arrangement of a city with respect to it’s offices, particularly the ones who has authority over all matters. For, what has authority in a city is everywhere the governing body. And the governing body is the regime. . Since there are many forms of regimes, there must be many varieties of citizen and especially of citizens who are subjects. There are different kinds of citizens; and he is a citizen in the highest sense who shares in the honours of the state. . A citizen is one who shares in governing and being governed. He differs under different forms of government, but in the best state he is one who is able and willing to be governed and to govern with a view to the life of virtue. . Who is a good citizen? . Those who care for good government take into consideration virtue and vice in states. Whence it may be further inferred that virtue must be the care of a state which is truly so called. . Citizens differ from one another, but the salvation of the community is the common business of them all. This community is the constitution; the virtue of the citizen must therefore be relative to the constitution of which he is a member. . The same question may also be approached by another road, from a consideration of the best constitution . Forms of regimes . Regimes which have a regard to the common interest are constituted in accordance with strict principles of justice, and are therefore true forms; but those which regard only the interest of the rulers are all defective and perverted forms, for they are despotic, whereas a state is a community of freemen. . The true forms of government, therefore, are those in which the one, or the few, or the many, govern with a view to the common interest; but governments which rule with a view to the private interest, whether of the one or of the few, or of the many, are perversions. For the members of a state, if they are truly citizens, ought to participate in its advantages. . Of forms of government in which one rules, we call that which regards the common interests, kingship or royalty; that in which more than one, but not many, rule, aristocracy; and it is so called, either because the rulers are the best men, or because they have at heart the best interests of the state and of the citizens. But when the citizens at large administer the state for the common interest, the government is called by the generic name –a constitution. And there is a reason for this use of language. One man or a few may excel in virtue; but as the number increases it becomes more difficult for them to attain pertection in every kind of virtue, though they may in military virtue, for this is found in the masses. Hence in constitutional government the fighting-men have the supreme power, and those who possess arms are the citizens. . Of the above-mentioned forms, the perversions are as follows: of royalty, tyranny; of aristocracy, oligarchy; of constitutional government, democracy. For tyranny is a kind of monarchy which has in view the interest of the monarch only; oligarchy has in view the interest of the wealthy; democracy, of the needy: none of them the common good of all. . . * Aristotle’s classifications of political constitutions. By Chiswick Chap, CC BY-SA 4.0, via Wikimedia Commons* . The real difference between democracy and oligarchy is poverty and wealth. Wherever men rule by reason of their wealth, weather they be few or many, that is an oligarchy, and where the poor rule, that is a democracy and wealth… and freedom are the grounds on which the two classes lay claim to the government. . Kings rule according to law over voluntary subjects, but tyrants over involuntary; and the one are guarded by their fellow-citizens the others are guarded against them. . The purpose of a state/regime? . Man is by nature a political animal. And therefore, men, even when they do not require one another’s help, desire to live together; not but that they are also brought together by their common interests in proportion as they severally attain to any measure of well-being. This is certainly the chief end, both of individuals and of states. . A state exists for the sake of a good life, and not for the sake of life only; Nor does a state exist for the sake of alliance and security from injustice, nor yet for the sake of exchange and mutual intercourse; It is manifest therefore that a state is not merely the sharing of a common locality for the purpose of preventing mutual injury and exchanging goods. These are necessary preconditions of a state’s existence, yet nevertheless, even if all these conditions are present, that does not therefore make a state, which is a community of families and aggregations of families in well-being, for the sake of a perfect and self-sufficing life. Such a community can only be established among those who live in the same place and intermarry. Hence arise in cities family connections, brotherhoods, common sacrifices, amusements which draw men together. But these are created by friendship, for the will to live together is friendship. The end of the state is the good life, and these are the means towards it. And the state is the union of families and villages in a perfect and self-sufficing life, by which we mean a happy and honorable life. . If wealth and freedom are necessary elements, justice and valor are equally so; for without the former qualities a state cannot exist at all, without the latter not well. . Similarity in states . A question is raised respecting the state, whether a certain act is or is not an act of the state; for example, in the transition from an oligarchy or a tyranny to a democracy. In such cases persons refuse to fulfill their contracts or any other obligations, on the ground that the tyrant, and not the state, contracted them; they argue that some constitutions are established by force, and not for the sake if the common good. . This question runs up into another: on what principle shall we ever say that the state is the same, Or different? It would be a very superficial view which considered only the place and the inhabitants (for the soil and the population may be separated, and some of the inhabitants may live in one place and some in another). Again, shall we say that while the race of inhabitants, as well as their place of abode, remain the same, the city is also the same, although the citizens are always dying and being born, as we call rivers and fountains the same, although the water is always flowing away and coming again. Or shall we say that the generations of men, like the rivers, are the same, but that the state changes? For, since the state is a partnership, and is a partnership of citizens in a constitution, when the form of government changes, and becomes different, then it may be supposed that the state is no longer the same. And in this manner we speak of every union or composition of elements as different when the form of their composition alters; And if this is true it is evident that the sameness of the state consists chiefly in the sameness of the constitution, and it may be called or not called by the same name, whether the inhabitants are the same or entirely different. It is quite another question, whether a state ought or ought not to fulfil engagements when the form of government changes. . Our conclusion, then, is that political society exists for the sake of noble actions, and not of mere companionship. Hence they who contribute most to such a society have a greater share in it. . All the partisans of different forms of government speak of a part of justice only. . Justice in a regime . In all sciences and arts the end is the good, and the greatest good - this is the political science of which the good is justice, in other words, the common interest. . Laws, when good, should be supreme; and that the magistrate or magistrates should regulate those matters only on which the laws are unable to speak with precision owing to the difficulty of any general principle embracing all particulars. But what are good laws? The goodness or badness, justice or injustice, of laws varies of necessity with the constitutions of states. This, however, is clear, that the laws must be adapted to the constitutions. But if so, true forms of government will of necessity have just laws, and perverted forms of government will have unjust laws. . Magistrates do many things from spite and partiality. Desire is a wild beast, and passion perverts the minds of rulers, even when they are the best of men. The law is reason unaffected by desire. in seeking for justice men seek for the mean or neutral, for the law is the mean. . What is just or right is to be interpreted in the sense of ‘what is equal’. All men think justice to be a sort of equality. For they admit that justice is a thing and has a relation to persons, and that equals ought to have equality. But there still remains a question: equality or inequality of what? . those who are equal in one thing ought not to have an equal share in all, nor those who are unequal in one thing to have an unequal share in all, it is certain that all forms of government which rest on either of these principles are perversions. All men have a claim in a certain sense, as I have already admitted, but all have not an absolute claim. .",
            "url": "https://zshn25.github.io/Aristotle-definitions/",
            "relUrl": "/Aristotle-definitions/",
            "date": " • Sep 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Reasons for earning money: The good and the ugly",
            "content": ". The whole post is a collection of excerpts from Aristotle’s Politics, Book 1. Art of household management must either find ready to hand, or itself provide, such things necessary to life, and useful for the community of the family or state, as can be stored. They are the elements of true riches; for the amount of the property which is needed for a good life is not unlimited. There is a boundary fixed, just as there is in other arts; for the instruments of any art are never unlimited, either in number or size, and riches may be defined as a number of instruments to be used in a household or in a state. . There is another variety of the art of acquisition which is commonly and rightly called an art of wealth-getting, and has in fact suggested the notion that riches and property have no limit. Being nearly connected with the preceding, it is often identified with it. But though they are not very different, neither are they the same. The kind already described is given by nature, the other is gained by experience and art. One uses the material which the other one provides . Of everything which we possess there are two uses: both belong to the thing as such, but not in the same manner, for one is the proper, and the other the improper or secondary use of it. For example, a shoe is used for wear, and is used for exchange; both are uses of the shoe. He who gives a shoe in exchange for money or food to him who wants one, does indeed use the shoe as a shoe, but this is not its proper or primary purpose, for a shoe is not made to be an object of barter. The same may be said of all possessions, for the art of exchange extends to all of them, and it arises at first from what is natural, from the circumstance that some have too little, others too much. Hence we may infer that retail trade is not a natural part of the art of getting wealth; had it been so, men would have ceased to exchange when they had enough. . This sort of barter is not part of the wealth-getting art and is not contrary to nature, but is needed for the satisfaction of men’s natural wants. The other or more complex form of exchange grew, as might have been inferred, out of the simpler. When the inhabitants of one country became more dependent on those of another, and they imported what they needed, and exported what they had too much of, money necessarily came into use. For the various necessaries of life are not easily carried about, and hence men agreed to employ in their dealings with each other something which was intrinsically useful and easily applicable to the purposes of life, for example, iron, silver, and the like. Of this the value was at first measured simply by size and weight, but in process of time they put a stamp upon it, to save the trouble of weighing and to mark the value. When the use of coin had once been discovered, out of the barter of necessary articles arose the other art of wealth getting, namely, retail trade; which was at first probably a simple matter, but became more complicated as soon as men learned by experience whence and by what exchanges the greatest profit might be made. Originating in the use of coin, the art of getting wealth is generally thought to be chiefly concerned with it, and to be the art which produces riches and wealth; having to consider how they may be accumulated. . But the art of wealth-getting which consists in household management, on the other hand, has limit; the unlimited acquisition of wealth is not its business. And, therefore, in one point of view, all riches must have a limit: nevertheless, as a matter of fact, we find the opposite to be the case; for all getters of wealth increase their hoard of coin without limit. The source of the confusion is the near connection between the two kinds of wealth-getting; in either, the instrument is the same, although the use is different, and so they pass into one another; for each is a use of the same property, but with a difference: accumulation is the end in the one case, but there is a further end in the other. . Hence some persons are led to believe that getting wealth is the object of household management, and the whole idea of their lives is that they ought either to increase their money without limit, or at any rate not to lose it. The origin of this disposition in men is that they are intent upon living only, and not upon living well; and, as their desires are unlimited they also desire that the means of gratifying them should be without limit. Those who do aim at a good life seek the means of obtaining bodily pleasures; and, since the enjoyment of these appears to depend on property, they are absorbed in getting wealth: and so there arises the second species of wealth-getting. For, as their enjoyment is in excess, they seek an art which produces the excess of enjoyment; and, if they are not able to supply their pleasures by the art of getting wealth, they try other arts, using in turn every faculty in a manner contrary to nature. The quality of courage, for example, is not intended to make wealth, but to inspire confidence; neither is this the aim of the general’s or of the physician’s art; but the one aims at victory and the other at health. Nevertheless, some men turn every quality or art into a means of getting wealth; this they conceive to be the end, and to the promotion of the end they think all things must contribute. . Thus, then, we have considered the art of wealth-getting which is unnecessary, and why men want it; and also the necessary art of wealth-getting, which we have seen to be different from the other, and to be a natural part of the art of managing a household, concerned with the provision of food, not, however, like the former kind, unlimited, but having a limit. . Further Reading . Philosophy of Money and Finance, The Stanford Encyclopedia of Philosophy (SEP) | Aristotle’s Politics | .",
            "url": "https://zshn25.github.io/kinds-of-wealth-getting-Aristotle/",
            "relUrl": "/kinds-of-wealth-getting-Aristotle/",
            "date": " • Aug 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Notes on Justice from Plato's Republic",
            "content": "In Plato’s Republic 1, Socrates attempts to define what justice is and why pursuing justice is good not only because what having the reputation of being just leads to, but also in by itself. This distinction between seeming to be just, and being just is characterized by the question, “if one removes all the consequences of being unjust, doesn’t a just person act same as a unjust person?”. . First, it is said that justice is to give what one is owed. But, such a definition is ambiguous and open to interpretation. Furthermore, knowledge is a prerequisite here, as in order to know what one is owed, knowledge is required. . So, in order to define justice more precisely, Socrates proposes to first look not at justice in an individual, but by hypothetically constructing justice in a just city and later claims an individual to be similar to a city and thus the definition of justice in a city translates to that of an individual. . Socrates‘s just city . A city comes into, because none of us are self sufficient. And, the fundamental idea of Socrates‘s just city lies in the idea that it’s citizens should pursue one and only one role. The argument to this is that a person does a better job if the person practices one job. The doer has to pay close attention to the work rather than treating it as a secondary occupation. The job to be done won’t wait for the doer‘s leisure. One man one job is the key principal for a just city. A jack of all trades is a master of none. . Socrates’s city consists of citizens belonging to mainly three classes, the worker class (bronze), the warrior class (silver) and the ruler class (gold). A person can only be good at one role if the role is pursued as the only role and not having multiple roles, thus the purpose of each class must be to perform it’s own function and nothing else. Children of citizens of one class belong to the same class unless in exceptional cases. The elite (gold and silver) class, which is a small proportion of the society, are not allowed to own private property, must live together in open spaces, get paid a wage just to get by, are not allowed to touch currency and must abolish family pursuits. . Education of the philosopher king . To prevent the guardian class from dominating inside Socrates’s city and to make them love their own, they should be educated properly, first in arts, including poetry, literature, music, etc, because it softens the heart of the guardians and makes them harmonious and then in physical training. . Socrates criticizes the education of the arts during that time, and accuses it of having wrong ideals. He proposes to censor and control the arts (education through storytelling) and instead educate his guardian class in his own way of philosophy. This translates to the control of oneself by controlling of one’s desires and passions. . In order to ensure social order and not having mobility of social status of people of the 3 main classes, Socrates proposes to use myths and justifies them by calling them the noble lies to mis-educate the majority of the society. . What is justice? . Remember the idea of each class performing it’s own function and nothing else, since that is what leads to having the job done well? That, Socrates claims, is justice. Each part of the society, like each part of an individual’s soul, having and doing what is a it’s own and belongs to it, not externally (as for the city) but what is within, is justice. . “A man may neither take what is another’s (including job), not be deprived of what is his own.” . The citizens of the bronze/working class, who have the function to produce the necessities of life but also benefit the most, correspond to the bronze part of the individual’s soul, the part that has desires and appetites. Both require moderation and control in order to be virtuous. | The citizens of the silver/warrior class, who have the function to protect and defend the city, correspond to the silver part of the individual’s soul, spiritedness or passion or courage. | The citizens of the gold/ruling class, who are the wise and virtuous philosopher kings who understand true political knowledge, correspond to the gold part of the individual’s soul, the reasoning, which comes from the right knowledge or wisdom. | . Justice is the combination and harmonious interaction of the moderate, the courageous and the wise. Justice is to the soul as health is to the body. And just as the philosopher king should be in control of the city, the reason should be in control of appetites and spiritedness. . “One who is just doesn’t allow any part of himself to do the work of another part or allow the various classes within him to meddle with each other. He regulates well what is his own and rules himself. He puts himself in order, in his own friend, and harmonizes the three parts of himself like three limiting notes in a musical scale — high, low and middle. He binds together those parts and any other that may be in between, and from having been many things he becomes entirely one, moderate and harmonious. Only then does he act. And when he does anything, whether acquiring wealth, taking care of his body, engaging in politics, or in private contracts — in all of these, he believes that the action is just and fine that preserves this inner harmony and helps achieve it, and calls it so, and regards as wisdom the knowledge that oversees such actions. And he believes that the action that destroys this harmony is unjust, and calls it so, and regards the belief that oversees it as ignorance.” 443 d,e . References . The Republic, Plato &#8617; . |",
            "url": "https://zshn25.github.io/plato-republic/",
            "relUrl": "/plato-republic/",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "The purpose in life",
            "content": "During hard times it’s difficult to remember the motivation of getting by. We ask ourselves, in the grand scheme of things, does anything matter? . It is easy to go down the rabbit hole thinking nothing matters, after all we are an extremely tiny part of the whole universe existing for an extremely tiny amount of time. How can what we as individuals may do be of any importance? . . Fig.1: If time was scaled down to 1 year, human life is a blink of an eye 0.23s. By Efbrazil, CC BY-SA 3.0 , via Wikimedia Commons. Click to see a similar interactive infographic for space . For such times, this post is a reminder that in the grand scheme of things, we may as well not be of great importance but what if we’re wrong? What if we are the only hope of the universe. What if the universe has waited for all these billions of years just to produce something as complicated as us humans. What if we are one of the most important things that matters in the entire universe. . Beyond self . This is also a reminder to take in other perspectives. Yes, in the dimensions of space and time, we are just a tiny microscopic blip. But, in the realm of ideas and value, we are highly significant. We have made extremely huge progress in our understanding of nature and our abilities to build and craft tools using this knowledge which we could have never imagined. All of which wouldn’t have been possible without generations and generations of tiny little steps towards progress. Of course the person who played his part in making these steps would think that in the grand scheme of things he doesn’t matter but we know that he does. We all need to play our part. We are a part of this universe. We may as well be finite but the work we do to make progress may not be. . Such perspectives can be hard to come in western societies because of the heavy focus on self. Once we let go of our ego, we realize that we are a part of the universe and the present. We essentially make the universe meaningful. We may be powerless mortal beings but we have this connection to the eternal. And then, we realize the answer to the question, “what is the purpose of life?” The purpose of is to live. To play our part. By just existing. . He who has a why to live for can bear with almost any how . - Nietzsche . It did not really matter what we expected from life, but rather what life expected from us. We needed to stop asking about the meaning of life, and instead to think of ourselves as those who were being questioned by life-daily and hourly. Our answer must consist, not in talk and meditation, but in right action and in right conduct. Life ultimately means taking the responsibility to find the right answer to its problems and to fulfill the tasks which it constantly sets for each individual. . These tasks, and therefore the meaning of life, differ from man to man, and from moment to moment. Thus it is impossible to define the meaning of life in a general way. Questions about the meaning of life can never be answered by sweeping statements. “Life” does not mean something vague, but something very real and concrete, just as life’s tasks are also very real and concrete. They form man’s destiny, which is different and unique for each individual. No man and no destiny can be compared with any other man or any other destiny. No situation repeats itself, and each situation calls for a different response. Sometimes the situation in which a man finds himself may require him to shape his own fate by action. At other times it is more advantageous for him to make use of an opportunity for contemplation and to realize assets in this way. Sometimes man may be required simply to accept fate, to bear his cross. Every situation is distinguished by its uniqueness, and there is always only one right answer to the problem posed by the situation at hand. . Man‘s search for meaning, Viktor Frankl | . My purpose . I believe that the purpose of life is to pass information. Living beings (including viruses) do this using our DNA. Via evolution, newer generations adapt to the ever changing environment, giving them an advantage to sustain themselves. Humans are special in this regard as we also have other means of transferring information which is much more efficient. Recent advances in technology have enabled us to communicate and share knowledge in an unprecedented way. Although we as individuals are mortal, ideas are immortal, and by passing knowledge, we progress. The meaning of life for me is enabling this advancement. . I ultimately find purpose in my curiosity. It enables me to learn as much as I can, to explore and discover the unknown, to expand ideas, to seek answers, to invent and create solutions, to inspire others, to fail, to get humiliated and to repeat. In the process, I can share my experiences with others and there-by transfer knowledge. . Live as if you were to die tomorrow. Learn as if you were to live forever. . Mahatma Gandhi . Disclaimer: Formal definitions and philosophies of the meaning of life are out of scope of this post. I may later expand this topic here. .",
            "url": "https://zshn25.github.io/purpose/",
            "relUrl": "/purpose/",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "A minimal working example of a custom Vector class in C++",
            "content": "This post describes my minimum working implementation of a Vector class in C++, similar to the one from standard library’s std::vector but with only required functionality. The objective is to implement a generic array of dynamic size, i.e. which grows in size as new data gets added to it. We need at least 2 variables to keep track of the capacity of the array and the current index (which is also the size of the data available in the array). We also need a way to access these variables. . class Vector { size_t capacity_ = 0; // current memory capacity size_t curr_idx_ = 0; // current vector size (same as numel) public: size_t size() const {return curr_idx_;} // returns current size of our vector size_t capacity() const {return capacity_;} // returns current capacity of our vector }; . Indexing the vector . We store a pointer to the first element of the array. With this, we can access every element of the array up to it’s size. The pointer to the first element will be a private member. So, we also need a way to access the vector’s elements for read and write purposes. . In order to access the vector’s individual elements, we need a method to index the vector. This indexing method will be used for both reading and writing the vector’s elements. We define an operator[] which returns the element’s reference as follows . template&lt;class T&gt; class Vector { T* vector_ = nullptr; // pointer to first data element size_t capacity_ = 0; // current memory capacity size_t curr_idx_ = 0; // current vector size (same as numel) public: // ... //same as above // Element read/write access T&amp; operator[](const size_t index); // return element reference at index }; //// Definitions // Element read/write access template&lt;class T&gt; T&amp; Vector&lt;T&gt;::operator[](const size_t index) { if (index &gt;= curr_idx_) throw std::invalid_argument(&quot;Index must be less than vector&#39;s size&quot;); return vector_[index]; } . We need to allocate a dynamic memory using new[], which reserves the input amount of memory for our array. . Dynamic allocation of memory means we also need to free the memory manually upon destruction. . template&lt;class T&gt; class Vector { T* vector_ = nullptr; // pointer to first data element size_t capacity_ = 0; // current memory capacity size_t curr_idx_ = 0; // current vector size (same as numel) public: // Constructors Vector() = default; // default constructor // Destructor ~Vector() {delete[] vector_;} }; . Rule of 0/3/5: Defining copy constructor and copy assignment . Since we are explicitly defining a destructor for manual memory management, we also need to define the copy constructor and the copy assignment by the rule of 0/3. . template&lt;class T&gt; class Vector { T* vector_ = nullptr; // pointer to first data element size_t capacity_ = 0; // current memory capacity size_t curr_idx_ = 0; // current vector size (same as numel) public: // Constructors Vector() = default; // default constructor Vector(const Vector&lt;T&gt;&amp; another_vector); // copy constructor Vector&lt;T&gt;&amp; operator=(const Vector&lt;T&gt; &amp;); // copy assignment // Destructor ~Vector() {delete[] vector_;} }; . The copy constructor is used to copy an object of the same type. In our case, we need the copy constructor to copy another vector’s elements. Note that when not defined explicitly, the compiler defines a default copy constructor which does not do what we want. So, it is necessary to define a copy constructor . // Declaration same as before //// Definitions // Copy constructor template&lt;class T&gt; Vector&lt;T&gt;::Vector(const Vector&lt;T&gt;&amp; another_vector) { delete[] vector_; // Delete before copying everything from another vector // Copy everything from another vector curr_idx_ = another_vector.size(); capacity_ = another_vector.capacity(); vector_ = new T[capacity_]; for (size_t i=0; i &lt; capacity_; ++i) vector_[i] = another_vector[i]; } . The copy assignment is similar to the copy constructor but is called when the = operator is used, for e.g. Vector vector = another_vector;. The only difference here will be that the copy assignment will return the pointer to the object while the copy constructor doesn’t have to return anything. . // Declaration same as before //// Definitions // Copy assignment template&lt;class T&gt; Vector&lt;T&gt;&amp; Vector&lt;T&gt;::operator=(const Vector&lt;T&gt;&amp; another_vector) { delete[] vector_; // Delete before copying everything from another vector // Copy everything from another vector curr_idx_ = another_vector.size(); capacity_ = another_vector.capacity(); vector_ = new T[capacity_]; for (size_t i=0; i &lt; capacity_; ++i) vector_[i] = another_vector[i]; return *this; } . Adding and removing elements from the vector . We need a way to add elements to our vector. We can define an additional constructor (apart from the default one), which takes capacity as input and allocates memory of the given capacity. We can further extend this constructor to initialize our vector with a default value. . template&lt;class T&gt; class Vector { T* vector_ = nullptr; // pointer to first data element size_t capacity_ = 0; // current memory capacity size_t curr_idx_ = 0; // current vector size (same as numel) public: // Constructors Vector() = default; // default constructor Vector(const Vector&lt;T&gt;&amp; another_vector); // copy constructor Vector&lt;T&gt;&amp; operator=(const Vector&lt;T&gt; &amp;); // copy assignment Vector(size_t capacity, T initial = T{}); // constructor based on capacity and a default value // Destructor ~Vector() {delete[] vector_;} }; //// Definitions template&lt;class T&gt; Vector&lt;T&gt;::Vector(size_t capacity, T initial): capacity_{capacity}, curr_idx_{capacity}, vector_{new T[capacity]{}} // allocate stack and store its pointer { for (size_t i=0; i &lt; capacity; ++i) vector_[i] = initial; // initialize } . The above constructor can be called as follows . Vector&lt;int&gt; vector(10); // initializes a vector with capacity 10 Vector&lt;int&gt; vector_ones(10, 1) // initializes with an initial value . At this point, a minimum working example of a static array is complete. If we also need to make our vector dynamic, we need a way to increase the vector’s capacity if needed. This will be useful if we want to add elements to the array after we initialize it. . Push back and pop methods . Since the goal of our array is to be dynamic, we also need ways to add and remove elements from it after initialization. To add and remove elements from our vector, we define emplace_back and pop methods respectively. The array also needs to increase its capacity if it needs to. For this, we define a private reserve method, which reserves the input amount of memory. . template&lt;class T&gt; class Vector { public: // ... //same as above void emplace_back(const T&amp; element); // pass element by constant reference T pop(); // pops the last element private: // ... // same as above void reserve(const size_t capacity); }; //// Definitions // ... // same as above template&lt;class T&gt; void Vector&lt;T&gt;::emplace_back(const T&amp; element) { // If no cacacity, increase capacity if (curr_idx_ == capacity_) { if (capacity_ == 0) // handing initial when reserve(8); else reserve(capacity_*2); } // Append an element to the array vector_[curr_idx_] = element; curr_idx_++; } template&lt;class T&gt; T Vector&lt;T&gt;::pop() { if (curr_idx_ &gt; 0) // Nothing to pop otherwise { T to_return = vector_[curr_idx_-1]; // store return value before deleting // vector_[curr_idx_-1]-&gt;~T(); // delete from memory curr_idx_--; return to_return; } else throw std::out_of_range(&quot;Nothing to pop&quot;) } . While appending an element to our vector, the emplace_back function will check if the maximum capacity of the array is reached. This is done by comparing the curr_idx_ (size) of our vector with it’s capacity_. If capacity is same as size, then we reserve more memory. We do this inside the reserve function as follows . // Memory allocation template&lt;class T&gt; inline void Vector&lt;T&gt;::reserve(const size_t capacity) { // Handle case when given capacity is less than equal to size. (No need to reallocate) if (capacity &gt; curr_idx_) { // Reserves memory of size capacity for the vector_ T* temp = new T[capacity]; // Move previous elements to this memory for (size_t i=0; i &lt; capacity_; ++i) temp[i] = vector_[i]; delete[] vector_; // Delete old vector capacity_ = capacity; vector_ = temp; // Copy assignment } } . This completes our minimal working example of a vector class. In the next tutorials, we will extend this class to have iterators, a print function and mathematical operators. . Note that this implementation is just a minimal working example and is mainly for learning and educational purposes. The standard library’s vector must be preferred in practice. . . © Zeeshan Khan Suri, . If this article was helpful to you, consider citing . @misc{suri_cpp_vector_class_2021, title={A minimal working example of a custom Vector class in C + +}, url={https://zshn25.github.io/c++-vector-mwe-tutorial/}, journal={Curiosity}, author={Suri, Zeeshan Khan}, year={2021}, month={June}} .",
            "url": "https://zshn25.github.io/c++-vector-mwe-tutorial/",
            "relUrl": "/c++-vector-mwe-tutorial/",
            "date": " • May 13, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Privacy",
            "content": ". This post is still in draft mode. Some of the ideas are complete but needs expansion. Suggestions and comments are always welcome It is often important and helpful to formally define what we are talking about for everyone to be on the same page. Privacy is the ability to choose to express oneself selectively. It is the subtle distinction between what is morally acceptable between the bedroom, the bathroom, the dining table and the street. There are often things that we share with our friends/family over dinner which we would rather not share in public. Or, there might be things in our minds we would be embarrassed and even guilty to share with anyone else. This subtle distinction, when crossed could lead to undesirable consequences. For example, we would be okay finding ourselves naked in a shower but the same in a public place would be obnoxious. . This post is mostly about “digital privacy” and is used interchangeably. It is privacy in the digital age. Where technology and internet have made it extremely easy to infringe the our privacy. I will argue why we should care about our privacy, how (and by whom) it is being infringed, and what can you do about it. . Diving in topics such as this are like going down a never ending rabbit hole. I will be customarily expanding this post with new ideas and methods(of protecting oneself from being exploited) in the future but takes considerable amount time. I feel myself responsible to educate myself and in the process, share it with you here. While it is not obligatory, your support might definitely be helpful. The Why! Why should you care? . Most people ask me why they should care about their own privacy. It’s not like they have anything to hide. By definition (see above), this argument is flawed. Everyone has things to hide. The real question is from whom? As, things that we are okay with sharing with our beloved ones, we might not be willing to share with others. For example, nobody would agree to keep their houses unlocked (for others to come in and see), just because we have nothing to hide or you’re doing nothing wrong. You already care. . You might have nothing to hide, but you have something to fear . Most people do not realize the consequences of sharing things online or just allowing applications to access their information. If you are worried of keeping your house unlocked, you should as well be worried to let others into your life through your virtual windows. Applications are constantly tracking every single activity you do online. From, exactly how many milliseconds you look at a post, where exactly do you look on a picture, to how fast do you scroll through a post. Your every single interaction imaginable is being tracked by almost every modern application and is being stored permanently linking it to you. More of this is the How is your online privacy getting infringed section. . You might be taken out of context or misinterpreted . If one would give me six lines written by the hand of the most honest man, I would find something in them to have him hanged. . -Cardinal Richelieu . Privacy protects us from those who can misinterpret what we say, from those who maliciously want to harm us and from the powerful who want to abuse their power over us. Your personal preferences such as which flavor or ice cream you like might definitely be harmless but your political/religious believes could be used against you. You might be denied access to services, for example, your bank might deny you credit because you You might be denied from entering certain countries. . I need privacy, not because my actions are questionable, but because your judgement and intentions are. . Right to privacy . If you think privacy is unimportant for you because you have nothing to hide, you might as well say free speech is unimportant for you because you have nothing useful to say. . -Edward Snowden . Privacy speaks freedom . Humans are social beings. We care about what others think of us. We try to adjust our behavior depending on the social norms and others’ feedback. Imagine yourself in the middle of a busy street full of people. Would you be as free as you would be in your house all by yourself? Probably not. Not only humans change their behavior in others presence but also the possibility of someone being present changes our behavior. You are your true self when nobody’s looking and nobody could be looking; because you have the freedom to be so. You don’t have to constantly adjust yourself in the fear of being judged. . Privacy brings freedom. I am free to have my life private, to have a pay where I can go and be free of the judgmental eyes. I am free to allow whomever I want (or no one at all) to monitor me. To be able to live freely as if no one is looking. To be my true self. . That freedom is taken away when privacy is taken away ( for e.g., via surveillance). Institutions want to be in control and the most effective way of being in control is to take away freedom. To force compliance of social norms and orthodoxy. To brainwash the members into believing what they want them to believe. . . Freedom as a virtue, frees one from the social norms and orthodoxies and nourishes creative open thinking Freedom is not worth having if it does not include freedom of making mistakes. . Mahatma Gandhi . End-to-end encryption is not enough . The content of your messages or calls might be unnessary for companies to track you. Metadata such as who you’re talking to, when, from where, for how long, etc are enough for companies. Researchers found that just a smartphone’s accelerometer data can reveal people’s location, passwords, body features, age, gender, level of intoxication, driving style, and can be used to reconstruct words spoken next to the device. Services like WhatsApp recently realized that people started to care about privacy and started to advertise their End-toEnd encrytion . How is your online privacy getting infringed . You are being tracked . We rely on various social media and other applications for our everyday life. But do you know that these applications are tracking every single activity you do not only when the application is running but all the time? Most of these applications such as Facebook are free to users. But then how come the company is worth billions of dollars? What do they actually sell? . All 533,000,000 Facebook records were just leaked for free.This means that if you have a Facebook account, it is extremely likely the phone number used for the account was leaked.I have yet to see Facebook acknowledging this absolute negligence of your data. https://t.co/ysGCPZm5U3 pic.twitter.com/nM0Fu4GDY8 . &mdash; Alon Gal (Under the Breach) (@UnderTheBreach) April 3, 2021 A recent leak of Facebook’s database allows the private data to be misused by malicious indented actors 1. . Political propaganda . Nobody needs to justify why they “need” a right: the burden of justification falls on the one seeking to infringe upon the right. . -Edward Snowden . It is one thing that your Google search on . The world is full of ill-intended malicious people who wouldn’t mind destroying someone else’s life for their own benefit. All it takes is one mistake from innocent people being in the wrong place at the wrong time to screw them over. . Microsoft reports that 75 percent of U.S. recruiters and human-resource professionals now do online research about candidates, often using information provided by search engines, social-networking sites, photo/video-sharing sites, personal web sites and blogs, and Twitter. They also report that 70 percent of U.S. recruiters have rejected candidates based on internet information. . Source . Your information may be used against you by your government in order to gain political power. Think about the social credit system. It is already being practiced in some countries. For e.g., let’s say your country implements it and starts surveilling on you 24x7 and as an open-minded individual, you enter a discussion with your peers and talk against some things your government has done recently. There is a good chance that because once-upon-a-time you raised your voice against the government, you are denied flight tickets for your next vacation. . This again brings up my point on personal freedom. Privacy speaks freedom. The ability to criticize the government is the exact thing that keeps them in check. Taking away that ability by penalizing free speech takes away our freedom and restricts us . What can you do about it? . Governments approve surveillance programs citing anti-terrorism and the question arises “security versus privacy”. Schneier argues that the real question should be liberty versus control2. He argues that foreign physical threat is as bad as … . The privacy paradox . Many people are indeed concerned about their privacy but behave as they didn’t. This is known as the privacy paradox. . Humans are social beings and we seek others’ approval. We have a need for others to know what we are doing, which is why we voluntarily publish information about us online. But as social beings, we also need to acknowledge that there are things which let our close ones, our psychologists, our physicians, our lawyers to know about us which we would never be willing others know. People easily say they don’t value privacy while their actions negate their belief. . Other reasons for the privacy paradox might be “finding it technologically difficult to take certain actions that disallow institutions to take advantage of it. Applications are purposefully designed to make it as difficult for a regular user as possible to care about their privacy by changing the default settings, which are often extracting as much of user’s personal data as possible. Most users do not have the knowledge and experience to protect themselves. . Who ? Government (5 eyes) Companies . Choose companies and services that are interested in their users’ privacy and take measures to advocate it. This website gives great alternatives to the daily applications we use everyday. . What can you do? . Use encryption everywhere . How Strong Encryption Can Help Avoid Online Surveillance | Install https://www.eff.org/https-everywhere browser plugin | Change your default search engine to Duckduckgo on all browsers and start using it. | Avoid platforms which are notorious for privacy abuse. | Use Tor. Tor uses decentralized. | Tip: Use temp mail | Use OpenSource software. You can find opensource alternatives to almost every software. One example is Signal over Whatsapp | Visit privacytools.io and find alternatives to your everyday software. | Use tools like PrivacyBadger and UBlock Origin to block Ads and other stuff. | Encourage your friends and family to join the privacy preserving social media | . Extreme: . Use Qubes OS with Whonix virtual machine (VM) which routes all your network traffic through Tor. Qubes can also sandbox applications in their own VM so that the application does not affect anything else outside their own sandbox VM. If you don’t want to install the complete OS, you can also use Whonix VM seperately. | Setup matrix network on your own server for secure, decentralized communication . Follow this guide | https://chrome.google.com/webstore/detail/google-analytics-opt-out/fllaojicojecljbmefodhfapmkghcbnh/related?hl=en | . Closing thoughts . Book recommendation . To summarize, I would like to point you to the following TED talk which will hopefully fill-in the gaps I left unfilled. . References . 533 million Facebook users’ phone numbers and personal data have been leaked online &#8617; . | The Eternal Value of Privacy &#8617; . |",
            "url": "https://zshn25.github.io/why-privacy-matters/",
            "relUrl": "/why-privacy-matters/",
            "date": " • May 13, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Layers fusion for faster neural network inference",
            "content": "In the previous post, we proved that convolutions are linear. There are other linear layers in a neural network such as a batch normalization layer. A batch normalization layer normalizes its input batch to have zero mean and unit standard deviation, which are calculated from the input batch.1 It basically translates/shifts and scales the input batch, thus being a linear operation. In many network architectures such as ResNets2 and DenseNets3, a convolutional layer followed by a batch norm layer is used. . During training, the mean and standard deviation of the input batch are used in the batch normalization and are eventually learnt. During inference, these estimates of mean and standard deviation are used instead. The idea of this post is to fuse these two consecutive layers during inference, thereby reducing computation and thus inference time. . Note that this must not be done during training since the input batch&#39;s mean and standard deviation are not yet learnt and fusing before training will be same as removing the batch normalization completely. . Pytorch provides a utility function to fuse convolution and batch norm, although this was meant for the use of quantization. In this post, I share the following function to recursively check and fuse all consecutive convolution and batch norm layers. . from torch import nn from torch.nn.utils.fusion import fuse_conv_bn_eval def fuse_all_conv_bn(model): &quot;&quot;&quot; Fuses all consecutive Conv2d and BatchNorm2d layers. License: Copyright Zeeshan Khan Suri, CC BY-NC 4.0 &quot;&quot;&quot; stack = [] for name, module in model.named_children(): # immediate children if list(module.named_children()): # is not empty (not a leaf) fuse_all_conv_bn(module) if isinstance(module, nn.BatchNorm2d): if isinstance(stack[-1][1], nn.Conv2d): setattr(model, stack[-1][0], fuse_conv_bn_eval(stack[-1][1], module)) setattr(model, name, nn.Identity()) else: stack.append((name, module)) . Test . Fusing all convolution and batch norm layers of ResNet101 makes the resulting model ~25% faster with negligible difference in the model&#39;s output. . import torch from torchvision.models.resnet import resnet101 model=resnet101(pretrained=True).to(&#39;cuda&#39;) model.eval() rand_input = torch.randn((1,3,256,256)).to(&#39;cuda&#39;) # Forward pass output = model(rand_input) print(&quot;Inference time before fusion:&quot;) %timeit model(rand_input) # Fuse Conv BN fuse_all_conv_bn(model) print(&quot; nInference time after fusion:&quot;) %timeit model(rand_input) # compare result print(&quot; nError between outputs before and after fusion:&quot;) torch.norm(torch.abs(output - model(rand_input))).data . . Inference time before fusion: 43.4 ms ± 17.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Inference time after fusion: 31.2 ms ± 7.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Error between outputs before and after fusion: . tensor(3.5947e-05, device=&#39;cuda:0&#39;) . Note that the same can be done with any network with 2 or more consecutive linear layers to reduce inference time. . © Zeeshan Khan Suri, . If this article was helpful to you, consider citing . @misc{suri_Layers-fusion-for-faster-inference_2021, title={Layers fusion for faster neural network inference}, url={https://zshn25.github.io/Layers-fusion-for-faster-inference/}, journal={Curiosity}, author={Suri, Zeeshan Khan}, year={2021}, month={Apr}} . References . 1. Ioffe, S. &amp; Szegedy, C.. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:448-456 Link.↩ . 2. K. He, X. Zhang, S. Ren and J. Sun, &quot;Deep Residual Learning for Image Recognition,&quot; 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90. ↩ . 3. G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, &quot;Densely Connected Convolutional Networks,&quot; 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2261-2269, doi: 10.1109/CVPR.2017.243. ↩ .",
            "url": "https://zshn25.github.io/Layers-fusion-for-faster-inference/",
            "relUrl": "/Layers-fusion-for-faster-inference/",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "The virtue of fasting during Ramadan",
            "content": "In order to maintain economic equality and fairness in society, wealth needs to flow from the wealthy to the unfortunate. But often times, wealthy people do not understand the hardships of the poor, not because they are bad but simply because they haven’t experienced what it feels like to be poor. In order to experience this, Muslims are required to give up their worldly conveniences and step in the shoe of a poor person. During the month of Ramadan, practitioners are required to abstain from food, water, bad speech and sex from dawn till dusk.1 Fasting during Ramadan teaches Muslims self-control, discipline, sacrifice, empathy and compassion for the less fortunate and encourages generosity towards the poor. Most Muslims donate a percentage of their income as the obligatory tax (Zakāt) to the poor.2 Ramadan also encourages Muslims to devote time away from worldly activities into devotion and worship by rewarding greater than usual good deeds. Both fasting during Ramadan and charity (per year) are two of the five pillars of Islam and are necessary.3. . I think fasting is a great practice for exercising self-control, sacrifice and compassion towards the poor. Accompanying it with charity makes it a great custom for everyone (not only Muslims) to follow . https://en.wikipedia.org/wiki/Ramadan#Fasting &#8617; . | https://en.wikipedia.org/wiki/Zakat &#8617; . | https://en.wikipedia.org/wiki/Five_Pillars_of_Islam &#8617; . |",
            "url": "https://zshn25.github.io/fasting-during-ramadan/",
            "relUrl": "/fasting-during-ramadan/",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Compilation of 3D mesh models",
            "content": ". A compilation of 3D mesh models found on the web for research purposes. . Disclaimer: All images shown here are the sole property of their respective owners. I do not claim any Copyright/authority over them. . Note: This list is not regularly updated and is looking for maintainers. If you want to include some dataset, please open a Pull Request. . OmniObject3D, 2023: 6k objects in 190 categories . | Objaverse, 2023: 800K+ Annotated 3D Objects . . | Amazon Berkeley Objects (ABO) Dataset, 2022 . . | 3D-FUTURE: 3D FUrniture shape with TextURE, 2021 . . | Co3D: Common Objects in 3D, 2021 . . | AKB-48 Articulation Objects . . | Toys4K, 2021: 4k objects . . | Hypersim, 2021 . . | Habitat-Matterport 3D Research Dataset (HM3D), Link2, Link3, 2021 . | Omnidata, Link2, 2021 . . | TASKONOMY Dataset, 2021 . | BlendedMVS, 2020 . . | Scanned Objects by Google Research, 2020 . . | ScanObjectNN: Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data, 2019: 15k real scanned objects . . | Replica, ReplicaCAD, 2019 . . | ABC Dataset, 2019 . . Random examples from the dataset. Most models are mechanical parts with sharp edges and well defined surfaces . | SHREC’19, Shape Correspondence with Isometric and Non-Isometric Deformations, 2019 . . | SHREC’19, Correspondence in Humans with Different Connectivity, 2019 . . | Pix3D, 2018 . . | Morgan McGuire, Computer Graphics Archive, 2017 . . | MPII Human Shape . . | Tanks and Temples . | 2D-3D Semantics, 2017 . . | GREYC 3D Colored Mesh Database, 2017 . . | Thingi10K, 2017 . . | ObjectNet3D, 2016 . . | ShapeNet, 2016 . . | NIST CAD Models, 2016 . . | Kids with Topological Noise, 2016 . . | A Large Dataset of Object Scans, 2016 . . | TOSCA PARTIAL, 2015 . . | ModelNet, 2015 . . | PASCAL+, 2014 . . | FAUST, 2014 . . | Kids, 2014 . . | Human 3.6, 2014 . . . .   .   | | | | . | Body Models, 2014 . . | Clutter, 2013 . . | IKEA Dataset, 2013 . . . .   .   | | | | .   | | | | . | SHREC’10,’11, 2010, 2011 . . . .   .   | | | | . | A Benchmark for 3D Mesh Segmentation, 2009 . . | TOSCA High-res, 2008 . . | McGill 3D Shape Benchmark, 2008 . . | SHREC’07, Watertight Dataset, 2007 . . | Human Face, 2006 . . | Non-rigid world, 2006 . . | SCAPE, 2004 . . | The Brown Mesh Set, 2004 . . | Mesh Data from Deformation Transfer for Triangle Meshes, 2004 . . . . . .   .   | | | | | | .   | | | | | | . | The Princeton Shape Benchmark, 2003 . | . Other Miscellaneous Data . Keenan’s 3D Model Repository . , mostly under CC0 1.0 Universal (CC0 1.0) Public Domain Dedication . . . . . .   .   | | | | | | . Scotty3Dataset, 2020 CC-BY 4.0 . . See Keenan’s 3D Model Repository for more . | The Utah 3D Animation Repository . . . . . . .   .   | | | | | | | . | The Stanford 3D Scanning Repository . . . . . . .   .   | | | | | | | . | Killeroo 3D Scan . . | Cyberware Whole Body Scans . . . . . .   .   | | | | | | . | MPI Informatics Building Model . . | Cornell Box . . | NASA 3D Model Repo . . . . . . .   .   | | | | | | | . | Aim@Shape . | ORCA: Open Research Content Archive . . . . .   .   | | | | | . | MorphoSource . . | Procedurally Generated Random Objects . . | . Other compilations . Wikipedia - List_of_common_3D_test_models . | Free 3D Models Download: The Best Sites of 2022 . | Aim@Shape Links . | Level of Detail for 3D Graphics . | Point Cloud Library Dataset . | Yulan Guo’s website . | . . © Zeeshan Khan Suri, . If this article was helpful to you, consider citing . @misc{suri_compilation_3d_models, title={ Compilation of 3D mesh models }, url={https://zshn25.github.io/compilation-3d-mesh-resources/}, journal={Curiosity}, author={Suri, Zeeshan Khan}, year={2021}, month={Aug}} .",
            "url": "https://zshn25.github.io/compilation-3d-mesh-resources/",
            "relUrl": "/compilation-3d-mesh-resources/",
            "date": " • Apr 14, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Practicing Gratitude",
            "content": "I believe that most of us belong somewhere in the middle of this bell curve, which approximates how fortunate (in terms of many factors) people are. On one extreme are people who were born in extreme poverty without basic necessities, or with crippling physical and mental disabilities or during times of warfare and crisis or belonging to races/regions with extreme discrimination and injustice or have unfortunately undergone negative and life changing tragedies. On the other hand are the fortunate, with extreme amounts of wealth, health, freedom, luck, etc.. While most of us belong somewhere in the middle, we often question why are we not fortunate enough to be born in the richest, socially elite and forget than while belonging to the normal also means not belonging to the negative side. . . My hypothesis on how fortunate we are. X-axis: How fortunate we are. Y-axis: Population. Image by D Wells, CC BY-SA 4.0, via Wikimedia Commons. . This post is a reminder that while it’s not perfect, we should be grateful of being fortunate enough of not belonging to the extreme negative side. Let’s be grateful of our basic needs being fulfilled, of having access to food, clean water and shelter. Let’s be grateful to be born during such prosperous times with little conflicts and epidemics (except Covid-19). Let’s be grateful for having people who love and care for us, for the sacrifices many have suffered for our freedom and liberty to be who we want to be, to learn what we want to learn with access to unprecedented amount of knowledge the world has gathered for us and being able to live in the wonderful times where our personal freedoms are respected, where practicing equality is promoted and spreading of discrimination and hate is frowned upon. Let’s be grateful for being physically and mentally healthy without major disabilities and immediate access to world class healthcare and to not have undergone life crippling accidents/tragedies. Let’s be grateful for the world changing innovations happening in all fields every day. Let’s be grateful for this beautiful normal life. .",
            "url": "https://zshn25.github.io/practicing-gratitude/",
            "relUrl": "/practicing-gratitude/",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Dihydrogen Monoxide - The Truth",
            "content": "Dihydrogen monoxide: . is also known as hydroxyl acid, and is the major component of acid rain. | contributes to the “greenhouse effect”. | may cause severe burns. | contributes to the erosion of our natural landscape. | accelerates corrosion and rusting of many metals. | may cause electrical failures and decreased effectiveness of automobile brakes. | has been found in excised tumors of terminal cancer patients. | . Despite the danger, dihydrogen monoxide is often used: . as an industrial solvent and coolant. | in nuclear power plants. | in the production of styrofoam. | as a fire retardant. | in many forms of cruel animal research. | in the distribution of pesticides. Even after washing, produce remains contaminated by this chemical. | as an additive in certain “junk-foods” and other food products. . Coalition to ban DHMO by Jackson, Craig . | . P.S. . April fool .",
            "url": "https://zshn25.github.io/dihydrogen-monoxide/",
            "relUrl": "/dihydrogen-monoxide/",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Is convolution linear?",
            "content": "Discrete convolutions are characterized as matrix multiplications and are thus able to execute really fast on GPUs. But, how are they characterized as matrix multiplications? Are convolutions linear? Let’s find out. . 1. Definitions: . 1.1 Convolution . Let $f,g$ be two real valued functions in 1D $f,g : mathbb{R} to mathbb{R} $, the convolution of $f$ with $g$ is defined as . $f * g = displaystyle int_{ mathbb{R}} ! f(t) g(x-t) , mathrm{d}t$ . An example of the 1D convolution of a box function with itself can be seen in the example below . Convolution of a box function with itself. By Brian Amberg derivative work: Tinos, CC BY-SA 3.0. . 1.2 Linearity . Let $K$ be a mapping $K : A to B $ of two vector spaces $A,B$. Such a mapping is linear if . $K( alpha x+ beta y) = alpha K(x)+ beta K(y) $ . for all $ x in A, y in B$ and scalars $ alpha , beta in mathbb{R}$. . In simple words, a linear mapping/transformation preserves vector addition and scalar multiplication. It doesn’t matter whether the linear mapping is applied before or after vector addition and scalar multiplication. . . 2. Linearity of Convolution . To show that convolution is linear, for $ x,y,f : mathbb{R} to mathbb{R}, alpha , beta in mathbb{R}$, we need to prove . $( alpha x + beta y) * f stackrel{!}{=} alpha (x * f) + beta (y * f)$ . 2.1 Proof . (αx+βy)∗f=∫R ⁣(αx(t)+βy(t))f(x−t) dt=∫R ⁣(αx(t)f(x−t)+βy(t)f(x−t)) dt=α∫R ⁣x(t)f(x−t) dt+β∫R ⁣y(t)f(x−t) dt(Linearity of integral)=α(x∗f)+β(y∗f) large{ begin{aligned} ( alpha x + beta y) * f &amp;= int_{ mathbb{R}} ! ( alpha x(t) + beta y(t)) f(x-t) , mathrm{d}t &amp;= int_{ mathbb{R}} ! ( alpha x(t)f(x-t) + beta y(t)f(x-t)) , mathrm{d}t &amp;= alpha int_{ mathbb{R}} ! x(t)f(x-t) , mathrm{d}t + beta int_{ mathbb{R}} ! y(t)f(x-t) , mathrm{d}t quad text{(Linearity of integral)} &amp;= alpha( x * f) + beta( y * f) end{aligned} }(αx+βy)∗f​=∫R​(αx(t)+βy(t))f(x−t)dt=∫R​(αx(t)f(x−t)+βy(t)f(x−t))dt=α∫R​x(t)f(x−t)dt+β∫R​y(t)f(x−t)dt(Linearity of integral)=α(x∗f)+β(y∗f)​ . proves that convolution is a linear operator. This proof directly follows from that fact that an integral is a linear mapping of real-valued (integrable) functions to $ mathbb{R}$. . $ small{ displaystyle int_a^b{[{c_1}{f_1}(x)+{c_2}{f_2}(x)+ cdots +{c_n}{f_n}(x)]dx}={c_1} displaystyle int_a^b{f_1(x)dx}+{c_2} displaystyle int_a^b{f_2(x)dx}+ cdots +{c_n} displaystyle int_a^b{f_n(x)dx}}$ . . 3. Discrete convolution as matrix multiplication . So, if convolutions are linear, we should be able to express the discrete convolution as a matrix multiplication. In fact, one of the input function is converted to a Toeplitz matrix, enabling a discrete convolution to be characterized by a convolution. . y=h∗x=[h10⋯00h2h1⋮⋮h3h2⋯00⋮h3⋯h10hm−1⋮⋱h2h1hmhm−1⋮h20hm⋱hm−2⋮00⋯hm−1hm−2⋮⋮hmhm−1000⋯hm][x1x2x3⋮xn]y = h ast x = begin{bmatrix} h_1 &amp; 0 &amp; cdots &amp; 0 &amp; 0 h_2 &amp; h_1 &amp; &amp; vdots &amp; vdots h_3 &amp; h_2 &amp; cdots &amp; 0 &amp; 0 vdots &amp; h_3 &amp; cdots &amp; h_1 &amp; 0 h_{m-1} &amp; vdots &amp; ddots &amp; h_2 &amp; h_1 h_m &amp; h_{m-1} &amp; &amp; vdots &amp; h_2 0 &amp; h_m &amp; ddots &amp; h_{m-2} &amp; vdots 0 &amp; 0 &amp; cdots &amp; h_{m-1} &amp; h_{m-2} vdots &amp; vdots &amp; &amp; h_m &amp; h_{m-1} 0 &amp; 0 &amp; 0 &amp; cdots &amp; h_m end{bmatrix} begin{bmatrix} x_1 x_2 x_3 vdots x_n end{bmatrix}y=h∗x=⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎡​h1​h2​h3​⋮hm−1​hm​00⋮0​0h1​h2​h3​⋮hm−1​hm​0⋮0​⋯⋯⋯⋱⋱⋯0​0⋮0h1​h2​⋮hm−2​hm−1​hm​⋯​0⋮00h1​h2​⋮hm−2​hm−1​hm​​⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎤​⎣⎢⎢⎢⎢⎢⎢⎡​x1​x2​x3​⋮xn​​⎦⎥⎥⎥⎥⎥⎥⎤​ – Discrete convoliton using Toeplitz matrix . If this article was helpful to you, consider citing . @misc{suri_is-convolution-linear_2021, title={Is convolution linear?}, url={https://zshn25.github.io/is-convolution-linear/}, journal={Curiosity}, author={Suri, Zeeshan Khan}, year={2021}, month={Mar}} .",
            "url": "https://zshn25.github.io/is-convolution-linear/",
            "relUrl": "/is-convolution-linear/",
            "date": " • Mar 11, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "ResNet feature pyramid in Pytorch",
            "content": ". Feature Pyramids are features at different resolutions. Since Neural Networks compute features at various levels, (for e.g. the earliest layers of a CNN produce low level features such as Edges and later layers produce higher level features) it would be great to use not only the higher level features but also the previous ones for further processing. . One of the application of such feature pyramid is to be used in an autoencoder architecture with skip connections from encoder to decoder like U-Net. . . In this example, we look at ResNet from Pytorch. ResNet is one of the earliest but also one of the best performing network architectures for various tasks. We inherit the ResNet class and write our own forward method to output a pyramid of feature maps instead. . class ResNetFeatures(ResNet): def __init__(self,**kwargs): super(ResNetFeatures,self).__init__(**kwargs) def _forward_impl(self, x: torch.Tensor) -&gt; List[torch.Tensor]: # See note [TorchScript super()] x = self.conv1(x) x = self.bn1(x) x0 = self.relu(x) x = self.maxpool(x0) x1 = self.layer1(x) x2 = self.layer2(x1) x3 = self.layer3(x2) x4 = self.layer4(x3) return [x0,x1,x2,x3,x4] # returns features with channels [64,64,128,256,512] . That&#39;s it. Now, to use it, add it to the source of ResNet. I have made a Gist on how to do this here. Now you can give an argument features_only and it will return the feature pyramid as defined above. . Test code . ## This file is a modified version of [Pytorch&#39;s ResNet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) and inherits it&#39;s licence. ## License: Copyright Zeeshan Khan Suri, CC BY-NC 4.0 import torch from torch import Tensor import torch.nn as nn try: from torch.hub import load_state_dict_from_url except ImportError: from torch.utils.model_zoo import load_url as load_state_dict_from_url from typing import Type, Any, Callable, Union, List, Optional __all__ = [&#39;ResNet&#39;, &#39;ResNetFeatures&#39;, &#39;resnet18&#39;, &#39;resnet34&#39;, &#39;resnet50&#39;, &#39;resnet101&#39;, &#39;resnet152&#39;, &#39;resnext50_32x4d&#39;, &#39;resnext101_32x8d&#39;, &#39;wide_resnet50_2&#39;, &#39;wide_resnet101_2&#39;] model_urls = { &#39;resnet18&#39;: &#39;https://download.pytorch.org/models/resnet18-5c106cde.pth&#39;, &#39;resnet34&#39;: &#39;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#39;, &#39;resnet50&#39;: &#39;https://download.pytorch.org/models/resnet50-19c8e357.pth&#39;, &#39;resnet101&#39;: &#39;https://download.pytorch.org/models/resnet101-5d3b4d8f.pth&#39;, &#39;resnet152&#39;: &#39;https://download.pytorch.org/models/resnet152-b121ed2d.pth&#39;, &#39;resnext50_32x4d&#39;: &#39;https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth&#39;, &#39;resnext101_32x8d&#39;: &#39;https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth&#39;, &#39;wide_resnet50_2&#39;: &#39;https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth&#39;, &#39;wide_resnet101_2&#39;: &#39;https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth&#39;, } def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -&gt; nn.Conv2d: &quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation) def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -&gt; nn.Conv2d: &quot;&quot;&quot;1x1 convolution&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) class BasicBlock(nn.Module): expansion: int = 1 def __init__( self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None ) -&gt; None: super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;) if dilation &gt; 1: raise NotImplementedError(&quot;Dilation &gt; 1 not supported in BasicBlock&quot;) # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x: Tensor) -&gt; Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out class Bottleneck(nn.Module): # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) # while original implementation places the stride at the first 1x1 convolution(self.conv1) # according to &quot;Deep residual learning for image recognition&quot;https://arxiv.org/abs/1512.03385. # This variant is also known as ResNet V1.5 and improves accuracy according to # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. expansion: int = 4 def __init__( self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None ) -&gt; None: super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x: Tensor) -&gt; Tensor: identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out class ResNet(nn.Module): def __init__( self, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., nn.Module]] = None ) -&gt; None: super(ResNet, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d self._norm_layer = norm_layer self.inplanes = 64 self.dilation = 1 if replace_stride_with_dilation is None: # each element in the tuple indicates if we should replace # the 2x2 stride with a dilated convolution instead replace_stride_with_dilation = [False, False, False] if len(replace_stride_with_dilation) != 3: raise ValueError(&quot;replace_stride_with_dilation should be None &quot; &quot;or a 3-element tuple, got {}&quot;.format(replace_stride_with_dilation)) self.groups = groups self.base_width = width_per_group self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = norm_layer(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;relu&#39;) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) # type: ignore[arg-type] elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) # type: ignore[arg-type] def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int, stride: int = 1, dilate: bool = False) -&gt; nn.Sequential: norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if dilate: self.dilation *= stride stride = 1 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def _forward_impl(self, x: Tensor) -&gt; Tensor: # See note [TorchScript super()] x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x def forward(self, x: Tensor) -&gt; Tensor: return self._forward_impl(x) class ResNetFeatures(ResNet): def __init__(self,**kwargs): super(ResNetFeatures,self).__init__(**kwargs) def _forward_impl(self, x: torch.Tensor) -&gt; torch.Tensor: # See note [TorchScript super()] x = self.conv1(x) x = self.bn1(x) x0 = self.relu(x) x = self.maxpool(x0) x1 = self.layer1(x) x2 = self.layer2(x1) x3 = self.layer3(x2) x4 = self.layer4(x3) return [x0,x1,x2,x3,x4] # returns features with channels [64,64,128,256,512] def _resnet( arch: str, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], pretrained: bool, progress: bool, features_only = False, **kwargs: Any ): if features_only: model = ResNetFeatures(block=block, layers=layers, **kwargs) else: model = ResNet(block, layers, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return model def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;ResNet-18 model from `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; return _resnet(&#39;resnet18&#39;, BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs) def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;ResNet-34 model from `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; return _resnet(&#39;resnet34&#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;ResNet-50 model from `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; return _resnet(&#39;resnet50&#39;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;ResNet-101 model from `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; return _resnet(&#39;resnet101&#39;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;ResNet-152 model from `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; return _resnet(&#39;resnet152&#39;, Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs) def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;ResNeXt-50 32x4d model from `&quot;Aggregated Residual Transformation for Deep Neural Networks&quot; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; kwargs[&#39;groups&#39;] = 32 kwargs[&#39;width_per_group&#39;] = 4 return _resnet(&#39;resnext50_32x4d&#39;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;ResNeXt-101 32x8d model from `&quot;Aggregated Residual Transformation for Deep Neural Networks&quot; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; kwargs[&#39;groups&#39;] = 32 kwargs[&#39;width_per_group&#39;] = 8 return _resnet(&#39;resnext101_32x8d&#39;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;Wide ResNet-50-2 model from `&quot;Wide Residual Networks&quot; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; kwargs[&#39;width_per_group&#39;] = 64 * 2 return _resnet(&#39;wide_resnet50_2&#39;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs) def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet: r&quot;&quot;&quot;Wide ResNet-101-2 model from `&quot;Wide Residual Networks&quot; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr &quot;&quot;&quot; kwargs[&#39;width_per_group&#39;] = 64 * 2 return _resnet(&#39;wide_resnet101_2&#39;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs) . . model = resnet18(pretrained=True, features_only=True) features = model(torch.randn((1,3,265,265))) [print(f.shape) for f in features] . torch.Size([1, 64, 133, 133]) torch.Size([1, 64, 67, 67]) torch.Size([1, 128, 34, 34]) torch.Size([1, 256, 17, 17]) torch.Size([1, 512, 9, 9]) . [None, None, None, None, None] . © Zeeshan Khan Suri, . If this article was helpful to you, consider citing . @misc{suri_ResNet-feature-pyramid-in-Pytorch_2021, title={ResNet feature pyramid in Pytorch}, url={https://zshn25.github.io/ResNet-feature-Pyramid-in-Pytorch/}, journal={Curiosity}, author={Suri, Zeeshan Khan}, year={2021}, month={Mar}} .",
            "url": "https://zshn25.github.io/ResNet-feature-pyramid-in-Pytorch/",
            "relUrl": "/ResNet-feature-pyramid-in-Pytorch/",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "$GME short squeeze",
            "content": ". This week (starting 25/01/2021) is an exciting event in the history of stocks, where rare events happening to the GME stock. Although, this might be a definite opportunity to make/lose money, I am in it for the knowledge of how the stock market works. As I am trying to figure out this event and learn more about options trading, I came across this comment which explains things so clearly that I decided to share it here, for my future reference on options knowledge and also to give my readers an insight. . On the question, what is going on with GME, this following comment explains the situation in ELI5 terms . Comment from discussion All right WTF is going on with GME. Many people shorted. To hedge their shorts (since GME is sky-rocketing already), they will need to buy shares. But, since there are only limited shares available, this will drive the price more up, adding more pressure on other short sellers. This cycle repeats and is called the short squeeze. . Some keywords to understand . Shorting or short selling is when you bet your money on the stock declining. You sell shares at the current market price and you believe the share’s value decreases, you’ve sold high. Now, what if you don’t already own any shares? You can borrow shares (from the market), by making the market buy it from you in some future date. Where will you have it in the future? You buy it from the market’s future price (which you believe to be lower than current price) and hence have made money. . Many short sellers have borrowed GME shares, speculating it to crash and sold these borrowed shares to the market. Now, there are only limited shares available and due to heavy short selling, the (borrowed) shares are not actually available. . Companies which have already shorted, (predicted that the stock will go down) but since the opposite is happening (GME has sky rocketed to ATH since people have shorted), the short sellers will buy GME stock to reduce their future losses (or, reduce risk). This is called hedging. . The same user further explains with an example why the market makers (MM) would buy . Comment from discussion All right WTF is going on with GME. What to do? . This is a rare event where normal people like us can coordinate and take money for MM. I do not advice to do anything. Do your own DD, but I have bought some position today at open and hoping the squeeze to happen. Note that short squeeze could take days to happen. And the next question is, when to sell? Very good question. Nobody knows. Depending on your apatite for loss, you can wait for it to truly rocket or take whatever gains without FOMO on future gains. . Edit 1 26/01/2021 . Today we have seen another gamma squeeze where GME rose till $155 (over 100%) but at the end of the day, GME came down, closing at +20%. What drove the price down was believed to be a short ladder attack, where a short seller sells to another short seller and within a short time, the other short seller sells it back. What this does is make people think that other shareholders are selling at lower price, thus creating a havoc and making some of the investors sell at lower price. . GME - How shorts manipulated you, and how you can be better from r/wallstreetbets Edit 27/01/2021 . . Yesterday we have seen yet another gamma squeeze but this time it did not drop back. It was amazing to see GME rise by so much after hours. Maybe it was due to Elon Musk’s tweet? . Gamestonk!! https://t.co/RZtkDzAewJ . &mdash; Elon Musk (@elonmusk) January 26, 2021 The Endgame strategy? . GME Endgame from r/wallstreetbets Final Edit: . GME peaked at $450 as the result of the squeeze. Many brokers including the famous Robinhood in the US halted trading on GME. That means, no one was allowed to buy GME shares anymore. Only selling was allowed. As a result, the price fell from there. Many analysts later said that it GME wouldn’t be halted, it would indeed reach $1000. . . I use Trading212 for buying stocks, which does not charge any commission or any order fee. We both can get a free stock share worth up to €100 if you use the following link to sign up. Do not forget to deposit at least 1€ after signing up. Create a Trading 212 Invest account using this link https://www.trading212.com/invite/FzPJYhiT and we both get a free share! If you already have an account, you can try my Promo Code FzPJYhiT to get a free share. . Disclaimer: This is not a financial advice. Do your own DD before investing. .",
            "url": "https://zshn25.github.io/gme-stock/",
            "relUrl": "/gme-stock/",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Hot chocolate",
            "content": "I once visited a local Café and ordered Hot Chocolate. Didn’t expect much of it and thought it would be one of those powdered ready-made Cocoa mix. I was pleasantly surprised by how delicious this hot chocolate was. So, I decided to make such hot chocolate myself and looked for recipes. But I couldn’t find any raw Cacao without sugar. The 100% chocolates I could found after a lot of searching are . . Fig. 1: 100% chocolates I found on Amazon but the Sevenhills Cacao Mass is much cheaper. [Disclaimer] . Ingredients . Cacao | Milk | Cream | Spices (Optional) | Orange Zest (Optional) | Sugar or other sweeteners (Optional) | . Recipe . Take around 5 pieces of cacao mass. . . | Add the cacao and heavy cream to a bowl on medium heat. The high fat in the heavy cream makes the Cacao flavor come out . . | Stir with a spoon (preferably wooden to not destroy the non-stick bowl) until it looks like this. At this point you have very thick Keto hot chocolate (more like a pudding) and you can already consume it this way. . . | For the drink, add milk while continuing to stir . . | Add a pinch of Chilli, 3 pinches of Cinnamon powder or one stick, 2 pinches of Cardamom or 2 pods and 3 Cloves. You can also add Orange Zest for a true mulled chocolate feeling. You can also optionally add sugar or any other sweeteners, depending on your taste. . . | Let it simmer on low heat while stirring once in a while. The longer you leave it simmer, the thicker the hot chocolate becomes. . . | Enjoy!! . | Disclaimer: Amazon affiliate links .",
            "url": "https://zshn25.github.io/hot-chocolate/",
            "relUrl": "/hot-chocolate/",
            "date": " • Jan 23, 2021"
        }
        
    
  
    
  
    
        ,"post21": {
            "title": "How to Release on Github",
            "content": "To release on Git, you need to tag a commit first. . Tagging a commit . A tag is a label attached to a specific commit. There are two types of tags . Lightweight | Annotated | . git tag command is used to view all tags in the repo. . Lightweight tag . The lightweight tag is a simple reference to a commit. . git tag &lt;tagname&gt; &lt;commit&gt; creates a lightweight tag. If &lt;commit&gt; is optional and defaults to HEAD. . Example . $ git tag v0.1 HEAD^ # tag the previous commit $ git tag # view all tags v0.1 $ git tag v1.0 # tag the current commit $ git tag # view all tags v0.1 v1.0 . Annotated tag . The annotated tag is a full git object which includes additional (annotated) information such as tag author info, tag date, tag message and tag commit ID. In general, annotated tags are recommended over lightweight . To tag a commit with an annotated tag, use the git tag command with -a option. You must also specify a message using -m option . Example . $ git tag -a -m &quot;feature release 1.0&quot; v1.0 . Pushing tags . git push does not automatically push the tags to remote repository. . To transfer a single tag, use git push &lt;remote&gt; &lt;tagname&gt; | . | To transfer all tags, use git push &lt;remote&gt; --tags | . | . Releases . Once you tag a commit, you can use this to create a release. On GitHub, you can create a release by following this tutorial .",
            "url": "https://zshn25.github.io/how-to-release-on-git/",
            "relUrl": "/how-to-release-on-git/",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Nutrition",
            "content": "Preventing disease is the first step to long-term health and well-being. By following basic nutrition advice you can improve your health by significant amounts. . . Obesity as a worldwide public health crisis . Western diet is now a major cause of obesity, which is a recognized chronic disease with well-defined health consequences such as Cardiovascular diseases, type 2 diabetes, obstructive sleep apnea, certain types of cancer, osteoarthritis, depression 1. Obesity is labelled by the WHO as a worldwide public health crisis 2. Western diet is normally defined as highly processed foods with very little whole grains, fruits and vegetables. The rate at which obesity is increasing is also increasing at an alarming rate. Data shows that 39% of adults aged 18 years and older were overweight or obese in 20163. . Fig. 1: Obesity is increasing worldwide at an alarming rate. Source . The need to be aware of nutrition in order to avoid obesity and stay healthy now is more than ever. Controlling our food behavior is essential to our long term well being. . Why is obesity increasing? . We are increasingly outsourcing our food preparation. Convenient food has become a norm. These fast-food is often designed as snack-food and marketed to be eaten continuously throughout the day. Such foods are not only less healthy, very high in salt, trans-fat and sugar, but we also tend to overeat them. . This is not an Ad. As seen in the above commercial, nutrient dense foods tend to spoil fast while processed foods usually have very low nutrients in order to increase their shelf life (even pests don’t want to eat processed food). The refining process removes important nutrients such as fiber, iron, B vitamins, in order to increase the food’s shelf life. Their business model is to use the cheapest available raw ingredients, process it as much as possible to increase shelf life and add large amounts salt, fat and sugar to make them seem attractive. . . Focus on food rather than nutrients . We have all been taught that food is made up of 3 macronutrients . Carbohydrates | Proteins | Fats | . Historically, food was a scarce resource. In the animal kingdom, presence of food was unpredictable. So, we evolved to store food in the body for later use. All calories (including proteins, carbs and fats) that aren’t converted into usable energy, ATP, are stored in the body in the form of adipose tissue for later use. This is what we call body fat. It is not necessarily bad to have body fat but if you have excess of body fat, it can lead to severe long term-health issues. . Is dietary fat really bad? . To address obesity, a lot of focus on reducing saturated fats in diets were made. The processed food industry responded by supplementing fat with sugar to make the food seem more appealing. This also increased the shelf life of foods and so almost every packaged food now comes with enormous amounts of sugar and other sweeteners. This fueled our modern obesity and diabetes. . Despite of an enormous increase in the supply of fat-free foods, obesity rates continued to rise. It is important to know the difference between good and bad fats . The good, the bad, and the ugly . Unsaturated fats, such as Omega-3 fatty acids, which are found in fish and some nuts,seeds, are good for long term health. In fact, Omega-3 fatty acids are the only kinds which cannot be made in human body. So, it is essential to have them. . | Saturated fats are bad for health 4. They increase the amound of bad chelestrol in the blood (LDL). . | While naturally occurring unsaturated fats, such as the ones found in olive oil, avocado, fish and nuts are good, chemically engineered saturated fats, such as the ones found in margarines and frying oils have week chemical bonds and are easily converted into trans-fats 5. Trans fats increase the amount of bad cholesterol in the blood (LDL) and decreases the amount of good cholesterol (HDL), which increases the risk of heart disease 6. These must be avoided as much as possible. . | . Are carbohydrates really bad? . A lot of recent focus has been on the fact that we overconsume carbohydrates. A lot of popular diet trends encourage us to drastically reduce carbs from our diet, even to the extend of avoiding fresh fruits and even grains. At the same time, they seem to suggest that high intake of proteins and fats can be eaten freely. But, a diet with high amounts of animal protein and no whole grains, fruits could leave us with serious problems in the long run. . Carbs has a large variety of foods, some of which are very important for our health, such as brown rice, rolled oats which are rich in fiber; and some are are bad for our health, such as corn-syrup, which has no fiber. In general, the lower the glycemic index (GI) of the carbohydrate, the better. A list of low, medium and bad carbs w.r.t GI is mentioned here. . What about proteins? . All proteins are not created equal. Plant based proteins contain more fiber and less saturated fat than animal based proteins. People who eat lots of plant based diet, have much better health and longevity than people who eat heavy meat diet 7. A diet high in poor quality animal protein, such as processed meat, and high fat cuts, can be harmful to health. Processed meats tend to have very high amounts of sodium, which is a contributor to high blood pressure. . Thinking of food as a whole and not as macronutrients . Categorizing foods into these basic micronutrients may be helpful to study foods but it is not helpful to communicate about them. Food is much more complex than just these macronutrients. So, rather than focusing on nutrients, we should focus on food, because ultimately we don’t eat nutrients but food. . Key takeaways: . The quality of the source of nutrients matters a lot for long term health. . Eat naturally occurring unsaturated fats, such as avocado, olive oil, nuts | low GI carbs with lots of fiber | plant-based proteins. | . | Reduce saturated fats, such as red meat | medium GI carbs with less fiber | animal based proteins. | . | Avoid trans fats, such as oils used for frying in fast food restaurants | refined sugar and carbohydrates with high GI | processed meats | . | . . Exercise . eating --&gt; energy++ excercizing --&gt; energy-- . For people who are trying to lose excess weight, favoring energy expenditure over energy storage needs to be prioritized. This can be achieved by consuming fewer calories and exercising more. . But, not only the quantity of the food, but also the quality of the food we eat matters. . . Summary . When we outsource our food preparation to large companies or restaurants, we get low nutrient dense food. . Nutrient density=nutrients per gramcalories per gram text{Nutrient density} = frac{ text{nutrients per gram}}{ text{calories per gram}}Nutrient density=calories per gramnutrients per gram​ . Home-made food, cooked using high quality ingredients, having natural unsaturated fats, low GI, lots of fiber, and mostly plant-based protein will lead to good long-term health and longevity. The following food pyramid summaries this in a graphical, easy to remember illustration. . . Fig.3: Food pyramid. Spmallare, CC BY 3.0, via Wikimedia Commons . Even if you’re not obese, being aware of the food you eat and maintaining physical exercise will ensure long term health. . . References . https://en.wikipedia.org/wiki/Obesity#Effects_on_health &#8617; . | Hurt RT, Kulisek C, Buchanan LA, McClave SA. The obesity epidemic: challenges, health initiatives, and implications for gastroenterologists. Gastroenterol Hepatol (N Y). 2010;6(12):780-792. &#8617; . | WHO (2018) – Fact sheet – Obesity and overweight. Updated February 2018 &#8617; . | https://en.wikipedia.org/wiki/Fat#Cis_and_trans_fats &#8617; . | https://en.wikipedia.org/wiki/Saturated_fat#Association_with_diseases &#8617; . | https://www.webmd.com/diet/guide/understanding-trans-fats &#8617; . | https://www.health.harvard.edu/blog/eat-more-plants-fewer-animals-2018112915198 &#8617; . |",
            "url": "https://zshn25.github.io/nutrition/",
            "relUrl": "/nutrition/",
            "date": " • Oct 30, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ":wave: Welcome to Curiosity :tm: . My name is Zeeshan Khan Suri. I believe in life long learning :mortar_board:, I yearn for knowledge :books: and I seek :eyes: the truth :microscope:. In Curiosity, I plan to document :pencil: my journey :roller_coaster: towards this goal :dart:, for that is what that matters in the end :rocket:. Be open minded :haircut_woman:, humble :speak_no_evil: and try not to judge :balance_scale: me by my perspective :performing_arts:. Read about my purpose in life to know me better. . Donations . Ether Wallet address: 0xc1be9fd4821e50f5300b027a4254250bb2510387 . Acknowledgements . This blog is also my self exploration into website building without knowing anything about websites. This learning experience was made possible into a live website thanks to these resources and people. . Jekyll, the static site-generator with Minima theme with lots of customizations. | Github Pages for hosting my website for free. | Fastpages, Jupyter Notebooks to Markdown converter with many other functionalities. (It’s great!) | Jekyll TOC | Jekyll-Codex | Dark mode | .",
          "url": "https://zshn25.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zshn25.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}